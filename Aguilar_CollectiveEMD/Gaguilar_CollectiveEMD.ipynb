{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gaguilar-CollectiveEMD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GNV_kqwPPffd",
        "P5Y_NOU9I3zi",
        "75TQmQgbnHwM",
        "jLm3eMGNGM4x",
        "mzd0dE0kXUXn",
        "8QiYJ45QEfUB",
        "qyV04iUobxUU",
        "zgqGsSq3i1w4"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNV_kqwPPffd"
      },
      "source": [
        "## **Installation and Import Stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy6bAJBnkymz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef6af86-5a43-44ef-ef0c-4ff918a06f59"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBgg3mXCCiTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee39516a-cf3b-4965-e89d-b21a36706d39"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX-zR01HClOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e522300-6fb9-4742-9a29-0663b1d75e03"
      },
      "source": [
        "%cd gdrive/My Drive/gaguilar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1D5I4ZlzhFow8fFGxbipMXBZSuRXJ01yA/gaguilar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g583AD8QCsWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b685ac17-4b5f-4642-cef7-936af7eb8ea7"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "seed_number = 42\n",
        "\n",
        "print(\"GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
        "physical_devices= tf.config.experimental.list_physical_devices('GPU')\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# tf.compat.v1.set_random_seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n",
            "GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13413609558998971057\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 16183459840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 6995042542037162829\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xub_9pUDCxTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b599c42f-5268-4e24-96de-dc803bb3b93b"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co65HRppuavQ",
        "outputId": "de6097df-2341-444c-b470-72eef21e9720"
      },
      "source": [
        "!pip3 install emoji\n",
        "!pip3 install python-crfsuite\n",
        "!pip3 install sklearn-crfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.7/dist-packages (0.9.7)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS8U38HSufb2"
      },
      "source": [
        "# print('intermittent')\n",
        "import os\n",
        "# import tensorflow as tf\n",
        "# # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "# tf.set_random_seed(42)\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(seed_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhFmhfKqtyef"
      },
      "source": [
        "from keras import backend as K\n",
        "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(seed_number)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
        "K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV3r4HPYMDv1",
        "outputId": "4c51bac4-4039-426d-dcab-0e4ad3382b4a"
      },
      "source": [
        "!pip3 install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCD1hqkMiie_",
        "outputId": "98ae3ea7-e015-4dfa-a8d1-19116bea30f9"
      },
      "source": [
        "!pip3 install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.14)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPO-zRE9m9z_",
        "outputId": "a1116048-3c25-458b-8cd5-9fd0baeaaf2c"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiacJKWghMqs",
        "outputId": "f34b6b1c-5d73-492d-8d60-589aca62c5c5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "np.random.seed(seed_number)\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f076a046ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Y_NOU9I3zi"
      },
      "source": [
        "## **Setting some global stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyZI772RI8lk",
        "outputId": "d3462fa5-a9ea-45e6-dea5-2b085376bb12"
      },
      "source": [
        "import torch\n",
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "# seed = 123\n",
        "torch.manual_seed(seed_number)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed_number)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzbjfBbwlarT"
      },
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "class PhraseEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,output_size,device):\n",
        "        super(PhraseEmbedding, self).__init__()\n",
        "        self.print_once=True\n",
        "        self.dense_layer = nn.Linear(input_size,output_size)\n",
        "        self.non_linear_layer = nn.Tanh()\n",
        "        self.cosine_layer = nn.CosineSimilarity(dim=0)\n",
        "        self.device = device\n",
        "        return\n",
        "\n",
        "    def encode(self, input_embedding):\n",
        "\n",
        "        # print(input_embedding.size())\n",
        "        input_sentence_embedding = input_embedding.squeeze(0)\n",
        "        # print(input_sentence_embedding.size())\n",
        "        # print('-----')\n",
        "\n",
        "        # Max Pool\n",
        "        # max_pooled_embedding = torch.max(input_sentence_embedding,dim=0)\n",
        "\n",
        "        #Average Pool\n",
        "        average_pooled_embedding = torch.mean(input_sentence_embedding,dim=0).to(device=self.device)\n",
        "        # print(average_pooled_embedding.size())\n",
        "\n",
        "        x = self.dense_layer(average_pooled_embedding)\n",
        "        # print(x.size())\n",
        "\n",
        "        out = self.non_linear_layer(x)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        # print(len(input_tuple))\n",
        "        input_source = input_tuple[0]\n",
        "        input_target = input_tuple[1]\n",
        "\n",
        "        output_source = self.encode(input_source)\n",
        "        output_target = self.encode(input_target)\n",
        "\n",
        "        similarity = self.cosine_layer(output_source, output_target)\n",
        "        # print(similarity)\n",
        "        return similarity\n",
        "\n",
        "    def getEmbedding(self, input_embeddings):\n",
        "        with torch.no_grad():\n",
        "            return self.encode(input_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75TQmQgbnHwM"
      },
      "source": [
        "## **Initialization of some components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzKhH-AKDsS"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "sentence_tokenizer._params.abbrev_types.add('ret.')\n",
        "sentence_tokenizer._params.abbrev_types.add('rep.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MiXK5XBm2fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f71e17d-768d-4880-8e7b-f81bb7695274"
      },
      "source": [
        "from os import path\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "# Initialize network\n",
        "output_embedding_size = 100\n",
        "\n",
        "phraseEmbeddingModel = PhraseEmbedding(100, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# define checkpoint saved path\n",
        "ckp_path = \"entityEmbedding/model_checkpoints/checkpoint_model100.pt\" #100\n",
        "\n",
        "if(path.exists(ckp_path)):\n",
        "    # load the saved checkpoint\n",
        "    entityPhraseEmbedder, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "\n",
        "    print(\"starting with model at epoch:\", start_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting with model at epoch: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YIxqQu4xuBZ"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(seed_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLm3eMGNGM4x"
      },
      "source": [
        "## **Entity Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkK6Oyp6GUkZ"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# import numpy as np\n",
        "# np.random.seed(seed_number)\n",
        "\n",
        "\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,50)\n",
        "    self.linear2 = nn.Linear(50,25)\n",
        "    self.linear3 = nn.Linear(25,1)\n",
        "    # self.linear4 = nn.Linear(6,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    # x = F.relu(self.linear3(x))\n",
        "    # x = F.relu(self.linear4(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifier():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # # using embedding + syntax features\n",
        "        # self.combined_feature_list=['length']+['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(100)]\n",
        "\n",
        "        # separately using only semantic features\n",
        "        self.combined_feature_list=['length']+['cf_'+str(i) for i in range(100)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length'#,\n",
        "            # 'normalized_cap',\n",
        "            # 'normalized_substring-cap',\n",
        "            # 'normalized_s-o-sCap',\n",
        "            # 'normalized_all-cap',\n",
        "            # 'normalized_non-cap',\n",
        "            # 'normalized_non-discriminative'\n",
        "            ]+['normalized_cf_'+str(i) for i in range(100)]\n",
        "\n",
        "        # create scaler\n",
        "        self.scaler = StandardScaler()\n",
        "        # self.scaler = MinMaxScaler()\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.0001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 1000\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            # # fit and transform in one step\n",
        "            training_inputs_array_standardized = self.scaler.fit_transform(training_inputs_array)\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array_standardized).type(torch.float)\n",
        "            # training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.9))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifier_checkpoint_model100.pt\" #100\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        f_path = checkpoint_dir + '/classifier_checkpoint_model100.pt' #100\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_training_loss,',',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                if(combined_validation_loss<best_loss):\n",
        "                # if(f1>best_f1):\n",
        "                    best_loss = combined_validation_loss\n",
        "                    # best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs_array_standardized = self.scaler.fit_transform(test_inputs_array)\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array_standardized).type(torch.float)\n",
        "        # test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzd0dE0kXUXn"
      },
      "source": [
        "## **PHASE I: Local NER Engine (Results already returned from Gaguilar, mainly dataframe preparation for Phase II)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHzZZ8E-XT4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ce973e-3424-45a9-821e-04deabc91000"
      },
      "source": [
        "import re\n",
        "import emoji\n",
        "from emoji import demojize\n",
        "import string\n",
        "\n",
        "# import numpy as np\n",
        "import pandas  as pd\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "import trie as trie\n",
        "import ast\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"other\",\"since\",\"hence\",\"onto\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"up\")\n",
        "\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"v.\"]  #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"httpurl\",\"n't\",\"pls\",\"bye\",\"€\",\"blah\",\"vs\",\"ouch\",\"am\",\"pm\",\"omw\",\"http\",\"https\",\"tv\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"ip\",\"qft\",\"ima\",\"icymi\",\"bdsm\",\"ah\",\"ive\",\"qt\",\"dj\",\"dm\",\"pts\",\"pt\",\"yrs\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "\n",
        "class LocalNERModule():\n",
        "    def __init__(self, sentenceTokenizer, sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation, device):\n",
        "        self.counter=0\n",
        "\n",
        "        if(sentenceTokenizer):\n",
        "            self.my_sentence_tokenizer = sentenceTokenizer\n",
        "        else:\n",
        "            nltk.download('gutenberg')\n",
        "            gutenberg_text = \"\"\n",
        "            for file_id in gutenberg.fileids():\n",
        "                gutenberg_text += gutenberg.raw(file_id)\n",
        "            tokenizer_trainer = PunktTrainer()\n",
        "            tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "            tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "            self.my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "        self.quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        self.tweet_to_sentences_w_annotation = tweet_to_sentences_w_annotation\n",
        "        self.device = device\n",
        "        self.apostrophe_list =[\"'s\",'’s','s']\n",
        "        self.label_list = ['O','B','I']\n",
        "\n",
        "        self.tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "        self.contextual_embeddings = {}\n",
        "\n",
        "        self.gaguilar_sentence_outputs = sentence_df_dict_gaguilar\n",
        "\n",
        "        print('Started Local NER Engine!')\n",
        "\n",
        "    def is_float(self,string):\n",
        "        try:\n",
        "            f=float(string)\n",
        "            if(f==0.0):\n",
        "              return True\n",
        "            else:\n",
        "              return ((f) and (string.count(\".\")==1))\n",
        "      #return True# True if string is a number with a dot\n",
        "        except ValueError:  # if string is not a number\n",
        "          return False\n",
        "\n",
        "    def normalize_to_sentences(self, text):\n",
        "        tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "        tweetSentenceList_inter=self.custom_flatten(list(map(lambda sentText: self.my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "        tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "        return tweetSentenceList\n",
        "\n",
        "    def custom_flatten(self, mylist, outlist, ignore_types=(str, bytes, int)):\n",
        "        \n",
        "        if (mylist !=[]):\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.custom_flatten(item, outlist)\n",
        "                else:\n",
        "                    item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "\n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "    #         elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "    #             temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "    #             if(temp1):\n",
        "    #                 temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "    #             temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.custom_flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "\n",
        "    def remAmpersand(self,candidateStr):\n",
        "        candidateStr=candidateStr.replace('&amp;','')\n",
        "        return candidateStr\n",
        "\n",
        "    def normalizeToken(self,token):\n",
        "        lowercased_token = token.lower()\n",
        "        if token.startswith(\"@\"):\n",
        "            return \"@USER\"\n",
        "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "            return \"HTTPURL\"\n",
        "        elif len(token) == 1:\n",
        "            return demojize(token)\n",
        "        else:\n",
        "            if token == \"’\":\n",
        "                return \"'\"\n",
        "            elif token == \"…\":\n",
        "                return \"...\"\n",
        "            else:\n",
        "                return token\n",
        "\n",
        "    def normalizeTweet(self, tweet):\n",
        "        tokens = self.tweetTokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "        normTweet = \" \".join([self.normalizeToken(token) for token in tokens])\n",
        "\n",
        "        normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "        normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "        normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
        "\n",
        "        normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "        \n",
        "        return normTweet\n",
        "\n",
        "    #removing commonly used expletives, enunciated chat words and other common words (like days of the week, common expressions)\n",
        "    def slang_remove(self,ne_phrase):\n",
        "        phrase=ne_phrase.strip().strip(string.punctuation).lower()\n",
        "        p1= re.compile(r'([A-Za-z]+)\\1\\1{1,}')\n",
        "        match_lst = p1.findall(phrase)\n",
        "        if phrase in article_list:\n",
        "            return True\n",
        "        elif phrase in day_list:\n",
        "            return True\n",
        "        elif phrase in month_list:\n",
        "            return True\n",
        "        elif match_lst:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def all_slang(self, ne_phrase):\n",
        "        ne_words=ne_phrase.split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list\n",
        "        is_invalid=0\n",
        "\n",
        "        for word in ne_words:\n",
        "            if(word.strip().strip(string.punctuation).lower() in combined):\n",
        "                is_invalid+=1\n",
        "        if(is_invalid==len(ne_words)):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def start_end_token_check(self, ne_candidate):\n",
        "        positions = ne_candidate[1]\n",
        "        ne_words=ne_candidate[0].split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list+prep_list+article_list\n",
        "        # print(combined)\n",
        "        # print(ne_words,positions)\n",
        "        if(ne_words):\n",
        "            start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "            end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "            while((len(ne_words)>1)&((start_word in combined)|(end_word in combined))):\n",
        "                if(start_word in combined):\n",
        "                    ne_words.pop(0)\n",
        "                    positions.pop(0)\n",
        "                if(end_word in combined):\n",
        "                    ne_words.pop()\n",
        "                    positions.pop()\n",
        "                if(len(ne_words)>1):\n",
        "                    start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "                    end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        ne_phrase = (' '.join(ne_words)).strip()\n",
        "        # print(ne_phrase)\n",
        "        return (ne_phrase,positions)\n",
        "\n",
        "    def set_stopword_exceptions(self,words):\n",
        "        combined=cachedStopWords+prep_list+article_list+day_list\n",
        "        for word in words:\n",
        "            if word in combined:\n",
        "                self.swSet.add(word)\n",
        "    \n",
        "    def compile_token_embeddings(self, lst):\n",
        "        token_embeddings=[]\n",
        "        for tup in lst:\n",
        "            token = tup[0]\n",
        "            # tup[1] is feature dict, keys 0 to 99\n",
        "            embedding_dict = tup[1]\n",
        "            embedding_arr = np.array([embedding_dict['feat'+str(i)] for i in range(0,100)])\n",
        "            token_embeddings.append(torch.from_numpy(embedding_arr).float())\n",
        "        return token_embeddings\n",
        "\n",
        "    def extract(self, batch_number):\n",
        "        print(\"Running Local NER now\")\n",
        "        time_in=time.time()\n",
        "        self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'TweetSentence','tweetwordList', 'phase1Candidates','start_time','entry_batch'))\n",
        "        df_holder=[]\n",
        "        self.swSet= set()\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates','correct_candidates_tweet'))\n",
        "            #dict1 = {'tweetID':0, 'sentID':0, 'hashtags':'first', 'user':'user', 'TweetSentence':'sentence', 'phase1Candidates':'phase1Out','start_time':'now','entry_batch':'batch_number'}\n",
        "            self.CTrie=trie.Trie(\"ROOT\")\n",
        "            self.phase2stopWordList=[]\n",
        "            self.sentenceID = 0\n",
        "            self.f=0\n",
        "\n",
        "        for key, value in self.gaguilar_sentence_outputs.items():\n",
        "\n",
        "            now = datetime.datetime.now()\n",
        "            tweetID=str(value[0])\n",
        "            sentID=str(value[1])\n",
        "            sentence=str(value[2])\n",
        "\n",
        "            tweetWordList = sentence.split()\n",
        "            enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "            annnotated_mentions=[]\n",
        "\n",
        "            self.sentenceID=key\n",
        "\n",
        "            #need both entity phrases and phrase token positions\n",
        "            entities_from_sentence = value[3]\n",
        "            # print(entities_from_sentence)\n",
        "            phase1Out=\"\"\n",
        "\n",
        "            #extract sentence token level 100d embeddings\n",
        "            token_embedding_list= list(value[4])\n",
        "            entity_aware_embeddings = self.compile_token_embeddings(token_embedding_list)\n",
        "            # for tup in lst:\n",
        "            #     print(tup[0],'=>',tup[1])\n",
        "\n",
        "            just_candidates=[]\n",
        "\n",
        "            # place some necessary filters\n",
        "            entities_from_sentence= list(filter(lambda element: not self.slang_remove(element[0]), entities_from_sentence))\n",
        "            entities_from_sentence= list(map(lambda element: self.start_end_token_check(element), entities_from_sentence))\n",
        "            entities_from_sentence= list(filter(lambda element: not self.all_slang(element[0]), entities_from_sentence))\n",
        "            entities_from_sentence= list(filter(lambda element: len(element[0])>1, entities_from_sentence))\n",
        "            entities_from_sentence= list(filter(lambda element: element[0]!='', entities_from_sentence))\n",
        "\n",
        "            \n",
        "\n",
        "            for candidateTuple in entities_from_sentence:\n",
        "                candidateText, positions = candidateTuple\n",
        "                candidateText=(((candidateText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
        "                candidateText=(self.remAmpersand(candidateText).lstrip('“‘’”')).rstrip('“‘’”')\n",
        "                candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "                candidateText= candidateText.strip()\n",
        "                self.set_stopword_exceptions(candidateText.split())\n",
        "                just_candidates.append(candidateText)\n",
        "                # if(index==9423):\n",
        "                #     print(candidateText)\n",
        "                position = '*'+'*'.join(str(v) for v in positions)\n",
        "                position=position+'*'\n",
        "\n",
        "                phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
        "\n",
        "                combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
        "                if not ((candidateText in combined)|(len(candidateText)<=1)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
        "                    if(self.quickRegex.match(candidateText)):\n",
        "                        self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
        "            \n",
        "            # print('entities_from_sentence:',just_candidates)\n",
        "            #storing the outputs and token embeddings in dataframe tweetID,sen_index\n",
        "            dict1 = {'tweetID':str(tweetID), 'sentID':str(sentID), 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList,'phase1Candidates': just_candidates ,'phase1CandidatesWPositions':phase1Out,\n",
        "                        'contextual_embeddings':entity_aware_embeddings,\n",
        "                        'start_time':now,'entry_batch':batch_number}\n",
        "            df_holder.append(dict1)\n",
        "                    # self.contextual_embeddings[(tweetID,sen_index)] = entity_aware_embeddings\n",
        "\n",
        "        time_out=time.time()\n",
        "        self.append_rows(df_holder)\n",
        "        self.phase2stopWordList=list(set(self.phase2stopWordList)|self.swSet)\n",
        "        self.counter=self.counter+1\n",
        "        \n",
        "        #return (copy.deepcopy(self.df_out),copy.deepcopy(freqs),time_in,time_out)\n",
        "        return (self.df_out,self.contextual_embeddings,self.CTrie,time_in,time_out,self.phase2stopWordList,self.tweet_to_sentences_w_annotation)\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        self.df_out=self.df_out.append(df)\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QiYJ45QEfUB"
      },
      "source": [
        "## **Phase II: Global NER with Phrase Embedder to collect the Entity Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cU04h9cEo51"
      },
      "source": [
        "# coding: utf-8\n",
        "from nltk.corpus import stopwords\n",
        "import pandas  as pd\n",
        "# import NE_candidate_module as ne\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "import copy\n",
        "# import numpy\n",
        "import math\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from scipy import stats\n",
        "import emoji\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import trie as trie\n",
        "import re\n",
        "import ast\n",
        "import pickle\n",
        "import itertools\n",
        "from scipy import spatial\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.cluster import KMeans, MeanShift\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import torch\n",
        "\n",
        "#CLASSIFIER CLASS\n",
        "# import SVM as svm\n",
        "# import entityClassifier as entityClassifier\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"since\",\"hence\",\"onto\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"us\")\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"&;\",\"v.\"] #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"httpurl\",\"n't\",\"&amp;\",\"gee\",\"€\",\"hmm\",\"bye\",\"pls\",\"please\",\"yrs\",\"4get\",\"blah\",\"ooh\",\"ouch\",\"am\",\"tv\",\"ima\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"qft\",\"ip\",\"icymi\",\"bdsm\",\"ah\",\"http\",\"https\",\"pm\",\"omw\",\"pts\",\"pt\",\"ive\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "\n",
        "class GlobalNERModule():\n",
        "\n",
        "\n",
        "    def executor(self,max_batch_value,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others):\n",
        "    # def executor(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others)\n",
        "\n",
        "\n",
        "        # SET CB\n",
        "        # print(phase2stopwordList)\n",
        "        candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag = self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold)\n",
        "        candidate_featureBase_DF.to_csv(\"data/candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        # print(self.good_candidates)\n",
        "\n",
        "        # SET TF \n",
        "        untrashed_tweets=self.set_tf(data_frame_holder, candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "        #mark incomplete tweets\n",
        "        self.set_column_for_candidates_in_incomplete_tweets(candidate_featureBase_DF,untrashed_tweets)\n",
        "\n",
        "        # SAVE INCOMING TWEETS FOR ANNOTATION FOR OTHERS\n",
        "        # self.raw_tweets_for_others=pd.concat([self.raw_tweets_for_others,raw_tweets_for_others ])\n",
        "\n",
        "        # DROP TF\n",
        "        just_converted_tweets=self.get_complete_tf(untrashed_tweets)\n",
        "\n",
        "        #incomplete tweets at the end of current batch\n",
        "        incomplete_tweets=self.get_incomplete_tf(untrashed_tweets)\n",
        "\n",
        "        #all incomplete_tweets---> incomplete_tweets at the end of current batch + incomplete_tweets not reintroduced\n",
        "        # self.incomplete_tweets=incomplete_tweets #without reintroduction--- when everything is reintroduced, just incomplete_tweets\n",
        "        # self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized','annotation','stanford_candidates'])\n",
        "        # self.incomplete_tweets=pd.concat([incomplete_tweets,self.not_reintroduced],ignore_index=True)\n",
        "        self.incomplete_tweets=pd.concat([incomplete_tweets],ignore_index=True)\n",
        "\n",
        "        print('completed tweets:',len(just_converted_tweets))\n",
        "        print('incomplete tweets:',len(incomplete_tweets))\n",
        "\n",
        "\n",
        "        # #recording tp, fp , f1\n",
        "        # #self.accuracy_tuples_prev_batch.append((just_converted_tweets.tp.sum(), just_converted_tweets.total_mention.sum(),just_converted_tweets.fp.sum(),just_converted_tweets.fn.sum()))\n",
        "\n",
        "\n",
        "        # #operations for getting ready for next batch.\n",
        "        # # self.incomplete_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "        # self.incomplete_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "        # self.counter=self.counter+1\n",
        "\n",
        "        self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets)\n",
        "\n",
        "        time_out=time.time()\n",
        "\n",
        "        self.calculate_tp_fp_f1(z_score_threshold,untrashed_tweets,raw_tweets_for_others)\n",
        "\n",
        "        if(self.counter==(max_batch_value+1)):\n",
        "            # self.just_converted_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "            self.just_converted_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "\n",
        "            print('completed tweets: ', len(self.just_converted_tweets),'incomplete tweets: ', len(self.incomplete_tweets))\n",
        "            \n",
        "            print(len(list(self.just_converted_tweets.columns.values)))\n",
        "            print(len(list(self.incomplete_tweets.columns.values)))\n",
        "\n",
        "            combined_frame_list=[self.just_converted_tweets, self.incomplete_tweets]\n",
        "            complete_tweet_dataframe = pd.concat(combined_frame_list)\n",
        "\n",
        "            print('final tally: ', (len(self.just_converted_tweets)+len(self.incomplete_tweets)), len(complete_tweet_dataframe))\n",
        "\n",
        "            #to groupby tweetID and get one tuple per tweetID\n",
        "            complete_tweet_dataframe_grouped_df= (complete_tweet_dataframe.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "            complete_tweet_dataframe_grouped_df['tweetID']=complete_tweet_dataframe_grouped_df['tweetID'].astype(int)\n",
        "            self.complete_tweet_dataframe_grouped_df_sorted=(complete_tweet_dataframe_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "            print(list(self.complete_tweet_dataframe_grouped_df_sorted.columns.values))\n",
        "\n",
        "\n",
        "        #self.aggregator_incomplete_tweets.to_csv(\"all_incompletes.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "\n",
        "        #self.just_converted_tweets.to_csv(\"all_converteds.csv\", sep=',', encoding='utf-8')\n",
        "        #self.incomplete_tweets.to_csv(\"incomplete_for_last_batch.csv\", sep=',', encoding='utf-8')\n",
        "        return candidate_featureBase_DF, self.complete_tweet_dataframe_grouped_df_sorted,time_out \n",
        "        # return candidate_featureBase_DF, untrashed_tweets,time_out\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,entity_phrase_embedder,device,filename):\n",
        "        self.counter=0\n",
        "        self.decay_factor=2**(-1/2)\n",
        "        self.decay_base_staggering=2\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "        self.device=device\n",
        "        self.save_file= filename\n",
        "\n",
        "        self.entity_phrase_embedder = entity_phrase_embedder\n",
        "\n",
        "        context_feature_list=['cf_'+str(i) for i in range(100)]\n",
        "        self.candidateBaseHeaders=['candidate', 'batch', 'length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+context_feature_list+['cumulative']\n",
        "\n",
        "        ## When not running on a notebook\n",
        "        # self.entity_classifier = entityClassifier.EntityClassifier('data/candidate_train_records_100d.csv',True,self.device)\n",
        "\n",
        "        ################################## To do a fresh training\n",
        "        self.entity_classifier = EntityClassifier('data/candidate_train_records_large_100d.csv',True,self.device)\n",
        "\n",
        "        ################################### To Load a pre-trained model\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_100d.csv',False,self.device)\n",
        "\n",
        "        self.complete_tweet_dataframe_grouped_df_sorted=pd.DataFrame([], columns=['tweetID', 'TweetSentence', 'ambiguous_candidates', 'annotation', 'candidates_with_label', 'completeness', 'current_minus_entry', 'entry_batch', 'hashtags', 'index', 'only_good_candidates', 'output_mentions', 'phase1Candidates', 'sentID', 'stanford_candidates', 'user'])\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1_generic(self,raw_tweets_for_others,state_of_art):\n",
        "\n",
        "        column_candidates_holder = raw_tweets_for_others[state_of_art].tolist()\n",
        "        \n",
        "\n",
        "        column_annot_holder= raw_tweets_for_others['mentions_other'].tolist()\n",
        "        # column_annot_holder= raw_tweets_for_others['annotation_limited types'].tolist()\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=column_annot_holder[idx].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=ast.literal_eval(column_candidates_holder[idx])\n",
        "            else:\n",
        "                output_mentions_list=column_candidates_holder[idx].split(',')\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            # print(annotated_mention_list,output_mentions_list)\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall))    \n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "        # tweet_ids_df[\"tp\"+state_of_art]=true_positive_holder\n",
        "        # tweet_ids_df[\"fn\"+state_of_art]=false_negative_holder\n",
        "        # tweet_ids_df['fp'+state_of_art]= false_positive_holder\n",
        "        \n",
        "        # if(state_of_art==\"ritter_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"ritter_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # if(state_of_art==\"stanford_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"stanford_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "    def calculate_tp_fp_f1_alternate(self,raw_tweets_for_others, state_of_art):\n",
        "        if(state_of_art=='neuroner'):\n",
        "            # fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/venezuela_input_2019-05-10_12-33-16-15380/mentions_output.txt\",\"r\")\n",
        "            fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/tweets_3K_input_2019-04-26_16-49-32-20455/mentions_output.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='stanford_candidates'):\n",
        "            fp= open(\"/home/satadisha/Desktop/stanford-ner-2016-10-31/stanford_venezuela_mentions.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='ritter_candidates'):\n",
        "            tweets_ritter=pd.read_csv(\"/home/satadisha/Desktop/GitProjects/twitter_nlp-master/ritter-venezuela-output.csv\",sep =',', keep_default_na=False)\n",
        "        if(state_of_art=='calai_candidates'):\n",
        "            tweets_calai=pd.read_csv(\"/home/satadisha/Desktop/opencalai_versions/venezuela_output.csv\",sep =',', keep_default_na=False)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        for index, row in raw_tweets_for_others.iterrows():\n",
        "            \n",
        "\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=row['mentions_other'].split(';')\n",
        "            # tweet_level_candidate_list=row['annotation_limited types'].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='ritter_candidates'):\n",
        "                # output_mentions_list=ast.literal_eval(mentions_list[idx])\n",
        "                output_mentions_list=tweets_ritter[tweets_ritter['ID']==row['ID']]['Output'].iloc[0].split(',')\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=tweets_calai[tweets_calai['ID']==row['ID']]['calai_candidates'].iloc[0].split(',')\n",
        "            if((state_of_art=='neuroner')|(state_of_art=='stanford_candidates')):\n",
        "                #for 3k Tweets:\n",
        "                idx=int(row['ID'])\n",
        "\n",
        "                # #for others:\n",
        "                # idx=int(row['ID']-1)\n",
        "\n",
        "                output_mentions_list=mentions_list[idx].split(',')\n",
        "\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            print(annotated_mention_list,output_mentions_list)\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall)) \n",
        "        if(state_of_art==\"neuroner\"):\n",
        "            self.accuracy_vals_neuroner.append((f_measure,precision,recall))\n",
        "\n",
        "\n",
        "        # output_mentions_list= mentions_list[output_index].split(',')\n",
        "#     # output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "#     # output_mentions_list=list(filter(lambda element: element !='', output_mentions_list))\n",
        "\n",
        "    def calculate_tp_fp_f1_for_others(self,raw_tweets_for_others):\n",
        "\n",
        "        opencalai=\"calai_candidates\"\n",
        "        stanford=\"stanford_candidates\"\n",
        "        ritter=\"ritter_candidates\"\n",
        "        neuroner=\"neuroner\"\n",
        "\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,opencalai)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,stanford)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,ritter)\n",
        "\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,opencalai)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,stanford)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,ritter)\n",
        "\n",
        "        self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,neuroner)\n",
        "\n",
        "    #################################\n",
        "    #input candidate_feature_Base\n",
        "    #output candidate_feature_Base with [\"Z_score\"], [\"probability\"],[\"class\"]\n",
        "    #################################\n",
        "    def classify_candidate_base(self,z_score_threshold,candidate_featureBase_DF):\n",
        "\n",
        "        # #filtering test set based on z_score\n",
        "        # mert1=candidate_featureBase_DF['cumulative'].as_matrix()\n",
        "        #frequency_array = np.array(list(map(lambda val: val[0], sortedCandidateDB.values())))\n",
        "        # zscore_array1=stats.zscore(mert1)\n",
        "\n",
        "        zscore_array1=stats.zscore(candidate_featureBase_DF['cumulative'])\n",
        "\n",
        "        candidate_featureBase_DF['Z_ScoreUnweighted']=zscore_array1\n",
        "        cumulative_threshold=1.0 #set threshold here\n",
        "        z_score_threshold=candidate_featureBase_DF[candidate_featureBase_DF['cumulative']==cumulative_threshold].Z_ScoreUnweighted.tolist()[0]\n",
        "        print(cumulative_threshold,z_score_threshold)\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_new_with_z_score.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        #multi-word infrequent candidates ---> to be used for recall correction\n",
        "        infrequent_candidates=candidate_featureBase_DF[(candidate_featureBase_DF['Z_ScoreUnweighted'] < z_score_threshold) & (candidate_featureBase_DF.length>1)].candidate.tolist()\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF[candidate_featureBase_DF['Z_ScoreUnweighted'] >= z_score_threshold]\n",
        "\n",
        "        # # #######################with one unified classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        return (self.entity_classifier.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with only semantic classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # return (self.entity_classifierII.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with two separate classifiers--- requires some additional lines of code\n",
        "        # candidateList = candidate_featureBase_DF.candidate.tolist()\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # candidate_featureBase_DF.set_index(\"candidate\", inplace=True)\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF.index.name)\n",
        "\n",
        "        # # returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # candidate_featureBase_DF_classifierI = self.entity_classifierI.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "        # candidate_featureBase_DF_classifierII = self.entity_classifierII.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "\n",
        "        # # candidate_featureBase_DF_classifierI.to_csv('classifierI.csv', sep=',', encoding='utf-8')\n",
        "        # # candidate_featureBase_DF_classifierII.to_csv('classifierII.csv', sep=',', encoding='utf-8') #candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # final_probability_dict = {}\n",
        "        # print5=0\n",
        "        # for candidate in candidateList:\n",
        "        #     prob1 = candidate_featureBase_DF_classifierI.loc[candidate]['probability']\n",
        "        #     prob2 = candidate_featureBase_DF_classifierII.loc[candidate]['probability']\n",
        "        #     final_probability = max(prob1,prob2)\n",
        "        #     if(print5<5):\n",
        "        #         print(candidate,prob1,prob2,final_probability)\n",
        "        #         print5+=1\n",
        "        #     final_probability_dict[candidate] = final_probability\n",
        "\n",
        "        # candidate_featureBase_DF[\"probability\"] = pd.Series(final_probability_dict)\n",
        "\n",
        "        # candidate_featureBase_DF.reset_index(drop=False,inplace=True)\n",
        "        # print('after columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF[['candidate','probability']].head(5))\n",
        "\n",
        "        # return (candidate_featureBase_DF,infrequent_candidates)\n",
        "\n",
        "    # recall_correction\n",
        "    def set_partition_dict(self,candidate_featureBase_DF,infrequent_candidates):\n",
        "\n",
        "        #print(list(self.partition_dict.keys()))\n",
        "        ambiguous_bad_candidates=candidate_featureBase_DF[(((candidate_featureBase_DF.status==\"a\")|(candidate_featureBase_DF.status==\"b\"))&(candidate_featureBase_DF.length.astype(int)>1))]\n",
        "        good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "        flag1=False\n",
        "        flag2=False\n",
        "        if(len(ambiguous_bad_candidates)>0):\n",
        "            # ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1) \n",
        "            # ambiguous_bad_candidates_wFilter= ambiguous_bad_candidates[ambiguous_bad_candidates.max_column=='substring-cap']\n",
        "\n",
        "            #good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "            #print(ambiguous_bad_candidates_wFilter.candidate.tolist())\n",
        "\n",
        "            # for candidate in ambiguous_bad_candidates_wFilter.candidate.tolist():\n",
        "            for candidate in ambiguous_bad_candidates.candidate.tolist():\n",
        "                \n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "\n",
        "            flag1= True\n",
        "        if(len(infrequent_candidates)>0):\n",
        "            #print(len(ambiguous_bad_candidates_wFilter.candidate.tolist()))\n",
        "\n",
        "            for candidate in infrequent_candidates:\n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "            flag2= True\n",
        "        print(list(self.partition_dict.keys()))\n",
        "        return (flag1|flag2)\n",
        "\n",
        "    def get_reintroduced_tweets(self, reintroduction_threshold):\n",
        "        #no reintroduction\n",
        "        #no preferential selection\n",
        "        print(\"incomplete tweets in batch: \",len(self.incomplete_tweets))\n",
        "        # print(list(self.incomplete_tweets.columns.values))\n",
        "\n",
        "        reintroduced_tweets=self.incomplete_tweets[(self.counter-self.incomplete_tweets['entry_batch'])<=reintroduction_threshold]\n",
        "        self.not_reintroduced=self.incomplete_tweets[~self.incomplete_tweets.index.isin(reintroduced_tweets.index)]\n",
        "\n",
        "        print(\"reintroduced tweets: \",len(reintroduced_tweets))\n",
        "        # for i in range(self.counter):\n",
        "        #     print('i:',len(self.incomplete_tweets[self.incomplete_tweets['entry_batch']==i]))\n",
        "        return reintroduced_tweets\n",
        "\n",
        "    def set_cb(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold):\n",
        "\n",
        "        #input new_tweets, z_score, Updated candidatebase of phase1\n",
        "        #output candidate_featureBase_DF, Incomplete_tweets\n",
        "        data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "\n",
        "        candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted= self.extract(TweetBase,CTrie,phase2stopwordList,0)\n",
        "        phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "        phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "        df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        if((self.counter>0)&(len(self.incomplete_tweets)>0)):\n",
        "\n",
        "            #tweet candidates for Reintroduction\n",
        "            reintroduced_tweets=self.get_reintroduced_tweets(reintroduction_threshold)\n",
        "            candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted = self.extract(reintroduced_tweets,CTrie,phase2stopwordList,1)\n",
        "            phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "            phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "            df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        #print(len(df_holder))\n",
        "        data_frame_holder = pd.DataFrame(df_holder)\n",
        "        #print(len(self.incomplete_tweets),len(data_frame_holder),len(candidate_featureBase_DF))\n",
        "        \n",
        "\n",
        "        #set ['probabilities'] for candidate_featureBase_DF\n",
        "        candidate_featureBase_DF,self.infrequent_candidates= self.classify_candidate_base(z_score_threshold,candidate_featureBase_DF)\n",
        "\n",
        "        # set readable labels (a,g,b) for candidate_featureBase_DF based on ['probabilities.']\n",
        "        candidate_featureBase_DF=self.set_readable_labels(candidate_featureBase_DF)\n",
        "\n",
        "        self.good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        self.ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "        self.bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        print('good_candidates:',self.good_candidates)\n",
        "        print('bad_candidates:',self.bad_candidates)\n",
        "        print('ambiguous_candidates:',self.ambiguous_candidates)\n",
        "\n",
        "        # entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.good_candidates)]\n",
        "        # non_entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.bad_candidates)]\n",
        "        # ambiguous_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.ambiguous_candidates)]\n",
        "\n",
        "        correction_flag=self.set_partition_dict(candidate_featureBase_DF,self.infrequent_candidates)\n",
        "\n",
        "        ambiguous_turned_good=[]\n",
        "        ambiguous_turned_bad=[]\n",
        "        ambiguous_remaining_ambiguous=[]\n",
        "        converted_candidates=[]\n",
        "\n",
        "        #['probability'],['a,g,b']\n",
        "        return candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag\n",
        "\n",
        "\n",
        "        #flush out completed tweets\n",
        "        # input candidate base, looped over tweets (incomplete tweets+ new tweets)\n",
        "        # output: incomplete tweets (a tags in it.), incomplete_tweets[\"Complete\"]\n",
        "    def set_tf(self,data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        return self.set_completeness_in_tweet_frame(data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "    def get_incomplete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==False]\n",
        "\n",
        "    def get_complete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==True]\n",
        "\n",
        "    def compute_seen_tweets_so_far(self,start_batch,end_batch):\n",
        "        if(start_batch==end_batch):\n",
        "            sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch]\n",
        "\n",
        "\n",
        "        sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch:]\n",
        "\n",
        "\n",
        "        counter=0\n",
        "        for elem in sliced_seen_tweets:\n",
        "            counter=counter+elem\n",
        "\n",
        "        return counter\n",
        "\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "    #ME_EXTR=Mention.Mention_Extraction()\n",
        "\n",
        "\n",
        "    # experiment function\n",
        "    def set_x_axis(self,just_converted_tweets_for_current_batch):\n",
        "\n",
        "        self.incomplete_tweets.to_csv(\"set_x_axis_debug.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        self.incomplete_tweets['number_of_seen_tweets'] = self.incomplete_tweets['entry_batch'].apply(lambda x: self.compute_seen_tweets_so_far(x,self.counter))\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"entry_vs_tweet_seen_ratio\"]=self.incomplete_tweets['entry_batch']/self.incomplete_tweets['number_of_seen_tweets']\n",
        "\n",
        "\n",
        "        #counter_list= \n",
        "        self.incomplete_tweets[\"ratio_entry_vs_current\"]=self.incomplete_tweets['entry_batch']/self.counter\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"current_minus_entry\"]=self.counter-self.incomplete_tweets['entry_batch']\n",
        "\n",
        "        just_converted_tweets_for_current_batch[\"current_minus_entry\"]=self.counter-just_converted_tweets_for_current_batch['entry_batch']\n",
        "\n",
        "        return just_converted_tweets_for_current_batch\n",
        "\n",
        "\n",
        "\n",
        "    def set_column_for_candidates_in_incomplete_tweets(self,candidate_featureBase_DF,input_to_eval):\n",
        "\n",
        "        incomplete_candidates= input_to_eval['2nd Iteration Candidates'].tolist()\n",
        "\n",
        "        candidate_featureBase_DF= candidate_featureBase_DF.set_index('candidate')\n",
        "\n",
        "        candidate_with_label_holder=[]\n",
        "        one_level=[]\n",
        "        \n",
        "\n",
        "        for sentence_level_candidates in incomplete_candidates:\n",
        "\n",
        "            one_level.clear()\n",
        "\n",
        "            for candidate in sentence_level_candidates:\n",
        "                if candidate.lower() in candidate_featureBase_DF.index:\n",
        "                    # label=candidate_featureBase_DF.get_value(candidate.lower(),'status')\n",
        "                    label=candidate_featureBase_DF.at[candidate.lower(),'status']\n",
        "                    one_level.append((candidate,label))\n",
        "                else:\n",
        "                    one_level.append((candidate,\"na\"))\n",
        "\n",
        "            candidate_with_label_holder.append(copy.deepcopy(one_level))\n",
        "\n",
        "\n",
        "        input_to_eval[\"candidates_with_label\"]=candidate_with_label_holder\n",
        "        debug_candidates_label_list= input_to_eval['candidates_with_label'].tolist()\n",
        "        candidates_filtered_g_labeled=[]\n",
        "        row_level_candidates=[]\n",
        "        index_outer=0\n",
        "\n",
        "        candidates_filtered_a_labeled=[]\n",
        "        row_level_a_candidates=[]\n",
        "\n",
        "        for sentence_level in debug_candidates_label_list:\n",
        "\n",
        "            # sentence_level_candidates_unnormalized= incomplete_candidates_unnormalized[index_outer]\n",
        "            row_level_candidates.clear()\n",
        "            row_level_a_candidates.clear()\n",
        "            for candidate in sentence_level:\n",
        "                if(candidate[1]==\"g\"):\n",
        "                    candidate_str = self.erode_article(candidate[0])\n",
        "                    row_level_candidates.append(candidate_str)\n",
        "                if(((candidate[1]==\"b\")|(candidate[1]==\"a\"))&(candidate[0]==\"US\")):\n",
        "                    # print('here')\n",
        "                    row_level_candidates.append(candidate[0])\n",
        "                if(candidate[1]==\"a\"):\n",
        "                    row_level_a_candidates.append(candidate[0])\n",
        "\n",
        "            candidates_filtered_g_labeled.append(copy.deepcopy(row_level_candidates))\n",
        "            candidates_filtered_a_labeled.append(copy.deepcopy(row_level_a_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "\n",
        "        input_to_eval[\"only_good_candidates\"]=candidates_filtered_g_labeled\n",
        "        input_to_eval[\"ambiguous_candidates\"]=candidates_filtered_a_labeled\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1(self,z_score_threshold,input_to_eval,raw_tweets_for_others):\n",
        "\n",
        "        input_to_eval_grouped_df= (input_to_eval.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "        input_to_eval_grouped_df['tweetID']=input_to_eval_grouped_df['tweetID'].astype(int)\n",
        "        input_to_eval_df_sorted=(input_to_eval_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "\n",
        "        print(len(input_to_eval_df_sorted),len(raw_tweets_for_others))\n",
        "        \n",
        "        print(set(input_to_eval_df_sorted['tweetID'].values.tolist())-set(raw_tweets_for_others['ID'].values.tolist()))\n",
        "\n",
        "        input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_other'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_limited_types'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['annotation_limited types'].iloc[0])\n",
        "\n",
        "        column_candidates_holder = input_to_eval_df_sorted['only_good_candidates'].tolist()\n",
        "        # column_candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "\n",
        "        file1 = open(self.save_file+\".txt\", \"w\")\n",
        "        \n",
        "\n",
        "        column_annot_holder= input_to_eval_df_sorted['annotation'].tolist()\n",
        "        \n",
        "        output_str=''\n",
        "        # print(column_candidates_holder)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "        total_annotation=0\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        print('=========================Gaguilar_candidates')\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            output_mentions_list=[]\n",
        "            tweet_level_candidate_list=str(column_annot_holder[idx]).split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: quickRegex.match(element), annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: ((element !='')&(element !='nan')), annotated_mention_list))\n",
        "\n",
        "            for lst in column_candidates_holder[idx]:\n",
        "                output_mentions_list.extend(lst)\n",
        "\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "            output_mentions_list=list(map(lambda element: element.lower(), output_mentions_list))\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            print(idx, annotated_mention_list,output_mentions_list)\n",
        "            output_str+=','.join(output_mentions_list)+'\\n'\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= self.normalize(annotated_mention_list.pop())\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "            self.true_positive_count+=tp_counter_inner\n",
        "            self.false_positive_count+=fp_counter_inner\n",
        "            self.false_negative_count+=fn_counter_inner\n",
        "\n",
        "            print(self.true_positive_count,self.false_positive_count,self.false_negative_count)\n",
        "\n",
        "        print('TP||||FP||||FN')\n",
        "        print(self.true_positive_count,self.false_positive_count,self.false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        precision=(self.true_positive_count)/(self.true_positive_count+self.false_positive_count)\n",
        "        recall=(self.true_positive_count)/(self.true_positive_count+self.false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "        \n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "\n",
        "        # true_positive_count_IPQ=true_positive_count\n",
        "        # false_positive_count_IPQ = false_positive_count\n",
        "        # false_negative_count_IPQ= false_negative_count\n",
        "        # total_mention_count_IPQ=total_mentions\n",
        "\n",
        "\n",
        "        # tp_count=0\n",
        "        # tm_count=0\n",
        "        # fp_count=0\n",
        "        # fn_count=0\n",
        "\n",
        "        # for idx,tup in enumerate(self.accuracy_tuples_prev_batch):\n",
        "        #     # print(idx,tup)\n",
        "        #     tp_count+=tup[0]\n",
        "        #     tm_count+=tup[1]\n",
        "        #     fp_count+=tup[2]\n",
        "        #     fn_count+=tup[3]\n",
        "\n",
        "\n",
        "\n",
        "        # tp_count+=true_positive_count_IPQ\n",
        "        # tm_count+=total_mention_count_IPQ\n",
        "        # fp_count+=false_positive_count_IPQ\n",
        "        # fn_count+=false_negative_count_IPQ\n",
        "\n",
        "        # precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        # recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        # f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        file1.write(output_str)\n",
        "        file1.close()\n",
        "\n",
        "\n",
        "\n",
        "        self.accuracy_vals=(z_score_threshold,f_measure,precision,recall)\n",
        "\n",
        "        print('Precision:',precision)\n",
        "        print('Recall:',recall)\n",
        "        print('F1:',f_measure)\n",
        "\n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "\n",
        "        # input_to_eval[\"tp\"]=true_positive_holder\n",
        "        # input_to_eval[\"fn\"]=false_negative_holder\n",
        "        # input_to_eval['fp']= false_positive_holder\n",
        "        # input_to_eval[\"total_mention\"]=total_mention_holder\n",
        "\n",
        "        # input_to_eval[\"ambigious_not_in_annot\"]=ambigious_not_in_annotation_holder\n",
        "        # input_to_eval[\"inverted_loss\"]=input_to_eval[\"tp\"]/( input_to_eval[\"fn\"]+input_to_eval[\"ambigious_not_in_annot\"])\n",
        "\n",
        "        return input_to_eval\n",
        "\n",
        "\n",
        "    def recall_correction(self,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder):\n",
        "\n",
        "        corrected_phase2_candidates_holder=[]\n",
        "        index_outer=0\n",
        "        for candidates in phase2_candidates_holder:\n",
        "            unnormalized_candidates=phase2_unnormalized_candidates_holder[index_outer]\n",
        "            corrected_phase2_candidates=[]\n",
        "            for idx, candidate in enumerate(candidates):\n",
        "                unnormalized_candidate=unnormalized_candidates[idx]\n",
        "                # if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates))):\n",
        "                if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))):   #do this only for 3K tweets\n",
        "                    #print(candidate, self.partition_dict[candidate])\n",
        "                    corrected_phase2_candidates.extend(self.partition_dict[candidate])\n",
        "                else:\n",
        "                    if(((candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))&(candidate=='us')&(unnormalized_candidate=='US')):\n",
        "                        # print(index_outer)\n",
        "                        candidate=unnormalized_candidate\n",
        "                    # if((len(candidate.strip().strip(string.punctuation).split())>1)&(candidate.strip().strip(string.punctuation).split()[0].lower() in ['a','an','the'])):\n",
        "                    #     candidate = (' '.join(candidate.strip().strip(string.punctuation).split()[1:])).strip()\n",
        "                    corrected_phase2_candidates.append(candidate)\n",
        "            corrected_phase2_candidates_holder.append(copy.deepcopy(corrected_phase2_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "        \n",
        "        #print(corrected_phase2_candidates_holder)\n",
        "        data_frame_holder['2nd Iteration Candidates']=corrected_phase2_candidates_holder\n",
        "\n",
        "        return corrected_phase2_candidates_holder,data_frame_holder                  \n",
        "\n",
        "\n",
        "    def erode_article(self, entity_string):\n",
        "        if((len(entity_string.strip().strip(string.punctuation).split())>1)&(entity_string.lower().strip().strip(string.punctuation).split()[0] in ['a','an','the'])):\n",
        "            # print(entity_string)\n",
        "            entity_string = ' '.join(entity_string.strip().strip(string.punctuation).split()[1:])\n",
        "            # print(entity_string)\n",
        "        return entity_string.strip()\n",
        "\n",
        "\n",
        "    def set_completeness_in_tweet_frame(self,data_frame_holder,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        #print(candidate_featureBase_DF.head())\n",
        "        good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        merged_g_b= bad_candidates+good_candidates\n",
        "\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_before_labeling_comp.csv\", sep=',', encoding='utf-8')\n",
        "        ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "\n",
        "        if(correction_flag):\n",
        "            phase2_candidates_holder,data_frame_holder=self.recall_correction(candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder)\n",
        "\n",
        "         \n",
        "\n",
        "        \n",
        "        truth_vals=[False if any(x not in merged_g_b for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        intermediate_output_mentions=[list(filter(lambda candidate: ((candidate in good_candidates))|(candidate=='US'), list1)) for list1 in phase2_candidates_holder]\n",
        "\n",
        "        output_mentions=[list(map(lambda candidate: self.erode_article(candidate), list1)) for list1 in intermediate_output_mentions]\n",
        "\n",
        "        # truth_vals=[False if any(x in ambiguous_candidates for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        # for list1 in phase2_candidates_holder:\n",
        "        #     if any(x in ambiguous_candidates  for x in list1):\n",
        "        #         truth_vals.append(False)\n",
        "        #     else:\n",
        "        #         truth_vals.append(True)\n",
        " \n",
        "\n",
        "\n",
        "        #print(truth_vals)\n",
        "        completeness_series = pd.Series( (v for v in truth_vals) )\n",
        "        output_mentions_series = pd.Series( (v for v in output_mentions) )\n",
        "\n",
        "\n",
        "        data_frame_holder['output_mentions']=output_mentions_series\n",
        "        data_frame_holder['completeness']=completeness_series\n",
        "        data_frame_holder[\"current_minus_entry\"]=self.counter-data_frame_holder['entry_batch']\n",
        "\n",
        "        return data_frame_holder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_readable_labels(self,candidate_featureBase_DF):\n",
        "\n",
        "        #candidate_featureBase_DF['status'] = candidate_featureBase_DF['probability'].apply(lambda x: set(x).issubset(good_candidates))\n",
        "        candidate_featureBase_DF['status']='ne'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.50]='g'\n",
        "        candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.50)] = 'a'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
        "\n",
        "        return candidate_featureBase_DF\n",
        "\n",
        "\n",
        "\n",
        "    def normalize(self,word):\n",
        "        strip_op=word\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        if strip_op.endswith(\"'s\"):\n",
        "            li = strip_op.rsplit(\"'s\", 1)\n",
        "            return ''.join(li)\n",
        "        elif strip_op.endswith(\"’s\"):\n",
        "            li = strip_op.rsplit(\"’s\", 1)\n",
        "            return ''.join(li)\n",
        "        else:\n",
        "            return strip_op\n",
        "        #return strip_op\n",
        "\n",
        "    \n",
        "    def isSubstring(self,to_increase_element,id_to_incr,comparison_holder,phase1_holder_holder_copy):\n",
        "        combined_list=comparison_holder[id_to_incr]+phase1_holder_holder_copy[id_to_incr]\n",
        "\n",
        "        for idx,val in enumerate(comparison_holder[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[5]) and to_increase_element[5] != val[5]):\n",
        "                    return True\n",
        "        for idx,val in enumerate(phase1_holder_holder_copy[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[2]) and to_increase_element[5] != val[2]):\n",
        "                    return True   \n",
        "                \n",
        "        return False\n",
        "\n",
        "\n",
        "    def calculate_pmi(self,big,x1,x2,total):\n",
        "        big__= float(big/total)\n",
        "        x1__=float(x1/total)\n",
        "        x2__=float(x2/total)\n",
        "        pmi= math.log(big__/(x1__*x2__),2.71828182845)\n",
        "        pklv=big__*pmi\n",
        "        #return (1/(1+math.exp(-1*pmi)))\n",
        "        npmi= pmi/(-1.0*(math.log(big__,2.71828182845)))\n",
        "        return npmi,pklv\n",
        "        #return pklv\n",
        "\n",
        "    def multiSlice(self,s,cutpoints,good_candidates):\n",
        "        k = len(cutpoints)\n",
        "        multislices=[]\n",
        "        if k == 0:\n",
        "            curr_candidate=self.normalize(' '.join(s))\n",
        "\n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]        \n",
        "        else:\n",
        "            \n",
        "            curr_candidate=self.normalize(' '.join(s[:cutpoints[0]]))\n",
        "            alt_list=[curr_candidate]\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]\n",
        "\n",
        "            alt_list.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1))\n",
        "            multislices.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1) if self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) in good_candidates)\n",
        "\n",
        "            curr_candidate=self.normalize(' '.join(s[cutpoints[k-1]:]))\n",
        "            alt_list.append(curr_candidate)\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices.append(curr_candidate)\n",
        "            # print('::',alt_list)\n",
        "        return multislices\n",
        "\n",
        "\n",
        "\n",
        "    def get_substring_candidates(self,candidate_words,good_candidates):\n",
        "        n = len(candidate_words)\n",
        "        all_partitions=[]\n",
        "        all_partitions_length=[]\n",
        "        cuts = list(range(1,n))\n",
        "        for k in range(n):\n",
        "            # all_partitions_inner=[]\n",
        "            partition_list=[]\n",
        "            partition_length_list=[]\n",
        "            for cutpoints in itertools.combinations(cuts,k):\n",
        "                ret_list=self.multiSlice(candidate_words,cutpoints,good_candidates)\n",
        "                if(ret_list):\n",
        "                    partition_length=sum([len(elem.split()) for elem in ret_list])\n",
        "                    # print('==',ret_list,partition_length)\n",
        "                    if(partition_length==len(candidate_words)):\n",
        "                        return ret_list\n",
        "                    partition_list.append(ret_list)\n",
        "                    partition_length_list.append(partition_length)\n",
        "                    # yield ret_list\n",
        "            # print('------')\n",
        "            if(partition_length_list):\n",
        "                max_index=partition_length_list.index(max(partition_length_list))\n",
        "                all_partitions.append(partition_list[max_index])\n",
        "                all_partitions_length.append(partition_length_list[max_index])\n",
        "        # print(all_partitions)\n",
        "        if(all_partitions_length):\n",
        "            max_index=all_partitions_length.index(max(all_partitions_length))\n",
        "            # print(all_partitions[max_index])\n",
        "            return all_partitions[max_index]\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "\n",
        "    def verify(self, subsequence, CTrie):\n",
        "        return CTrie.__contains__(subsequence)\n",
        "\n",
        "\n",
        "\n",
        "    def check_sequence(self, sequence, l, CTrie):\n",
        "        result=[]\n",
        "        subsequence_length=l\n",
        "        while(subsequence_length>0):\n",
        "            shift=len(sequence)-subsequence_length\n",
        "            verified_subsequence=[]\n",
        "            verified=False\n",
        "            for i in range(0,shift+1):\n",
        "                list1=sequence[i:(i+subsequence_length)]\n",
        "                text=' '.join(str(e[0]) for e in list1)\n",
        "                subsequence=(self.normalize(text)).split()\n",
        "                #print(\"search for\", subsequence)\n",
        "                if self.verify(subsequence, CTrie):\n",
        "                    verified_subsequence.append(i)\n",
        "                    verified_subsequence.append(i+subsequence_length)\n",
        "                    #print(subsequence)\n",
        "                    #print(subsequence,[(verified_subsequence[0]-0),(int(sequence[-1][1])-verified_subsequence[1])])\n",
        "                    verified=True\n",
        "                    break\n",
        "            if(verified):\n",
        "                result.append(sequence[verified_subsequence[0]:verified_subsequence[1]])\n",
        "                if(verified_subsequence[0]-0)>0:\n",
        "                    subequence_to_check=sequence[0:verified_subsequence[0]]\n",
        "                    #since tokens before the starting position of the verified subsequence have already been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length-1))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                if(int(sequence[-1][1])-verified_subsequence[1])>0:\n",
        "                    subequence_to_check=sequence[(verified_subsequence[1]):]\n",
        "                    #since tokens following the end position of the verified subsequence have not been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                return result\n",
        "            else:\n",
        "                subsequence_length-=1\n",
        "        return result\n",
        "\n",
        "    # def flatten(self,mylist, outlist,ignore_types=(str, bytes, int, ne.NE_candidate)):\n",
        "    def flatten(self,mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "        if mylist !=[]:\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.flatten(item, outlist)\n",
        "                else:\n",
        "                    # if isinstance(item,ne.NE_candidate):\n",
        "                    #     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
        "                    #     item.reset_length()\n",
        "                    # else:\n",
        "                    if type(item)!= int:\n",
        "                        item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "            \n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "            # elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "            #     temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "            #     if(temp1):\n",
        "            #         temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "            #     temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "                #print(index, temp)\n",
        "            # elif \"...\" in word:\n",
        "            #     #print(\"here\")\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"...\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"...\")):\n",
        "            #             temp=list(map(lambda elem: elem+'...', temp))\n",
        "            #         else:\n",
        "            #            temp=list(map(lambda elem: elem+'...', temp[:-1]))+[temp[-1]]\n",
        "            #     # temp.append(temp1[-1])\n",
        "            # elif \"..\" in word:\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"..\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"..\")):\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp))\n",
        "            #         else:\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp[:-1]))+[temp[-1]]\n",
        "            #     #temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def get_Candidates(self, sequence, CTrie,flag):\n",
        "        #flag: debug_flag\n",
        "        candidateList=[]\n",
        "        left=0\n",
        "        start_node=CTrie\n",
        "        last_cand=\"NAN\"\n",
        "        last_cand_substr=\"\"\n",
        "        reset=False\n",
        "        right=0\n",
        "        while (right < len(sequence)):\n",
        "            # if(flag):\n",
        "            #     print(right)\n",
        "            if(reset):\n",
        "                start_node=CTrie\n",
        "                last_cand_substr=\"\"\n",
        "                left=right\n",
        "            curr_text=sequence[right][0]\n",
        "            curr_pos=[sequence[right][1]]\n",
        "            #normalized curr_text\n",
        "            curr=self.normalize(sequence[right][0])\n",
        "            cand_str=self.normalize(last_cand_substr+\" \"+curr)\n",
        "            cand_str_wPunct=(last_cand_substr+\" \"+curr_text).lower()\n",
        "            last_cand_sequence=sequence[left:(right+1)]\n",
        "            last_cand_text=' '.join(str(e[0]) for e in last_cand_sequence)\n",
        "            last_cand_text_norm=self.normalize(' '.join(str(e[0]) for e in last_cand_sequence))\n",
        "            if(flag):\n",
        "                print(\"==>\",cand_str,last_cand_text_norm)\n",
        "            if((cand_str==last_cand_text_norm)&((curr in start_node.path.keys())|(curr_text.lower() in start_node.path.keys()))):\n",
        "            #if (((curr in start_node.path.keys())&(cand_str==last_cand_text_norm))|(curr_text.lower() in start_node.path.keys())):\n",
        "                if flag:\n",
        "                    print(\"=>\",cand_str,last_cand_text)\n",
        "                reset=False\n",
        "                if (curr_text.lower() in start_node.path.keys()):\n",
        "                    if (start_node.path[curr_text.lower()].value_valid):\n",
        "                        last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                        last_cand_batch=start_node.path[curr_text.lower()].feature_list[-1]\n",
        "                        last_cand=last_cand_text\n",
        "                    elif(curr in start_node.path.keys()):\n",
        "                        if ((start_node.path[curr].value_valid)):\n",
        "                            last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                            last_cand=last_cand_text\n",
        "                            last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                        else:\n",
        "                            if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                                #print(\"hehe\",cand_str)\n",
        "                                right=left\n",
        "                                reset=True\n",
        "                    else:\n",
        "                        if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                            #print(\"hehe\",cand_str)\n",
        "                            right=left\n",
        "                            reset=True\n",
        "                elif ((start_node.path[curr].value_valid)&(cand_str==last_cand_text_norm)):\n",
        "                    # if flag:\n",
        "                    #     print(\"==\",last_cand_text)\n",
        "                    last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                    last_cand=last_cand_text\n",
        "                    last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                else:\n",
        "                    if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                        #print(\"hehe\",cand_str)\n",
        "                        right=left\n",
        "                        reset=True\n",
        "                if((curr_text.lower() in start_node.path.keys())&(cand_str==last_cand_text_norm)):\n",
        "                    start_node=start_node.path[curr_text.lower()]\n",
        "                    last_cand_substr=cand_str_wPunct\n",
        "                else:\n",
        "                    start_node=start_node.path[curr]\n",
        "                    last_cand_substr=cand_str\n",
        "            else:\n",
        "                #print(\"=>\",cand_str,last_cand_text)\n",
        "                if(last_cand!=\"NAN\"):\n",
        "                    candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "                    last_cand=\"NAN\"\n",
        "                    if(start_node!=CTrie):\n",
        "                        start_node=CTrie\n",
        "                        last_cand_substr=\"\"\n",
        "                        if curr in start_node.path.keys():\n",
        "                            # if(flag):\n",
        "                            #     print(\"here\",curr)\n",
        "                            reset=False\n",
        "                            if start_node.path[curr].value_valid:\n",
        "                                last_cand_text=curr_text\n",
        "                                last_cand_pos=curr_pos\n",
        "                                last_cand=last_cand_text\n",
        "                                last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                            left=right\n",
        "                            start_node=start_node.path[curr]\n",
        "                            last_cand_substr=curr\n",
        "                        else:\n",
        "                            reset=True\n",
        "                    else:\n",
        "                        reset=True\n",
        "                else:\n",
        "                    if(left<right):\n",
        "                        # if(flag):\n",
        "                        #     print(sequence[(left+1):(right+1)])\n",
        "                        #candidateList.extend(self.get_Candidates(sequence[(left+1):(right+1)], CTrie, flag))\n",
        "                        right=left\n",
        "                        # if(flag):\n",
        "                        #     print(\"++\",right)\n",
        "                    reset=True\n",
        "            right+=1\n",
        "        # if(flag):\n",
        "        #     print(last_cand)\n",
        "        if(last_cand!=\"NAN\"):\n",
        "            candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "        return candidateList\n",
        "\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        #self.data_frame_holder=self.data_frame_holder.append(df,ignore_index=True)\n",
        "        #self.data_frame_holder=self.data_frame_holder.reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def join_token_tuples(self,list_of_tuples):\n",
        "        #print(string.punctuation)\n",
        "        combined_str=(' '.join(tuple[0] for tuple in list_of_tuples)).lstrip(string.punctuation).rstrip(string.punctuation).strip()\n",
        "        combined_pos='*'.join(str(tuple[1]) for tuple in list_of_tuples)\n",
        "        combined_tuple=(combined_str,combined_pos,list_of_tuples[0][2],list_of_tuples[0][3],list_of_tuples[0][4],list_of_tuples[0][5],list_of_tuples[0][6])\n",
        "        return combined_tuple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def all_capitalized(self,candidate):\n",
        "        strip_op=candidate\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip())\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        prep_article_list=prep_list+article_list+self.phase2stopwordList\n",
        "        word_list=strip_op.split()\n",
        "        for i in range(len(word_list)):\n",
        "            word=word_list[i]\n",
        "            if((word[0].isupper())|(word[0].isdigit())):\n",
        "                continue\n",
        "            else:\n",
        "                if(word in prep_article_list):\n",
        "                    if (i!=0):\n",
        "                        continue\n",
        "                    else:\n",
        "                        return False\n",
        "                elif(word in conjoiner):\n",
        "                    continue\n",
        "                else:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def check_feature_update(self, candidate_tuple,non_discriminative_flag):\n",
        "        #print(candidate_tuple)\n",
        "        if(non_discriminative_flag):\n",
        "            return 7\n",
        "        candidateText=candidate_tuple[0]\n",
        "        position=candidate_tuple[1]\n",
        "        word_list=candidateText.split()\n",
        "        if candidateText.islower():\n",
        "            return 6\n",
        "        elif candidateText.isupper():\n",
        "            return 5\n",
        "        elif (len(word_list)==1):\n",
        "            #start-of-sentence-check\n",
        "            if self.all_capitalized(candidateText):\n",
        "                if(int(position[0])==0):\n",
        "                    return 4\n",
        "                else:\n",
        "                    return 2\n",
        "            else:\n",
        "                return 3\n",
        "        else:\n",
        "            if(self.all_capitalized(candidateText)):\n",
        "                return 2\n",
        "            else:\n",
        "                return 3\n",
        "\n",
        "\n",
        "    def update_Candidatedict(self,candidate_tuple,non_discriminative_flag,contextual_embedding_vector):\n",
        "        candidateText=candidate_tuple[0]\n",
        "        normalized_candidate=self.normalize(candidateText)\n",
        "        # print('adding:',normalized_candidate)\n",
        "\n",
        "        feature_list=[]\n",
        "        if(normalized_candidate in self.CandidateBase_dict.keys()):\n",
        "            feature_list=self.CandidateBase_dict[normalized_candidate]\n",
        "        else:\n",
        "            # feature_list=[0]*9 # only syntax\n",
        "            feature_list=[0]*109 # context embedding: 100\n",
        "\n",
        "            feature_list[0]=self.counter\n",
        "            feature_list[1]=len(normalized_candidate.split())\n",
        "\n",
        "        #syntax_feature to update\n",
        "        feature_to_update=self.check_feature_update(candidate_tuple,non_discriminative_flag)\n",
        "        feature_list[feature_to_update]+=1\n",
        "\n",
        "        # add up the context embedding features\n",
        "        # print(len(contextual_embedding_vector))\n",
        "        feature_list[8:-1]= np.add(feature_list[8:-1],contextual_embedding_vector).tolist()\n",
        "\n",
        "        #increment cumulative frequency\n",
        "        feature_list[-1]+=1\n",
        "        self.CandidateBase_dict[normalized_candidate]=feature_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def extract(self,tweetBaseInput,CTrie,phase2stopwordList,new_or_old):\n",
        "\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #output_queue\n",
        "            self.data_frame_holder_OQ=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.not_reintroduced=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.CandidateBase_dict= {}\n",
        "            self.ambiguous_candidate_distanceDict_prev={}\n",
        "            self.partition_dict={}\n",
        "            self.good_candidates=[]\n",
        "            self.bad_candidates=[]\n",
        "            self.ambiguous_candidates=[]\n",
        "\n",
        "            self.aggregator_incomplete_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.just_converted_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            #self.data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates'])\n",
        "            self.raw_tweets_for_others=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "\n",
        "            self.accuracy_tuples_prev_batch=[]\n",
        "            self.accuracy_vals=[]\n",
        "            \n",
        "            #frequency_w_decay related information\n",
        "            self.ambiguous_candidates_reintroduction_dict={}\n",
        "\n",
        "            #### other systems\n",
        "            self.accuracy_vals_stanford=[]\n",
        "            self.accuracy_vals_opencalai=[]\n",
        "            self.accuracy_vals_ritter=[]\n",
        "            self.accuracy_vals_neuroner=[]\n",
        "\n",
        "            self.number_of_seen_tweets_per_batch=[]\n",
        "        self.phase2stopwordList=phase2stopwordList\n",
        "        self.number_of_seen_tweets_per_batch.append(len(tweetBaseInput))\n",
        "\n",
        "\n",
        "        #data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase1_holder_holder=[]\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "        \n",
        "        #candidateBase_holder=[]\n",
        "\n",
        "        #this has to be changed to an append function since IPQ already has incomplete tweets from prev batch  \n",
        "        #print(len(tweetBaseInput))\n",
        "        #immediate_processingQueue = pd.concat([self.incomplete_tweets,TweetBase ])\n",
        "        #immediate_processingQueue.to_csv(\"impq.csv\", sep=',', encoding='utf-8')\n",
        "        \n",
        "\n",
        "\n",
        "        #print('In Phase 2',len(immediate_processingQueue))\n",
        "        #immediate_processingQueue=immediate_processingQueue.reset_index(drop=True)\n",
        "        combined_list_here=([]+list(cachedStopWords)+chat_word_list+day_list+month_list+article_list+prep_list)\n",
        "        combined_list_filtered=list(filter(lambda word: word not in (prep_list+article_list+month_list+self.phase2stopwordList), combined_list_here))\n",
        "        #--------------------------------------PHASE II---------------------------------------------------\n",
        "        for index, row in tweetBaseInput.iterrows():\n",
        "\n",
        "            #phase 1 candidates for one sentence\n",
        "            phase1_holder=[]\n",
        "\n",
        "            tweetText=str(row['TweetSentence'])\n",
        "            tweetWordList = row['tweetwordList']\n",
        "            sentID=str(row['sentID'])\n",
        "            tweetID=str(row['tweetID'])\n",
        "            phase1Candidates=str(row['phase1CandidatesWPositions'])\n",
        "            batch=int(row['entry_batch'])\n",
        "            contextual_embeddings_dict= {ind: embedding for ind, embedding in enumerate(row['contextual_embeddings'])}\n",
        "            \n",
        "            # print('====',tweetID,sentID)\n",
        "            # print('tweetWordList:',tweetWordList)\n",
        "            \n",
        "            non_discriminative_flag=False\n",
        "            phase1CandidatesList=[]\n",
        "\n",
        "            # print('phase1Candidates:',phase1Candidates)\n",
        "\n",
        "            if (phase1Candidates !='nan'):\n",
        "                phase1Raw=phase1Candidates.split(\"||\")\n",
        "                phase1Raw = list(filter(None, phase1Raw))\n",
        "\n",
        "\n",
        "                for entities_with_loc in phase1Raw:\n",
        "                    entity_to_store=entities_with_loc.split(\"::\")[0]\n",
        "                    #print(entity_to_store)\n",
        "                    position=entities_with_loc.split(\"::\")[1]\n",
        "                    #print(position)\n",
        "                    phase1_holder.append((entity_to_store,position))\n",
        "                    phase1_holder.clear()\n",
        "                    phase1CandidatesList.append(entity_to_store.lower())\n",
        "\n",
        "                phase1_holder_holder.append(copy.deepcopy(phase1_holder))\n",
        "                \n",
        "\n",
        "            else:\n",
        "                non_discriminative_flag=True\n",
        "                phase1_holder_holder.append([])\n",
        "\n",
        "            \n",
        "            # tweetWordList=self.getWords(tweetText)\n",
        "            # tweetWordList= [(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "            \n",
        "            tweetWordList_stopWords=list(filter (lambda word: ((((word[0].strip()).strip(string.punctuation)).lower() in combined_list_filtered)|(word[0].strip() in string.punctuation)|(word[0].startswith('@'))|(word[0].startswith('#'))), tweetWordList))\n",
        "\n",
        "            # phase 2 candidate tuples without stopwords for a sentence\n",
        "            c=[(y[0],str(y[1]),tweetID,sentID,'ne',batch,time) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "            #c=[(y[0],str(y[1])) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "\n",
        "            \n",
        "            sequences=[]\n",
        "            for k, g in groupby(enumerate(c), lambda element: element[0]-int(element[1][1])):\n",
        "                sequences.append(list(map(itemgetter(1), g)))\n",
        "\n",
        "            # print('phaseII candidates:')\n",
        "            ne_candidate_list=[]\n",
        "            for sequence in sequences:\n",
        "                seq_candidate_list=self.get_Candidates(sequence, CTrie,False)\n",
        "                if(seq_candidate_list):\n",
        "                    for candidate_tuple in seq_candidate_list:\n",
        "                        # print(candidate_tuple)\n",
        "\n",
        "                        # extract candidate token embeddings\n",
        "                        candidate_token_embeddings = torch.stack([contextual_embeddings_dict[int(position)] for position in candidate_tuple[1]])\n",
        "                        # print('candidate with token_embeddings:',candidate_tuple[0],candidate_token_embeddings.shape,len(candidate_tuple[1]))\n",
        "\n",
        "                        # !!necessary because this function during training receives [1,n,100] tensors that it squeezes; so might screw up 1-token sentences\n",
        "                        candidate_embedding = self.entity_phrase_embedder.getEmbedding(candidate_token_embeddings.unsqueeze(0))\n",
        "                        # print('candidate_embedding:',candidate_embedding.shape)\n",
        "\n",
        "                        #inserts into CandidateBase: Syntax and Context Feature Setting\n",
        "                        if not ((float(batch)<self.counter)&(candidate_tuple[-1]<self.counter)):\n",
        "                            self.update_Candidatedict(candidate_tuple,non_discriminative_flag,candidate_embedding.tolist())\n",
        "\n",
        "                    ne_candidate_list.extend(seq_candidate_list)\n",
        "\n",
        "            phase2_candidates=[self.normalize(e[0]) for e in ne_candidate_list]\n",
        "            phase2_candidates_unnormalized=[e[0] for e in ne_candidate_list]\n",
        "\n",
        "            phase2_candidates_holder.append(phase2_candidates)\n",
        "            phase2_unnormalized_candidates_holder.append(phase2_candidates_unnormalized)\n",
        "\n",
        "            dict1 = {'entry_batch':batch, 'tweetID':tweetID, 'sentID':sentID, 'TweetSentence':tweetText, 'phase1Candidates':phase1CandidatesList,'2nd Iteration Candidates':phase2_candidates,'2nd Iteration Candidates Unnormalized':phase2_candidates_unnormalized}\n",
        "\n",
        "            df_holder.append(dict1)\n",
        "            \n",
        "            #-------------------------------------------------------------------END of 1st iteration: RESCAN+CANDIDATE_UPDATION-----------------------------------------------------------\n",
        "\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_large_300d.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidateBase_dict_filtered={}\n",
        "        # for candidate in self.CandidateBase_dict:\n",
        "        #     if(candidate in candidate_list):\n",
        "        #         candidateBase_dict_filtered[candidate] = self.CandidateBase_dict[candidate]\n",
        "\n",
        "        # # candidateBase_dict_filtered = self.CandidateBase_dict\n",
        "\n",
        "        # candidate_featureBase_DF_filtered=pd.DataFrame.from_dict(candidateBase_dict_filtered, orient='index')\n",
        "        # candidate_featureBase_DF_filtered.columns=self.candidateBaseHeaders[1:]\n",
        "        # candidate_featureBase_DF_filtered.index.name=self.candidateBaseHeaders[0]\n",
        "        # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered.reset_index(drop=False)\n",
        "        # # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered[candidate_featureBase_DF_filtered['cumulative'] >= 5.0]\n",
        "\n",
        "        # print('before:', len(self.CandidateBase_dict),'after:',len(candidateBase_dict_filtered))\n",
        "        \n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "        # candidate_featureBase_DF_filtered['class'] = candidate_featureBase_DF_filtered.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # # candidate_featureBase_DF_filtered['class'] = 0\n",
        "\n",
        "        # candidate_featureBase_DF_filtered.to_csv(\"data/candidate_train_records_large_100d.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        candidate_featureBase_DF = pd.DataFrame.from_dict(self.CandidateBase_dict, orient='index')\n",
        "        candidate_featureBase_DF.columns = self.candidateBaseHeaders[1:]\n",
        "        candidate_featureBase_DF.index.name = self.candidateBaseHeaders[0]\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF.reset_index(drop=False)\n",
        "\n",
        "        return candidate_featureBase_DF,df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "\n",
        "        ## self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        ## self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets_for_current_batch)\n",
        "\n",
        "\n",
        "    def finish(self):\n",
        "        return self.accuracy_vals\n",
        "\n",
        "    def finish_other_systems(self):\n",
        "        stanford_f1=[]\n",
        "        stanford_precision=[]\n",
        "        stanford_recall=[]\n",
        "        print(\"*****************************************STANFORD***********************\")\n",
        "        for i in self.accuracy_vals_stanford:\n",
        "            stanford_f1.append(i[0])\n",
        "            stanford_precision.append(i[1])\n",
        "            stanford_recall.append(i[2])\n",
        "            # print(i)\n",
        "        print('stanford_f1:', stanford_f1)\n",
        "        print('stanford_precision:', stanford_precision)\n",
        "        print('stanford_recall:', stanford_recall)\n",
        "\n",
        "        print(sum(stanford_f1)/len(stanford_f1))\n",
        "        print(sum(stanford_precision)/len(stanford_precision))\n",
        "        print(sum(stanford_recall)/len(stanford_recall))\n",
        "\n",
        "        print(\"*****************************************Opencalai***********************\")\n",
        "        opencalai_f1=[]\n",
        "        opencalai_precision=[]\n",
        "        opencalai_recall=[]\n",
        "        for i in self.accuracy_vals_opencalai:\n",
        "            opencalai_f1.append(i[0])\n",
        "            opencalai_precision.append(i[1])\n",
        "            opencalai_recall.append(i[2])\n",
        "        print('opencalai_f1:', opencalai_f1)\n",
        "        print('opencalai_precision:', opencalai_precision)\n",
        "        print('opencalai_recall:', opencalai_recall)\n",
        "\n",
        "        print(sum(opencalai_f1)/len(opencalai_f1))\n",
        "        print(sum(opencalai_precision)/len(opencalai_precision))\n",
        "        print(sum(opencalai_recall)/len(opencalai_recall))\n",
        "        print(\"*****************************************Ritter***********************\")\n",
        "        ritter_f1=[]\n",
        "        ritter_precision=[]\n",
        "        ritter_recall=[]\n",
        "        for i in self.accuracy_vals_ritter:\n",
        "            ritter_f1.append(i[0])\n",
        "            ritter_precision.append(i[1])\n",
        "            ritter_recall.append(i[2])\n",
        "        print('ritter_f1:', ritter_f1)\n",
        "        print('ritter_precision:', ritter_precision)\n",
        "        print('ritter_recall:', ritter_recall)\n",
        "\n",
        "        print(sum(ritter_f1)/len(ritter_f1))\n",
        "        print(sum(ritter_precision)/len(ritter_precision))\n",
        "        print(sum(ritter_recall)/len(ritter_recall))\n",
        "        print(\"*****************************************Neuroner***********************\")\n",
        "        neuroner_f1=[]\n",
        "        neuroner_precision=[]\n",
        "        neuroner_recall=[]\n",
        "        for i in self.accuracy_vals_neuroner:\n",
        "            neuroner_f1.append(i[0])\n",
        "            neuroner_precision.append(i[1])\n",
        "            neuroner_recall.append(i[2])\n",
        "        print('neuroner_f1:', neuroner_f1)\n",
        "        print('neuroner_precision:', neuroner_precision)\n",
        "        print('neuroner_recall:',neuroner_recall)\n",
        "\n",
        "        print(sum(neuroner_f1)/len(neuroner_f1))\n",
        "        print(sum(neuroner_precision)/len(neuroner_precision))\n",
        "        print(sum(neuroner_recall)/len(neuroner_recall))\n",
        "\n",
        "        return (self.accuracy_vals_stanford,self.accuracy_vals_opencalai,self.accuracy_vals_ritter,self.accuracy_vals_neuroner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfCP0UkWHnXB"
      },
      "source": [
        "## **Running the Engine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TxMA6-ThnC8"
      },
      "source": [
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "tweets_unpartitoned=pd.read_csv(\"data/covid/covid_2K.csv\",sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3K/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade/roevwade.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika/pikapika.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity/ripcity.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billnye/billnye.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/wnut-test/wnut17test.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "#to train the Entity Classifiers\n",
        "# tweets_unpartitoned=pd.read_csv('data/deduplicated/deduplicated_test.csv',sep =';',keep_default_na=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBJlVu-gk74E",
        "outputId": "1ce5c7b8-1516-4015-a7eb-68f6815db042"
      },
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweets are in memory...\n",
            "1998 1998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRLdfhq-EHQf"
      },
      "source": [
        "# from executor import *\n",
        "# sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation = execute('emerging.test.conll')\n",
        "# # print(token_feature_tuple_list[:3])\n",
        "\n",
        "from executor import *\n",
        "\n",
        "# trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training()\n",
        "# sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation = execute('emerging.test.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "\n",
        "tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training('emerging.test.conll')\n",
        "# tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training('btc.emerging.test.conll')\n",
        "# tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training('deduplicated.emerging.test.conll')\n",
        "\n",
        "sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation = execute(tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1pcAMbnT70-"
      },
      "source": [
        "sentence_df_dict format: tweetID, sentID, sentence, list of extracted mentions, token wise 100-d features from DNN;;\n",
        "key-> serialized sentence #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-F2Bkz6V7CE"
      },
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))\n",
        "\n",
        "total_time=0\n",
        "g=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qszVo73c9-Re"
      },
      "source": [
        "#Running Phase I\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation, device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMQJ0J24y6Lm"
      },
      "source": [
        "return_tuple = local_NER_Module.extract(g)\n",
        "\n",
        "tweet_base=return_tuple[0]\n",
        "contextual_embeddings=return_tuple[1]\n",
        "candidate_base=return_tuple[2]\n",
        "elapsedTime= return_tuple[4] - return_tuple[3]\n",
        "phase2stopwordList=return_tuple[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=return_tuple[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0a_wcAFjV5"
      },
      "source": [
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'deduplicated_test')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'venezuela')\n",
        "\n",
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'covid')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_3K')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'roevwade')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billdeblasio')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'pikapika')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billnye')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'ripcity')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'wnut17')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'btc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjMQa7kpFlCd"
      },
      "source": [
        "time_in = time.time()\n",
        "candidate_base_post_Phase2, complete_tweet_dataframe,time_out = global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n",
        "# time_out = time.time()\n",
        "\n",
        "print('time_taken', (time_out-time_in))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyV04iUobxUU"
      },
      "source": [
        "## **Extracting Embeddings to train the Phrase Embedder on STS Dataset-- (NOT REQD TO RUN) unless training the phrase Embedder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLexEQkrlZzq"
      },
      "source": [
        "class PhraseEmbeddingDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, input, output):\n",
        "\n",
        "        # print(data[0])\n",
        "        # self.data = np.asarray(data)\n",
        "        # self.output = np.asarray(output)\n",
        "\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "\n",
        "        print(type(self.input),type(self.output))\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.input) == len(self.output)\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.input[idx]\n",
        "        y = self.output[idx]\n",
        "        return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKZgcTA_EPZd"
      },
      "source": [
        "def compile_token_embeddings(lst):\n",
        "    token_embeddings=[]\n",
        "    for tup in lst:\n",
        "        token = tup[0]\n",
        "        # tup[1] is feature dict, keys 0 to 99\n",
        "        embedding_dict = tup[1]\n",
        "        embedding_arr = np.array([embedding_dict['feat'+str(i)] for i in range(0,100)])\n",
        "        token_embeddings.append(torch.from_numpy(embedding_arr).float())\n",
        "    return token_embeddings\n",
        "\n",
        "def quick_extract_sentence_token_embeddings(gaguilar_sentence_outputs):\n",
        "    entity_embeddings=[]\n",
        "    for key, value in gaguilar_sentence_outputs.items():\n",
        "        token_embedding_list= list(value[4])\n",
        "        entity_aware_embeddings = compile_token_embeddings(token_embedding_list)\n",
        "        entity_embeddings.append(torch.stack(entity_aware_embeddings))\n",
        "    return entity_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ke_L8gkvew"
      },
      "source": [
        "def get_sts_data_scores(filename):\n",
        "    stsDataDictList=[]\n",
        "    stsData_columns =['sentence1', 'sentence2', 'score']\n",
        "    f=open(\"data/stsbenchmark/\"+filename+\".csv\",'r')\n",
        "    file_text= f.read()\n",
        "    lines=file_text.split('\\n')\n",
        "    for line in lines:\n",
        "        if(line):\n",
        "            fields=line.split('\\t')\n",
        "    #         print(len(fields))\n",
        "            dataDict={'sentence1':fields[5],'sentence2':fields[6],'score':float(fields[4])/5.0}\n",
        "    #         print(dataDict)\n",
        "            stsDataDictList.append(dataDict)\n",
        "    stsData=pd.DataFrame(stsDataDictList)\n",
        "    return stsData['score'].tolist()\n",
        "\n",
        "stsTrainDataScores = get_sts_data_scores('sts-train')\n",
        "print(len(stsTrainDataScores))\n",
        "# print(stsTrainData.columns)\n",
        "\n",
        "stsDevDataScores = get_sts_data_scores('sts-dev')\n",
        "print(len(stsDevDataScores))\n",
        "# print(stsDevData.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAByj-azNh0U"
      },
      "source": [
        "from executor import *\n",
        "trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnry5EJWCuTq"
      },
      "source": [
        "# from executor import *\n",
        "\n",
        "#Training Data\n",
        "#For Source Sentences:\n",
        "# training_source_sentence_df_dict_gaguilar, training_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.train.conll')\n",
        "training_source_sentence_df_dict_gaguilar, training_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.train.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "source_sentence_embeddings_train = quick_extract_sentence_token_embeddings(training_source_sentence_df_dict_gaguilar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pU7Z7aRNyPV"
      },
      "source": [
        "#Training Data\n",
        "#For Target Sentences:\n",
        "# training_target_sentence_df_dict_gaguilar, training_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.train.conll')\n",
        "training_target_sentence_df_dict_gaguilar, training_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.train.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "target_sentence_embeddings_train = quick_extract_sentence_token_embeddings(training_target_sentence_df_dict_gaguilar)\n",
        "assert len(source_sentence_embeddings_train)==len(target_sentence_embeddings_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiSaTRFOm8Me"
      },
      "source": [
        "#Validation Data\n",
        "#For Source Sentences:\n",
        "# dev_source_sentence_df_dict_gaguilar, dev_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.dev.conll')\n",
        "dev_source_sentence_df_dict_gaguilar, dev_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.dev.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "source_sentence_embeddings_dev = quick_extract_sentence_token_embeddings(dev_source_sentence_df_dict_gaguilar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0OQLHuK50-4"
      },
      "source": [
        "#Validation Data\n",
        "#For Target Sentences:\n",
        "# dev_target_sentence_df_dict_gaguilar, dev_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.dev.conll')\n",
        "dev_target_sentence_df_dict_gaguilar, dev_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.dev.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "target_sentence_embeddings_dev = quick_extract_sentence_token_embeddings(dev_target_sentence_df_dict_gaguilar)\n",
        "assert len(source_sentence_embeddings_dev)==len(target_sentence_embeddings_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QFGRmoWa010"
      },
      "source": [
        "#Printing some shapes\n",
        "print(len(source_sentence_embeddings_train), len(target_sentence_embeddings_train))\n",
        "print(source_sentence_embeddings_train[0].shape)\n",
        "embeddingSize=list(source_sentence_embeddings_train[0][0].shape)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89OV0OTDbSrU"
      },
      "source": [
        "# Datasets and DataLoaders\n",
        "training_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_train, target_sentence_embeddings_train))),stsTrainDataScores)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, shuffle=True)\n",
        "\n",
        "validation_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_dev, target_sentence_embeddings_dev))),stsDevDataScores)\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7iLY5dUga0h"
      },
      "source": [
        "def save_ckp(state, is_best, checkpoint_dir):\n",
        "    # f_path = checkpoint_dir + '/checkpoint.pt' \n",
        "    f_path = checkpoint_dir + '/checkpoint_model100.pt' #_model100\n",
        "    torch.save(state, f_path)\n",
        "    # if is_best:\n",
        "    #     best_fpath = best_model_dir +'/best_model.pt'\n",
        "    #     shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA95S7YQizvJ"
      },
      "source": [
        "# Initialize network\n",
        "phraseEmbeddingModel = PhraseEmbedding(embeddingSize, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuVuM-S9i_F4"
      },
      "source": [
        "checkpoint_dir='entityEmbedding/model_checkpoints'\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 200\n",
        "patience = 5\n",
        "\n",
        "# Train Network\n",
        "history_training= []\n",
        "history_validation = []\n",
        "best_loss = np.float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_batch_loss=[]\n",
        "    for batch_idx, (data, target) in enumerate(training_generator):\n",
        "        target = torch.tensor(float(target)).to(device=device)\n",
        "        out = phraseEmbeddingModel(data)\n",
        "        # print(out.item())\n",
        "        # if(not math.isnan(out.item())):\n",
        "            # print(data)\n",
        "        loss = criterion(out, target)\n",
        "        # print(loss.item())\n",
        "        training_batch_loss.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(phraseEmbeddingModel.parameters(), 1.0)\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "    combined_training_loss = np.mean(training_batch_loss)\n",
        "    print('combined epoch training loss:', combined_training_loss)\n",
        "    history_training.append(combined_training_loss)\n",
        "\n",
        "    #Validation: DO NOT BACKPROPAGATE HERE\n",
        "    validation_batch_loss = []\n",
        "    print_only_one=True\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(validation_generator):\n",
        "            target = torch.tensor(float(target)).to(device=device)\n",
        "            if(print_only_one):\n",
        "                # print(len(data[0]),len(data[1]))\n",
        "                # print(type(data))\n",
        "                print_only_one=False\n",
        "            out = phraseEmbeddingModel(data)\n",
        "            loss = criterion(out, target)\n",
        "            validation_batch_loss.append(loss.item())\n",
        "            # print(validation_batch_loss)\n",
        "    combined_validation_loss= np.mean(validation_batch_loss)\n",
        "    history_validation.append(combined_validation_loss)\n",
        "    # if(((epoch+1)%10==0)|(epoch == (num_epochs-1))):\n",
        "    print('\\nEpoch',str(epoch+1),' Validation Loss:',combined_validation_loss)\n",
        "    \n",
        "    if(combined_validation_loss<best_loss):\n",
        "        best_loss = combined_validation_loss\n",
        "        print('saving this checkpoint')\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        save_ckp(checkpoint, True, checkpoint_dir)\n",
        "        no_improvement_counter=0\n",
        "        print('=====================================================================\\n')\n",
        "    else:\n",
        "        no_improvement_counter+=1\n",
        "        if(no_improvement_counter>patience):\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooCO54HSjd2q"
      },
      "source": [
        "checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint_model100.pt' #300\n",
        "# model_dir = 'best-model'\n",
        "# save_ckp(checkpoint, True, checkpoint_dir, model_dir)\n",
        "\n",
        "# load the last checkpoint with the best model\n",
        "entityPhraseEmbedder, optimizer, start_epoch = load_ckp(checkpoint_path, phraseEmbeddingModel, optimizer)\n",
        "print('Loading model from epoch:', start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgqGsSq3i1w4"
      },
      "source": [
        "## **Regular EMD Runs (NO NEED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l4-QtqguiV0"
      },
      "source": [
        "# !python3 main.py btc.emerging.test.conll\n",
        "# !python3 main.py emerging.test.conll"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}