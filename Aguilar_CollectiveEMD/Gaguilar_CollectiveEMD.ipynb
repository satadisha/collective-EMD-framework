{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gaguilar-CollectiveEMD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GNV_kqwPPffd",
        "P5Y_NOU9I3zi",
        "75TQmQgbnHwM",
        "jLm3eMGNGM4x",
        "mzd0dE0kXUXn",
        "8QiYJ45QEfUB",
        "qyV04iUobxUU",
        "zgqGsSq3i1w4"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNV_kqwPPffd"
      },
      "source": [
        "## **Installation and Import Stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy6bAJBnkymz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef6af86-5a43-44ef-ef0c-4ff918a06f59"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBgg3mXCCiTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee39516a-cf3b-4965-e89d-b21a36706d39"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX-zR01HClOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e522300-6fb9-4742-9a29-0663b1d75e03"
      },
      "source": [
        "%cd gdrive/My Drive/gaguilar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1D5I4ZlzhFow8fFGxbipMXBZSuRXJ01yA/gaguilar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g583AD8QCsWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b685ac17-4b5f-4642-cef7-936af7eb8ea7"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "seed_number = 42\n",
        "\n",
        "print(\"GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
        "physical_devices= tf.config.experimental.list_physical_devices('GPU')\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# tf.compat.v1.set_random_seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n",
            "GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13413609558998971057\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 16183459840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 6995042542037162829\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xub_9pUDCxTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b599c42f-5268-4e24-96de-dc803bb3b93b"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co65HRppuavQ",
        "outputId": "de6097df-2341-444c-b470-72eef21e9720"
      },
      "source": [
        "!pip3 install emoji\n",
        "!pip3 install python-crfsuite\n",
        "!pip3 install sklearn-crfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.7/dist-packages (0.9.7)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS8U38HSufb2"
      },
      "source": [
        "# print('intermittent')\n",
        "import os\n",
        "# import tensorflow as tf\n",
        "# # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "# tf.set_random_seed(42)\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(seed_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhFmhfKqtyef"
      },
      "source": [
        "from keras import backend as K\n",
        "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(seed_number)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
        "K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV3r4HPYMDv1",
        "outputId": "4c51bac4-4039-426d-dcab-0e4ad3382b4a"
      },
      "source": [
        "!pip3 install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCD1hqkMiie_",
        "outputId": "98ae3ea7-e015-4dfa-a8d1-19116bea30f9"
      },
      "source": [
        "!pip3 install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.14)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPO-zRE9m9z_",
        "outputId": "a1116048-3c25-458b-8cd5-9fd0baeaaf2c"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiacJKWghMqs",
        "outputId": "f34b6b1c-5d73-492d-8d60-589aca62c5c5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "np.random.seed(seed_number)\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f076a046ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Y_NOU9I3zi"
      },
      "source": [
        "## **Setting some global stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyZI772RI8lk",
        "outputId": "d3462fa5-a9ea-45e6-dea5-2b085376bb12"
      },
      "source": [
        "import torch\n",
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "# seed = 123\n",
        "torch.manual_seed(seed_number)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed_number)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzbjfBbwlarT"
      },
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "class PhraseEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,output_size,device):\n",
        "        super(PhraseEmbedding, self).__init__()\n",
        "        self.print_once=True\n",
        "        self.dense_layer = nn.Linear(input_size,output_size)\n",
        "        self.non_linear_layer = nn.Tanh()\n",
        "        self.cosine_layer = nn.CosineSimilarity(dim=0)\n",
        "        self.device = device\n",
        "        return\n",
        "\n",
        "    def encode(self, input_embedding):\n",
        "\n",
        "        # print(input_embedding.size())\n",
        "        input_sentence_embedding = input_embedding.squeeze(0)\n",
        "        # print(input_sentence_embedding.size())\n",
        "        # print('-----')\n",
        "\n",
        "        # Max Pool\n",
        "        # max_pooled_embedding = torch.max(input_sentence_embedding,dim=0)\n",
        "\n",
        "        #Average Pool\n",
        "        average_pooled_embedding = torch.mean(input_sentence_embedding,dim=0).to(device=self.device)\n",
        "        # print(average_pooled_embedding.size())\n",
        "\n",
        "        x = self.dense_layer(average_pooled_embedding)\n",
        "        # print(x.size())\n",
        "\n",
        "        out = self.non_linear_layer(x)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        # print(len(input_tuple))\n",
        "        input_source = input_tuple[0]\n",
        "        input_target = input_tuple[1]\n",
        "\n",
        "        output_source = self.encode(input_source)\n",
        "        output_target = self.encode(input_target)\n",
        "\n",
        "        similarity = self.cosine_layer(output_source, output_target)\n",
        "        # print(similarity)\n",
        "        return similarity\n",
        "\n",
        "    def getEmbedding(self, input_embeddings):\n",
        "        with torch.no_grad():\n",
        "            return self.encode(input_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75TQmQgbnHwM"
      },
      "source": [
        "## **Initialization of some components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzKhH-AKDsS"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "sentence_tokenizer._params.abbrev_types.add('ret.')\n",
        "sentence_tokenizer._params.abbrev_types.add('rep.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MiXK5XBm2fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f71e17d-768d-4880-8e7b-f81bb7695274"
      },
      "source": [
        "from os import path\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "# Initialize network\n",
        "output_embedding_size = 100\n",
        "\n",
        "phraseEmbeddingModel = PhraseEmbedding(100, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# define checkpoint saved path\n",
        "ckp_path = \"entityEmbedding/model_checkpoints/checkpoint_model100.pt\" #100\n",
        "\n",
        "if(path.exists(ckp_path)):\n",
        "    # load the saved checkpoint\n",
        "    entityPhraseEmbedder, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "\n",
        "    print(\"starting with model at epoch:\", start_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting with model at epoch: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YIxqQu4xuBZ"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(seed_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLm3eMGNGM4x"
      },
      "source": [
        "## **Entity Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkK6Oyp6GUkZ"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# import numpy as np\n",
        "# np.random.seed(seed_number)\n",
        "\n",
        "\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,50)\n",
        "    self.linear2 = nn.Linear(50,25)\n",
        "    self.linear3 = nn.Linear(25,1)\n",
        "    # self.linear4 = nn.Linear(6,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    # x = F.relu(self.linear3(x))\n",
        "    # x = F.relu(self.linear4(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifier():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # # using embedding + syntax features\n",
        "        # self.combined_feature_list=['length']+['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(100)]\n",
        "\n",
        "        # separately using only semantic features\n",
        "        self.combined_feature_list=['length']+['cf_'+str(i) for i in range(100)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length'#,\n",
        "            # 'normalized_cap',\n",
        "            # 'normalized_substring-cap',\n",
        "            # 'normalized_s-o-sCap',\n",
        "            # 'normalized_all-cap',\n",
        "            # 'normalized_non-cap',\n",
        "            # 'normalized_non-discriminative'\n",
        "            ]+['normalized_cf_'+str(i) for i in range(100)]\n",
        "\n",
        "        # create scaler\n",
        "        self.scaler = StandardScaler()\n",
        "        # self.scaler = MinMaxScaler()\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.0001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 1000\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            # # fit and transform in one step\n",
        "            training_inputs_array_standardized = self.scaler.fit_transform(training_inputs_array)\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array_standardized).type(torch.float)\n",
        "            # training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.9))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifier_checkpoint_model100.pt\" #100\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        f_path = checkpoint_dir + '/classifier_checkpoint_model100.pt' #100\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_training_loss,',',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                if(combined_validation_loss<best_loss):\n",
        "                # if(f1>best_f1):\n",
        "                    best_loss = combined_validation_loss\n",
        "                    # best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs_array_standardized = self.scaler.fit_transform(test_inputs_array)\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array_standardized).type(torch.float)\n",
        "        # test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzd0dE0kXUXn"
      },
      "source": [
        "## **PHASE I: Local NER Engine (Results already returned from Gaguilar, mainly dataframe preparation for Phase II)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHzZZ8E-XT4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ce973e-3424-45a9-821e-04deabc91000"
      },
      "source": [
        "import re\n",
        "import emoji\n",
        "from emoji import demojize\n",
        "import string\n",
        "\n",
        "# import numpy as np\n",
        "import pandas  as pd\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "import trie as trie\n",
        "import ast\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"other\",\"since\",\"hence\",\"onto\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"up\")\n",
        "\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"v.\"]  #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"httpurl\",\"n't\",\"pls\",\"bye\",\"€\",\"blah\",\"vs\",\"ouch\",\"am\",\"pm\",\"omw\",\"http\",\"https\",\"tv\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"ip\",\"qft\",\"ima\",\"icymi\",\"bdsm\",\"ah\",\"ive\",\"qt\",\"dj\",\"dm\",\"pts\",\"pt\",\"yrs\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "\n",
        "class LocalNERModule():\n",
        "    def __init__(self, sentenceTokenizer, sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation, device):\n",
        "        self.counter=0\n",
        "\n",
        "        if(sentenceTokenizer):\n",
        "            self.my_sentence_tokenizer = sentenceTokenizer\n",
        "        else:\n",
        "            nltk.download('gutenberg')\n",
        "            gutenberg_text = \"\"\n",
        "            for file_id in gutenberg.fileids():\n",
        "                gutenberg_text += gutenberg.raw(file_id)\n",
        "            tokenizer_trainer = PunktTrainer()\n",
        "            tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "            tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "            self.my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "        self.quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        self.tweet_to_sentences_w_annotation = tweet_to_sentences_w_annotation\n",
        "        self.device = device\n",
        "        self.apostrophe_list =[\"'s\",'’s','s']\n",
        "        self.label_list = ['O','B','I']\n",
        "\n",
        "        self.tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "        self.contextual_embeddings = {}\n",
        "\n",
        "        self.gaguilar_sentence_outputs = sentence_df_dict_gaguilar\n",
        "\n",
        "        print('Started Local NER Engine!')\n",
        "\n",
        "    def is_float(self,string):\n",
        "        try:\n",
        "            f=float(string)\n",
        "            if(f==0.0):\n",
        "              return True\n",
        "            else:\n",
        "              return ((f) and (string.count(\".\")==1))\n",
        "      #return True# True if string is a number with a dot\n",
        "        except ValueError:  # if string is not a number\n",
        "          return False\n",
        "\n",
        "    def normalize_to_sentences(self, text):\n",
        "        tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "        tweetSentenceList_inter=self.custom_flatten(list(map(lambda sentText: self.my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "        tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "        return tweetSentenceList\n",
        "\n",
        "    def custom_flatten(self, mylist, outlist, ignore_types=(str, bytes, int)):\n",
        "        \n",
        "        if (mylist !=[]):\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.custom_flatten(item, outlist)\n",
        "                else:\n",
        "                    item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "\n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "    #         elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "    #             temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "    #             if(temp1):\n",
        "    #                 temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "    #             temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.custom_flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "\n",
        "    def remAmpersand(self,candidateStr):\n",
        "        candidateStr=candidateStr.replace('&amp;','')\n",
        "        return candidateStr\n",
        "\n",
        "    def normalizeToken(self,token):\n",
        "        lowercased_token = token.lower()\n",
        "        if token.startswith(\"@\"):\n",
        "            return \"@USER\"\n",
        "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "            return \"HTTPURL\"\n",
        "        elif len(token) == 1:\n",
        "            return demojize(token)\n",
        "        else:\n",
        "            if token == \"’\":\n",
        "                return \"'\"\n",
        "            elif token == \"…\":\n",
        "                return \"...\"\n",
        "            else:\n",
        "                return token\n",
        "\n",
        "    def normalizeTweet(self, tweet):\n",
        "        tokens = self.tweetTokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "        normTweet = \" \".join([self.normalizeToken(token) for token in tokens])\n",
        "\n",
        "        normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "        normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "        normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
        "\n",
        "        normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "        \n",
        "        return normTweet\n",
        "\n",
        "    #removing commonly used expletives, enunciated chat words and other common words (like days of the week, common expressions)\n",
        "    def slang_remove(self,ne_phrase):\n",
        "        phrase=ne_phrase.strip().strip(string.punctuation).lower()\n",
        "        p1= re.compile(r'([A-Za-z]+)\\1\\1{1,}')\n",
        "        match_lst = p1.findall(phrase)\n",
        "        if phrase in article_list:\n",
        "            return True\n",
        "        elif phrase in day_list:\n",
        "            return True\n",
        "        elif phrase in month_list:\n",
        "            return True\n",
        "        elif match_lst:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def all_slang(self, ne_phrase):\n",
        "        ne_words=ne_phrase.split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list\n",
        "        is_invalid=0\n",
        "\n",
        "        for word in ne_words:\n",
        "            if(word.strip().strip(string.punctuation).lower() in combined):\n",
        "                is_invalid+=1\n",
        "        if(is_invalid==len(ne_words)):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def start_end_token_check(self, ne_candidate):\n",
        "        positions = ne_candidate[1]\n",
        "        ne_words=ne_candidate[0].split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list+prep_list+article_list\n",
        "        # print(combined)\n",
        "        # print(ne_words,positions)\n",
        "        if(ne_words):\n",
        "            start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "            end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "            while((len(ne_words)>1)&((start_word in combined)|(end_word in combined))):\n",
        "                if(start_word in combined):\n",
        "                    ne_words.pop(0)\n",
        "                    positions.pop(0)\n",
        "                if(end_word in combined):\n",
        "                    ne_words.pop()\n",
        "                    positions.pop()\n",
        "                if(len(ne_words)>1):\n",
        "                    start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "                    end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        ne_phrase = (' '.join(ne_words)).strip()\n",
        "        # print(ne_phrase)\n",
        "        return (ne_phrase,positions)\n",
        "\n",
        "    def set_stopword_exceptions(self,words):\n",
        "        combined=cachedStopWords+prep_list+article_list+day_list\n",
        "        for word in words:\n",
        "            if word in combined:\n",
        "                self.swSet.add(word)\n",
        "    \n",
        "    def compile_token_embeddings(self, lst):\n",
        "        token_embeddings=[]\n",
        "        for tup in lst:\n",
        "            token = tup[0]\n",
        "            # tup[1] is feature dict, keys 0 to 99\n",
        "            embedding_dict = tup[1]\n",
        "            embedding_arr = np.array([embedding_dict['feat'+str(i)] for i in range(0,100)])\n",
        "            token_embeddings.append(torch.from_numpy(embedding_arr).float())\n",
        "        return token_embeddings\n",
        "\n",
        "    def extract(self, batch_number):\n",
        "        print(\"Running Local NER now\")\n",
        "        time_in=time.time()\n",
        "        self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'TweetSentence','tweetwordList', 'phase1Candidates','start_time','entry_batch'))\n",
        "        df_holder=[]\n",
        "        self.swSet= set()\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates','correct_candidates_tweet'))\n",
        "            #dict1 = {'tweetID':0, 'sentID':0, 'hashtags':'first', 'user':'user', 'TweetSentence':'sentence', 'phase1Candidates':'phase1Out','start_time':'now','entry_batch':'batch_number'}\n",
        "            self.CTrie=trie.Trie(\"ROOT\")\n",
        "            self.phase2stopWordList=[]\n",
        "            self.sentenceID = 0\n",
        "            self.f=0\n",
        "\n",
        "        for key, value in self.gaguilar_sentence_outputs.items():\n",
        "\n",
        "            now = datetime.datetime.now()\n",
        "            tweetID=str(value[0])\n",
        "            sentID=str(value[1])\n",
        "            sentence=str(value[2])\n",
        "\n",
        "            tweetWordList = sentence.split()\n",
        "            enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "            annnotated_mentions=[]\n",
        "\n",
        "            self.sentenceID=key\n",
        "\n",
        "            #need both entity phrases and phrase token positions\n",
        "            entities_from_sentence = value[3]\n",
        "            # print(entities_from_sentence)\n",
        "            phase1Out=\"\"\n",
        "\n",
        "            #extract sentence token level 100d embeddings\n",
        "            token_embedding_list= list(value[4])\n",
        "            entity_aware_embeddings = self.compile_token_embeddings(token_embedding_list)\n",
        "            # for tup in lst:\n",
        "            #     print(tup[0],'=>',tup[1])\n",
        "\n",
        "            just_candidates=[]\n",
        "\n",
        "            # place some necessary filters\n",
        "            entities_from_sentence= list(filter(lambda element: not self.slang_remove(element[0]), entities_from_sentence))\n",
        "            entities_from_sentence= list(map(lambda element: self.start_end_token_check(element), entities_from_sentence))\n",
        "            entities_from_sentence= list(filter(lambda element: not self.all_slang(element[0]), entities_from_sentence))\n",
        "            entities_from_sentence= list(filter(lambda element: len(element[0])>1, entities_from_sentence))\n",
        "            entities_from_sentence= list(filter(lambda element: element[0]!='', entities_from_sentence))\n",
        "\n",
        "            \n",
        "\n",
        "            for candidateTuple in entities_from_sentence:\n",
        "                candidateText, positions = candidateTuple\n",
        "                candidateText=(((candidateText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
        "                candidateText=(self.remAmpersand(candidateText).lstrip('“‘’”')).rstrip('“‘’”')\n",
        "                candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "                candidateText= candidateText.strip()\n",
        "                self.set_stopword_exceptions(candidateText.split())\n",
        "                just_candidates.append(candidateText)\n",
        "                # if(index==9423):\n",
        "                #     print(candidateText)\n",
        "                position = '*'+'*'.join(str(v) for v in positions)\n",
        "                position=position+'*'\n",
        "\n",
        "                phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
        "\n",
        "                combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
        "                if not ((candidateText in combined)|(len(candidateText)<=1)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
        "                    if(self.quickRegex.match(candidateText)):\n",
        "                        self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
        "            \n",
        "            # print('entities_from_sentence:',just_candidates)\n",
        "            #storing the outputs and token embeddings in dataframe tweetID,sen_index\n",
        "            dict1 = {'tweetID':str(tweetID), 'sentID':str(sentID), 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList,'phase1Candidates': just_candidates ,'phase1CandidatesWPositions':phase1Out,\n",
        "                        'contextual_embeddings':entity_aware_embeddings,\n",
        "                        'start_time':now,'entry_batch':batch_number}\n",
        "            df_holder.append(dict1)\n",
        "                    # self.contextual_embeddings[(tweetID,sen_index)] = entity_aware_embeddings\n",
        "\n",
        "        time_out=time.time()\n",
        "        self.append_rows(df_holder)\n",
        "        self.phase2stopWordList=list(set(self.phase2stopWordList)|self.swSet)\n",
        "        self.counter=self.counter+1\n",
        "        \n",
        "        #return (copy.deepcopy(self.df_out),copy.deepcopy(freqs),time_in,time_out)\n",
        "        return (self.df_out,self.contextual_embeddings,self.CTrie,time_in,time_out,self.phase2stopWordList,self.tweet_to_sentences_w_annotation)\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        self.df_out=self.df_out.append(df)\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QiYJ45QEfUB"
      },
      "source": [
        "## **Phase II: Global NER with Phrase Embedder to collect the Entity Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cU04h9cEo51"
      },
      "source": [
        "# coding: utf-8\n",
        "from nltk.corpus import stopwords\n",
        "import pandas  as pd\n",
        "# import NE_candidate_module as ne\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "import copy\n",
        "# import numpy\n",
        "import math\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from scipy import stats\n",
        "import emoji\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import trie as trie\n",
        "import re\n",
        "import ast\n",
        "import pickle\n",
        "import itertools\n",
        "from scipy import spatial\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.cluster import KMeans, MeanShift\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import torch\n",
        "\n",
        "#CLASSIFIER CLASS\n",
        "# import SVM as svm\n",
        "# import entityClassifier as entityClassifier\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"since\",\"hence\",\"onto\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"us\")\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"&;\",\"v.\"] #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"httpurl\",\"n't\",\"&amp;\",\"gee\",\"€\",\"hmm\",\"bye\",\"pls\",\"please\",\"yrs\",\"4get\",\"blah\",\"ooh\",\"ouch\",\"am\",\"tv\",\"ima\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"qft\",\"ip\",\"icymi\",\"bdsm\",\"ah\",\"http\",\"https\",\"pm\",\"omw\",\"pts\",\"pt\",\"ive\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "\n",
        "class GlobalNERModule():\n",
        "\n",
        "\n",
        "    def executor(self,max_batch_value,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others):\n",
        "    # def executor(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others)\n",
        "\n",
        "\n",
        "        # SET CB\n",
        "        # print(phase2stopwordList)\n",
        "        candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag = self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold)\n",
        "        candidate_featureBase_DF.to_csv(\"data/candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        # print(self.good_candidates)\n",
        "\n",
        "        # SET TF \n",
        "        untrashed_tweets=self.set_tf(data_frame_holder, candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "        #mark incomplete tweets\n",
        "        self.set_column_for_candidates_in_incomplete_tweets(candidate_featureBase_DF,untrashed_tweets)\n",
        "\n",
        "        # SAVE INCOMING TWEETS FOR ANNOTATION FOR OTHERS\n",
        "        # self.raw_tweets_for_others=pd.concat([self.raw_tweets_for_others,raw_tweets_for_others ])\n",
        "\n",
        "        # DROP TF\n",
        "        just_converted_tweets=self.get_complete_tf(untrashed_tweets)\n",
        "\n",
        "        #incomplete tweets at the end of current batch\n",
        "        incomplete_tweets=self.get_incomplete_tf(untrashed_tweets)\n",
        "\n",
        "        #all incomplete_tweets---> incomplete_tweets at the end of current batch + incomplete_tweets not reintroduced\n",
        "        # self.incomplete_tweets=incomplete_tweets #without reintroduction--- when everything is reintroduced, just incomplete_tweets\n",
        "        # self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized','annotation','stanford_candidates'])\n",
        "        # self.incomplete_tweets=pd.concat([incomplete_tweets,self.not_reintroduced],ignore_index=True)\n",
        "        self.incomplete_tweets=pd.concat([incomplete_tweets],ignore_index=True)\n",
        "\n",
        "        print('completed tweets:',len(just_converted_tweets))\n",
        "        print('incomplete tweets:',len(incomplete_tweets))\n",
        "\n",
        "\n",
        "        # #recording tp, fp , f1\n",
        "        # #self.accuracy_tuples_prev_batch.append((just_converted_tweets.tp.sum(), just_converted_tweets.total_mention.sum(),just_converted_tweets.fp.sum(),just_converted_tweets.fn.sum()))\n",
        "\n",
        "\n",
        "        # #operations for getting ready for next batch.\n",
        "        # # self.incomplete_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "        # self.incomplete_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "        # self.counter=self.counter+1\n",
        "\n",
        "        self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets)\n",
        "\n",
        "        time_out=time.time()\n",
        "\n",
        "        self.calculate_tp_fp_f1(z_score_threshold,untrashed_tweets,raw_tweets_for_others)\n",
        "\n",
        "        if(self.counter==(max_batch_value+1)):\n",
        "            # self.just_converted_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "            self.just_converted_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "\n",
        "            print('completed tweets: ', len(self.just_converted_tweets),'incomplete tweets: ', len(self.incomplete_tweets))\n",
        "            \n",
        "            print(len(list(self.just_converted_tweets.columns.values)))\n",
        "            print(len(list(self.incomplete_tweets.columns.values)))\n",
        "\n",
        "            combined_frame_list=[self.just_converted_tweets, self.incomplete_tweets]\n",
        "            complete_tweet_dataframe = pd.concat(combined_frame_list)\n",
        "\n",
        "            print('final tally: ', (len(self.just_converted_tweets)+len(self.incomplete_tweets)), len(complete_tweet_dataframe))\n",
        "\n",
        "            #to groupby tweetID and get one tuple per tweetID\n",
        "            complete_tweet_dataframe_grouped_df= (complete_tweet_dataframe.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "            complete_tweet_dataframe_grouped_df['tweetID']=complete_tweet_dataframe_grouped_df['tweetID'].astype(int)\n",
        "            self.complete_tweet_dataframe_grouped_df_sorted=(complete_tweet_dataframe_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "            print(list(self.complete_tweet_dataframe_grouped_df_sorted.columns.values))\n",
        "\n",
        "\n",
        "        #self.aggregator_incomplete_tweets.to_csv(\"all_incompletes.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "\n",
        "        #self.just_converted_tweets.to_csv(\"all_converteds.csv\", sep=',', encoding='utf-8')\n",
        "        #self.incomplete_tweets.to_csv(\"incomplete_for_last_batch.csv\", sep=',', encoding='utf-8')\n",
        "        return candidate_featureBase_DF, self.complete_tweet_dataframe_grouped_df_sorted,time_out \n",
        "        # return candidate_featureBase_DF, untrashed_tweets,time_out\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,entity_phrase_embedder,device,filename):\n",
        "        self.counter=0\n",
        "        self.decay_factor=2**(-1/2)\n",
        "        self.decay_base_staggering=2\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "        self.device=device\n",
        "        self.save_file= filename\n",
        "\n",
        "        self.entity_phrase_embedder = entity_phrase_embedder\n",
        "\n",
        "        context_feature_list=['cf_'+str(i) for i in range(100)]\n",
        "        self.candidateBaseHeaders=['candidate', 'batch', 'length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+context_feature_list+['cumulative']\n",
        "\n",
        "        ## When not running on a notebook\n",
        "        # self.entity_classifier = entityClassifier.EntityClassifier('data/candidate_train_records_100d.csv',True,self.device)\n",
        "\n",
        "        ################################## To do a fresh training\n",
        "        self.entity_classifier = EntityClassifier('data/candidate_train_records_large_100d.csv',True,self.device)\n",
        "\n",
        "        ################################### To Load a pre-trained model\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_100d.csv',False,self.device)\n",
        "\n",
        "        self.complete_tweet_dataframe_grouped_df_sorted=pd.DataFrame([], columns=['tweetID', 'TweetSentence', 'ambiguous_candidates', 'annotation', 'candidates_with_label', 'completeness', 'current_minus_entry', 'entry_batch', 'hashtags', 'index', 'only_good_candidates', 'output_mentions', 'phase1Candidates', 'sentID', 'stanford_candidates', 'user'])\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1_generic(self,raw_tweets_for_others,state_of_art):\n",
        "\n",
        "        column_candidates_holder = raw_tweets_for_others[state_of_art].tolist()\n",
        "        \n",
        "\n",
        "        column_annot_holder= raw_tweets_for_others['mentions_other'].tolist()\n",
        "        # column_annot_holder= raw_tweets_for_others['annotation_limited types'].tolist()\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=column_annot_holder[idx].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=ast.literal_eval(column_candidates_holder[idx])\n",
        "            else:\n",
        "                output_mentions_list=column_candidates_holder[idx].split(',')\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            # print(annotated_mention_list,output_mentions_list)\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall))    \n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "        # tweet_ids_df[\"tp\"+state_of_art]=true_positive_holder\n",
        "        # tweet_ids_df[\"fn\"+state_of_art]=false_negative_holder\n",
        "        # tweet_ids_df['fp'+state_of_art]= false_positive_holder\n",
        "        \n",
        "        # if(state_of_art==\"ritter_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"ritter_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # if(state_of_art==\"stanford_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"stanford_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "    def calculate_tp_fp_f1_alternate(self,raw_tweets_for_others, state_of_art):\n",
        "        if(state_of_art=='neuroner'):\n",
        "            # fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/venezuela_input_2019-05-10_12-33-16-15380/mentions_output.txt\",\"r\")\n",
        "            fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/tweets_3K_input_2019-04-26_16-49-32-20455/mentions_output.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='stanford_candidates'):\n",
        "            fp= open(\"/home/satadisha/Desktop/stanford-ner-2016-10-31/stanford_venezuela_mentions.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='ritter_candidates'):\n",
        "            tweets_ritter=pd.read_csv(\"/home/satadisha/Desktop/GitProjects/twitter_nlp-master/ritter-venezuela-output.csv\",sep =',', keep_default_na=False)\n",
        "        if(state_of_art=='calai_candidates'):\n",
        "            tweets_calai=pd.read_csv(\"/home/satadisha/Desktop/opencalai_versions/venezuela_output.csv\",sep =',', keep_default_na=False)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        for index, row in raw_tweets_for_others.iterrows():\n",
        "            \n",
        "\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=row['mentions_other'].split(';')\n",
        "            # tweet_level_candidate_list=row['annotation_limited types'].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='ritter_candidates'):\n",
        "                # output_mentions_list=ast.literal_eval(mentions_list[idx])\n",
        "                output_mentions_list=tweets_ritter[tweets_ritter['ID']==row['ID']]['Output'].iloc[0].split(',')\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=tweets_calai[tweets_calai['ID']==row['ID']]['calai_candidates'].iloc[0].split(',')\n",
        "            if((state_of_art=='neuroner')|(state_of_art=='stanford_candidates')):\n",
        "                #for 3k Tweets:\n",
        "                idx=int(row['ID'])\n",
        "\n",
        "                # #for others:\n",
        "                # idx=int(row['ID']-1)\n",
        "\n",
        "                output_mentions_list=mentions_list[idx].split(',')\n",
        "\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            print(annotated_mention_list,output_mentions_list)\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall)) \n",
        "        if(state_of_art==\"neuroner\"):\n",
        "            self.accuracy_vals_neuroner.append((f_measure,precision,recall))\n",
        "\n",
        "\n",
        "        # output_mentions_list= mentions_list[output_index].split(',')\n",
        "#     # output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "#     # output_mentions_list=list(filter(lambda element: element !='', output_mentions_list))\n",
        "\n",
        "    def calculate_tp_fp_f1_for_others(self,raw_tweets_for_others):\n",
        "\n",
        "        opencalai=\"calai_candidates\"\n",
        "        stanford=\"stanford_candidates\"\n",
        "        ritter=\"ritter_candidates\"\n",
        "        neuroner=\"neuroner\"\n",
        "\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,opencalai)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,stanford)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,ritter)\n",
        "\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,opencalai)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,stanford)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,ritter)\n",
        "\n",
        "        self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,neuroner)\n",
        "\n",
        "    #################################\n",
        "    #input candidate_feature_Base\n",
        "    #output candidate_feature_Base with [\"Z_score\"], [\"probability\"],[\"class\"]\n",
        "    #################################\n",
        "    def classify_candidate_base(self,z_score_threshold,candidate_featureBase_DF):\n",
        "\n",
        "        # #filtering test set based on z_score\n",
        "        # mert1=candidate_featureBase_DF['cumulative'].as_matrix()\n",
        "        #frequency_array = np.array(list(map(lambda val: val[0], sortedCandidateDB.values())))\n",
        "        # zscore_array1=stats.zscore(mert1)\n",
        "\n",
        "        zscore_array1=stats.zscore(candidate_featureBase_DF['cumulative'])\n",
        "\n",
        "        candidate_featureBase_DF['Z_ScoreUnweighted']=zscore_array1\n",
        "        cumulative_threshold=1.0 #set threshold here\n",
        "        z_score_threshold=candidate_featureBase_DF[candidate_featureBase_DF['cumulative']==cumulative_threshold].Z_ScoreUnweighted.tolist()[0]\n",
        "        print(cumulative_threshold,z_score_threshold)\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_new_with_z_score.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        #multi-word infrequent candidates ---> to be used for recall correction\n",
        "        infrequent_candidates=candidate_featureBase_DF[(candidate_featureBase_DF['Z_ScoreUnweighted'] < z_score_threshold) & (candidate_featureBase_DF.length>1)].candidate.tolist()\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF[candidate_featureBase_DF['Z_ScoreUnweighted'] >= z_score_threshold]\n",
        "\n",
        "        # # #######################with one unified classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        return (self.entity_classifier.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with only semantic classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # return (self.entity_classifierII.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with two separate classifiers--- requires some additional lines of code\n",
        "        # candidateList = candidate_featureBase_DF.candidate.tolist()\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # candidate_featureBase_DF.set_index(\"candidate\", inplace=True)\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF.index.name)\n",
        "\n",
        "        # # returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # candidate_featureBase_DF_classifierI = self.entity_classifierI.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "        # candidate_featureBase_DF_classifierII = self.entity_classifierII.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "\n",
        "        # # candidate_featureBase_DF_classifierI.to_csv('classifierI.csv', sep=',', encoding='utf-8')\n",
        "        # # candidate_featureBase_DF_classifierII.to_csv('classifierII.csv', sep=',', encoding='utf-8') #candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # final_probability_dict = {}\n",
        "        # print5=0\n",
        "        # for candidate in candidateList:\n",
        "        #     prob1 = candidate_featureBase_DF_classifierI.loc[candidate]['probability']\n",
        "        #     prob2 = candidate_featureBase_DF_classifierII.loc[candidate]['probability']\n",
        "        #     final_probability = max(prob1,prob2)\n",
        "        #     if(print5<5):\n",
        "        #         print(candidate,prob1,prob2,final_probability)\n",
        "        #         print5+=1\n",
        "        #     final_probability_dict[candidate] = final_probability\n",
        "\n",
        "        # candidate_featureBase_DF[\"probability\"] = pd.Series(final_probability_dict)\n",
        "\n",
        "        # candidate_featureBase_DF.reset_index(drop=False,inplace=True)\n",
        "        # print('after columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF[['candidate','probability']].head(5))\n",
        "\n",
        "        # return (candidate_featureBase_DF,infrequent_candidates)\n",
        "\n",
        "    # recall_correction\n",
        "    def set_partition_dict(self,candidate_featureBase_DF,infrequent_candidates):\n",
        "\n",
        "        #print(list(self.partition_dict.keys()))\n",
        "        ambiguous_bad_candidates=candidate_featureBase_DF[(((candidate_featureBase_DF.status==\"a\")|(candidate_featureBase_DF.status==\"b\"))&(candidate_featureBase_DF.length.astype(int)>1))]\n",
        "        good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "        flag1=False\n",
        "        flag2=False\n",
        "        if(len(ambiguous_bad_candidates)>0):\n",
        "            # ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1) \n",
        "            # ambiguous_bad_candidates_wFilter= ambiguous_bad_candidates[ambiguous_bad_candidates.max_column=='substring-cap']\n",
        "\n",
        "            #good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "            #print(ambiguous_bad_candidates_wFilter.candidate.tolist())\n",
        "\n",
        "            # for candidate in ambiguous_bad_candidates_wFilter.candidate.tolist():\n",
        "            for candidate in ambiguous_bad_candidates.candidate.tolist():\n",
        "                \n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "\n",
        "            flag1= True\n",
        "        if(len(infrequent_candidates)>0):\n",
        "            #print(len(ambiguous_bad_candidates_wFilter.candidate.tolist()))\n",
        "\n",
        "            for candidate in infrequent_candidates:\n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "            flag2= True\n",
        "        print(list(self.partition_dict.keys()))\n",
        "        return (flag1|flag2)\n",
        "\n",
        "    def get_reintroduced_tweets(self, reintroduction_threshold):\n",
        "        #no reintroduction\n",
        "        #no preferential selection\n",
        "        print(\"incomplete tweets in batch: \",len(self.incomplete_tweets))\n",
        "        # print(list(self.incomplete_tweets.columns.values))\n",
        "\n",
        "        reintroduced_tweets=self.incomplete_tweets[(self.counter-self.incomplete_tweets['entry_batch'])<=reintroduction_threshold]\n",
        "        self.not_reintroduced=self.incomplete_tweets[~self.incomplete_tweets.index.isin(reintroduced_tweets.index)]\n",
        "\n",
        "        print(\"reintroduced tweets: \",len(reintroduced_tweets))\n",
        "        # for i in range(self.counter):\n",
        "        #     print('i:',len(self.incomplete_tweets[self.incomplete_tweets['entry_batch']==i]))\n",
        "        return reintroduced_tweets\n",
        "\n",
        "    def set_cb(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold):\n",
        "\n",
        "        #input new_tweets, z_score, Updated candidatebase of phase1\n",
        "        #output candidate_featureBase_DF, Incomplete_tweets\n",
        "        data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "\n",
        "        candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted= self.extract(TweetBase,CTrie,phase2stopwordList,0)\n",
        "        phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "        phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "        df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        if((self.counter>0)&(len(self.incomplete_tweets)>0)):\n",
        "\n",
        "            #tweet candidates for Reintroduction\n",
        "            reintroduced_tweets=self.get_reintroduced_tweets(reintroduction_threshold)\n",
        "            candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted = self.extract(reintroduced_tweets,CTrie,phase2stopwordList,1)\n",
        "            phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "            phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "            df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        #print(len(df_holder))\n",
        "        data_frame_holder = pd.DataFrame(df_holder)\n",
        "        #print(len(self.incomplete_tweets),len(data_frame_holder),len(candidate_featureBase_DF))\n",
        "        \n",
        "\n",
        "        #set ['probabilities'] for candidate_featureBase_DF\n",
        "        candidate_featureBase_DF,self.infrequent_candidates= self.classify_candidate_base(z_score_threshold,candidate_featureBase_DF)\n",
        "\n",
        "        # set readable labels (a,g,b) for candidate_featureBase_DF based on ['probabilities.']\n",
        "        candidate_featureBase_DF=self.set_readable_labels(candidate_featureBase_DF)\n",
        "\n",
        "        self.good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        self.ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "        self.bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        print('good_candidates:',self.good_candidates)\n",
        "        print('bad_candidates:',self.bad_candidates)\n",
        "        print('ambiguous_candidates:',self.ambiguous_candidates)\n",
        "\n",
        "        # entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.good_candidates)]\n",
        "        # non_entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.bad_candidates)]\n",
        "        # ambiguous_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.ambiguous_candidates)]\n",
        "\n",
        "        correction_flag=self.set_partition_dict(candidate_featureBase_DF,self.infrequent_candidates)\n",
        "\n",
        "        ambiguous_turned_good=[]\n",
        "        ambiguous_turned_bad=[]\n",
        "        ambiguous_remaining_ambiguous=[]\n",
        "        converted_candidates=[]\n",
        "\n",
        "        #['probability'],['a,g,b']\n",
        "        return candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag\n",
        "\n",
        "\n",
        "        #flush out completed tweets\n",
        "        # input candidate base, looped over tweets (incomplete tweets+ new tweets)\n",
        "        # output: incomplete tweets (a tags in it.), incomplete_tweets[\"Complete\"]\n",
        "    def set_tf(self,data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        return self.set_completeness_in_tweet_frame(data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "    def get_incomplete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==False]\n",
        "\n",
        "    def get_complete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==True]\n",
        "\n",
        "    def compute_seen_tweets_so_far(self,start_batch,end_batch):\n",
        "        if(start_batch==end_batch):\n",
        "            sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch]\n",
        "\n",
        "\n",
        "        sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch:]\n",
        "\n",
        "\n",
        "        counter=0\n",
        "        for elem in sliced_seen_tweets:\n",
        "            counter=counter+elem\n",
        "\n",
        "        return counter\n",
        "\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "    #ME_EXTR=Mention.Mention_Extraction()\n",
        "\n",
        "\n",
        "    # experiment function\n",
        "    def set_x_axis(self,just_converted_tweets_for_current_batch):\n",
        "\n",
        "        self.incomplete_tweets.to_csv(\"set_x_axis_debug.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        self.incomplete_tweets['number_of_seen_tweets'] = self.incomplete_tweets['entry_batch'].apply(lambda x: self.compute_seen_tweets_so_far(x,self.counter))\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"entry_vs_tweet_seen_ratio\"]=self.incomplete_tweets['entry_batch']/self.incomplete_tweets['number_of_seen_tweets']\n",
        "\n",
        "\n",
        "        #counter_list= \n",
        "        self.incomplete_tweets[\"ratio_entry_vs_current\"]=self.incomplete_tweets['entry_batch']/self.counter\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"current_minus_entry\"]=self.counter-self.incomplete_tweets['entry_batch']\n",
        "\n",
        "        just_converted_tweets_for_current_batch[\"current_minus_entry\"]=self.counter-just_converted_tweets_for_current_batch['entry_batch']\n",
        "\n",
        "        return just_converted_tweets_for_current_batch\n",
        "\n",
        "\n",
        "\n",
        "    def set_column_for_candidates_in_incomplete_tweets(self,candidate_featureBase_DF,input_to_eval):\n",
        "\n",
        "        incomplete_candidates= input_to_eval['2nd Iteration Candidates'].tolist()\n",
        "\n",
        "        candidate_featureBase_DF= candidate_featureBase_DF.set_index('candidate')\n",
        "\n",
        "        candidate_with_label_holder=[]\n",
        "        one_level=[]\n",
        "        \n",
        "\n",
        "        for sentence_level_candidates in incomplete_candidates:\n",
        "\n",
        "            one_level.clear()\n",
        "\n",
        "            for candidate in sentence_level_candidates:\n",
        "                if candidate.lower() in candidate_featureBase_DF.index:\n",
        "                    # label=candidate_featureBase_DF.get_value(candidate.lower(),'status')\n",
        "                    label=candidate_featureBase_DF.at[candidate.lower(),'status']\n",
        "                    one_level.append((candidate,label))\n",
        "                else:\n",
        "                    one_level.append((candidate,\"na\"))\n",
        "\n",
        "            candidate_with_label_holder.append(copy.deepcopy(one_level))\n",
        "\n",
        "\n",
        "        input_to_eval[\"candidates_with_label\"]=candidate_with_label_holder\n",
        "        debug_candidates_label_list= input_to_eval['candidates_with_label'].tolist()\n",
        "        candidates_filtered_g_labeled=[]\n",
        "        row_level_candidates=[]\n",
        "        index_outer=0\n",
        "\n",
        "        candidates_filtered_a_labeled=[]\n",
        "        row_level_a_candidates=[]\n",
        "\n",
        "        for sentence_level in debug_candidates_label_list:\n",
        "\n",
        "            # sentence_level_candidates_unnormalized= incomplete_candidates_unnormalized[index_outer]\n",
        "            row_level_candidates.clear()\n",
        "            row_level_a_candidates.clear()\n",
        "            for candidate in sentence_level:\n",
        "                if(candidate[1]==\"g\"):\n",
        "                    candidate_str = self.erode_article(candidate[0])\n",
        "                    row_level_candidates.append(candidate_str)\n",
        "                if(((candidate[1]==\"b\")|(candidate[1]==\"a\"))&(candidate[0]==\"US\")):\n",
        "                    # print('here')\n",
        "                    row_level_candidates.append(candidate[0])\n",
        "                if(candidate[1]==\"a\"):\n",
        "                    row_level_a_candidates.append(candidate[0])\n",
        "\n",
        "            candidates_filtered_g_labeled.append(copy.deepcopy(row_level_candidates))\n",
        "            candidates_filtered_a_labeled.append(copy.deepcopy(row_level_a_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "\n",
        "        input_to_eval[\"only_good_candidates\"]=candidates_filtered_g_labeled\n",
        "        input_to_eval[\"ambiguous_candidates\"]=candidates_filtered_a_labeled\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1(self,z_score_threshold,input_to_eval,raw_tweets_for_others):\n",
        "\n",
        "        input_to_eval_grouped_df= (input_to_eval.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "        input_to_eval_grouped_df['tweetID']=input_to_eval_grouped_df['tweetID'].astype(int)\n",
        "        input_to_eval_df_sorted=(input_to_eval_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "\n",
        "        print(len(input_to_eval_df_sorted),len(raw_tweets_for_others))\n",
        "        \n",
        "        print(set(input_to_eval_df_sorted['tweetID'].values.tolist())-set(raw_tweets_for_others['ID'].values.tolist()))\n",
        "\n",
        "        input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_other'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_limited_types'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['annotation_limited types'].iloc[0])\n",
        "\n",
        "        column_candidates_holder = input_to_eval_df_sorted['only_good_candidates'].tolist()\n",
        "        # column_candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "\n",
        "        file1 = open(self.save_file+\".txt\", \"w\")\n",
        "        \n",
        "\n",
        "        column_annot_holder= input_to_eval_df_sorted['annotation'].tolist()\n",
        "        \n",
        "        output_str=''\n",
        "        # print(column_candidates_holder)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "        total_annotation=0\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        print('=========================Gaguilar_candidates')\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            output_mentions_list=[]\n",
        "            tweet_level_candidate_list=str(column_annot_holder[idx]).split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: quickRegex.match(element), annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: ((element !='')&(element !='nan')), annotated_mention_list))\n",
        "\n",
        "            for lst in column_candidates_holder[idx]:\n",
        "                output_mentions_list.extend(lst)\n",
        "\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "            output_mentions_list=list(map(lambda element: element.lower(), output_mentions_list))\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            print(idx, annotated_mention_list,output_mentions_list)\n",
        "            output_str+=','.join(output_mentions_list)+'\\n'\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= self.normalize(annotated_mention_list.pop())\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "            self.true_positive_count+=tp_counter_inner\n",
        "            self.false_positive_count+=fp_counter_inner\n",
        "            self.false_negative_count+=fn_counter_inner\n",
        "\n",
        "            print(self.true_positive_count,self.false_positive_count,self.false_negative_count)\n",
        "\n",
        "        print('TP||||FP||||FN')\n",
        "        print(self.true_positive_count,self.false_positive_count,self.false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        precision=(self.true_positive_count)/(self.true_positive_count+self.false_positive_count)\n",
        "        recall=(self.true_positive_count)/(self.true_positive_count+self.false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "        \n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "\n",
        "        # true_positive_count_IPQ=true_positive_count\n",
        "        # false_positive_count_IPQ = false_positive_count\n",
        "        # false_negative_count_IPQ= false_negative_count\n",
        "        # total_mention_count_IPQ=total_mentions\n",
        "\n",
        "\n",
        "        # tp_count=0\n",
        "        # tm_count=0\n",
        "        # fp_count=0\n",
        "        # fn_count=0\n",
        "\n",
        "        # for idx,tup in enumerate(self.accuracy_tuples_prev_batch):\n",
        "        #     # print(idx,tup)\n",
        "        #     tp_count+=tup[0]\n",
        "        #     tm_count+=tup[1]\n",
        "        #     fp_count+=tup[2]\n",
        "        #     fn_count+=tup[3]\n",
        "\n",
        "\n",
        "\n",
        "        # tp_count+=true_positive_count_IPQ\n",
        "        # tm_count+=total_mention_count_IPQ\n",
        "        # fp_count+=false_positive_count_IPQ\n",
        "        # fn_count+=false_negative_count_IPQ\n",
        "\n",
        "        # precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        # recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        # f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        file1.write(output_str)\n",
        "        file1.close()\n",
        "\n",
        "\n",
        "\n",
        "        self.accuracy_vals=(z_score_threshold,f_measure,precision,recall)\n",
        "\n",
        "        print('Precision:',precision)\n",
        "        print('Recall:',recall)\n",
        "        print('F1:',f_measure)\n",
        "\n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "\n",
        "        # input_to_eval[\"tp\"]=true_positive_holder\n",
        "        # input_to_eval[\"fn\"]=false_negative_holder\n",
        "        # input_to_eval['fp']= false_positive_holder\n",
        "        # input_to_eval[\"total_mention\"]=total_mention_holder\n",
        "\n",
        "        # input_to_eval[\"ambigious_not_in_annot\"]=ambigious_not_in_annotation_holder\n",
        "        # input_to_eval[\"inverted_loss\"]=input_to_eval[\"tp\"]/( input_to_eval[\"fn\"]+input_to_eval[\"ambigious_not_in_annot\"])\n",
        "\n",
        "        return input_to_eval\n",
        "\n",
        "\n",
        "    def recall_correction(self,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder):\n",
        "\n",
        "        corrected_phase2_candidates_holder=[]\n",
        "        index_outer=0\n",
        "        for candidates in phase2_candidates_holder:\n",
        "            unnormalized_candidates=phase2_unnormalized_candidates_holder[index_outer]\n",
        "            corrected_phase2_candidates=[]\n",
        "            for idx, candidate in enumerate(candidates):\n",
        "                unnormalized_candidate=unnormalized_candidates[idx]\n",
        "                # if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates))):\n",
        "                if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))):   #do this only for 3K tweets\n",
        "                    #print(candidate, self.partition_dict[candidate])\n",
        "                    corrected_phase2_candidates.extend(self.partition_dict[candidate])\n",
        "                else:\n",
        "                    if(((candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))&(candidate=='us')&(unnormalized_candidate=='US')):\n",
        "                        # print(index_outer)\n",
        "                        candidate=unnormalized_candidate\n",
        "                    # if((len(candidate.strip().strip(string.punctuation).split())>1)&(candidate.strip().strip(string.punctuation).split()[0].lower() in ['a','an','the'])):\n",
        "                    #     candidate = (' '.join(candidate.strip().strip(string.punctuation).split()[1:])).strip()\n",
        "                    corrected_phase2_candidates.append(candidate)\n",
        "            corrected_phase2_candidates_holder.append(copy.deepcopy(corrected_phase2_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "        \n",
        "        #print(corrected_phase2_candidates_holder)\n",
        "        data_frame_holder['2nd Iteration Candidates']=corrected_phase2_candidates_holder\n",
        "\n",
        "        return corrected_phase2_candidates_holder,data_frame_holder                  \n",
        "\n",
        "\n",
        "    def erode_article(self, entity_string):\n",
        "        if((len(entity_string.strip().strip(string.punctuation).split())>1)&(entity_string.lower().strip().strip(string.punctuation).split()[0] in ['a','an','the'])):\n",
        "            # print(entity_string)\n",
        "            entity_string = ' '.join(entity_string.strip().strip(string.punctuation).split()[1:])\n",
        "            # print(entity_string)\n",
        "        return entity_string.strip()\n",
        "\n",
        "\n",
        "    def set_completeness_in_tweet_frame(self,data_frame_holder,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        #print(candidate_featureBase_DF.head())\n",
        "        good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        merged_g_b= bad_candidates+good_candidates\n",
        "\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_before_labeling_comp.csv\", sep=',', encoding='utf-8')\n",
        "        ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "\n",
        "        if(correction_flag):\n",
        "            phase2_candidates_holder,data_frame_holder=self.recall_correction(candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder)\n",
        "\n",
        "         \n",
        "\n",
        "        \n",
        "        truth_vals=[False if any(x not in merged_g_b for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        intermediate_output_mentions=[list(filter(lambda candidate: ((candidate in good_candidates))|(candidate=='US'), list1)) for list1 in phase2_candidates_holder]\n",
        "\n",
        "        output_mentions=[list(map(lambda candidate: self.erode_article(candidate), list1)) for list1 in intermediate_output_mentions]\n",
        "\n",
        "        # truth_vals=[False if any(x in ambiguous_candidates for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        # for list1 in phase2_candidates_holder:\n",
        "        #     if any(x in ambiguous_candidates  for x in list1):\n",
        "        #         truth_vals.append(False)\n",
        "        #     else:\n",
        "        #         truth_vals.append(True)\n",
        " \n",
        "\n",
        "\n",
        "        #print(truth_vals)\n",
        "        completeness_series = pd.Series( (v for v in truth_vals) )\n",
        "        output_mentions_series = pd.Series( (v for v in output_mentions) )\n",
        "\n",
        "\n",
        "        data_frame_holder['output_mentions']=output_mentions_series\n",
        "        data_frame_holder['completeness']=completeness_series\n",
        "        data_frame_holder[\"current_minus_entry\"]=self.counter-data_frame_holder['entry_batch']\n",
        "\n",
        "        return data_frame_holder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_readable_labels(self,candidate_featureBase_DF):\n",
        "\n",
        "        #candidate_featureBase_DF['status'] = candidate_featureBase_DF['probability'].apply(lambda x: set(x).issubset(good_candidates))\n",
        "        candidate_featureBase_DF['status']='ne'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.50]='g'\n",
        "        candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.50)] = 'a'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
        "\n",
        "        return candidate_featureBase_DF\n",
        "\n",
        "\n",
        "\n",
        "    def normalize(self,word):\n",
        "        strip_op=word\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        if strip_op.endswith(\"'s\"):\n",
        "            li = strip_op.rsplit(\"'s\", 1)\n",
        "            return ''.join(li)\n",
        "        elif strip_op.endswith(\"’s\"):\n",
        "            li = strip_op.rsplit(\"’s\", 1)\n",
        "            return ''.join(li)\n",
        "        else:\n",
        "            return strip_op\n",
        "        #return strip_op\n",
        "\n",
        "    \n",
        "    def isSubstring(self,to_increase_element,id_to_incr,comparison_holder,phase1_holder_holder_copy):\n",
        "        combined_list=comparison_holder[id_to_incr]+phase1_holder_holder_copy[id_to_incr]\n",
        "\n",
        "        for idx,val in enumerate(comparison_holder[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[5]) and to_increase_element[5] != val[5]):\n",
        "                    return True\n",
        "        for idx,val in enumerate(phase1_holder_holder_copy[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[2]) and to_increase_element[5] != val[2]):\n",
        "                    return True   \n",
        "                \n",
        "        return False\n",
        "\n",
        "\n",
        "    def calculate_pmi(self,big,x1,x2,total):\n",
        "        big__= float(big/total)\n",
        "        x1__=float(x1/total)\n",
        "        x2__=float(x2/total)\n",
        "        pmi= math.log(big__/(x1__*x2__),2.71828182845)\n",
        "        pklv=big__*pmi\n",
        "        #return (1/(1+math.exp(-1*pmi)))\n",
        "        npmi= pmi/(-1.0*(math.log(big__,2.71828182845)))\n",
        "        return npmi,pklv\n",
        "        #return pklv\n",
        "\n",
        "    def multiSlice(self,s,cutpoints,good_candidates):\n",
        "        k = len(cutpoints)\n",
        "        multislices=[]\n",
        "        if k == 0:\n",
        "            curr_candidate=self.normalize(' '.join(s))\n",
        "\n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]        \n",
        "        else:\n",
        "            \n",
        "            curr_candidate=self.normalize(' '.join(s[:cutpoints[0]]))\n",
        "            alt_list=[curr_candidate]\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]\n",
        "\n",
        "            alt_list.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1))\n",
        "            multislices.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1) if self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) in good_candidates)\n",
        "\n",
        "            curr_candidate=self.normalize(' '.join(s[cutpoints[k-1]:]))\n",
        "            alt_list.append(curr_candidate)\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices.append(curr_candidate)\n",
        "            # print('::',alt_list)\n",
        "        return multislices\n",
        "\n",
        "\n",
        "\n",
        "    def get_substring_candidates(self,candidate_words,good_candidates):\n",
        "        n = len(candidate_words)\n",
        "        all_partitions=[]\n",
        "        all_partitions_length=[]\n",
        "        cuts = list(range(1,n))\n",
        "        for k in range(n):\n",
        "            # all_partitions_inner=[]\n",
        "            partition_list=[]\n",
        "            partition_length_list=[]\n",
        "            for cutpoints in itertools.combinations(cuts,k):\n",
        "                ret_list=self.multiSlice(candidate_words,cutpoints,good_candidates)\n",
        "                if(ret_list):\n",
        "                    partition_length=sum([len(elem.split()) for elem in ret_list])\n",
        "                    # print('==',ret_list,partition_length)\n",
        "                    if(partition_length==len(candidate_words)):\n",
        "                        return ret_list\n",
        "                    partition_list.append(ret_list)\n",
        "                    partition_length_list.append(partition_length)\n",
        "                    # yield ret_list\n",
        "            # print('------')\n",
        "            if(partition_length_list):\n",
        "                max_index=partition_length_list.index(max(partition_length_list))\n",
        "                all_partitions.append(partition_list[max_index])\n",
        "                all_partitions_length.append(partition_length_list[max_index])\n",
        "        # print(all_partitions)\n",
        "        if(all_partitions_length):\n",
        "            max_index=all_partitions_length.index(max(all_partitions_length))\n",
        "            # print(all_partitions[max_index])\n",
        "            return all_partitions[max_index]\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "\n",
        "    def verify(self, subsequence, CTrie):\n",
        "        return CTrie.__contains__(subsequence)\n",
        "\n",
        "\n",
        "\n",
        "    def check_sequence(self, sequence, l, CTrie):\n",
        "        result=[]\n",
        "        subsequence_length=l\n",
        "        while(subsequence_length>0):\n",
        "            shift=len(sequence)-subsequence_length\n",
        "            verified_subsequence=[]\n",
        "            verified=False\n",
        "            for i in range(0,shift+1):\n",
        "                list1=sequence[i:(i+subsequence_length)]\n",
        "                text=' '.join(str(e[0]) for e in list1)\n",
        "                subsequence=(self.normalize(text)).split()\n",
        "                #print(\"search for\", subsequence)\n",
        "                if self.verify(subsequence, CTrie):\n",
        "                    verified_subsequence.append(i)\n",
        "                    verified_subsequence.append(i+subsequence_length)\n",
        "                    #print(subsequence)\n",
        "                    #print(subsequence,[(verified_subsequence[0]-0),(int(sequence[-1][1])-verified_subsequence[1])])\n",
        "                    verified=True\n",
        "                    break\n",
        "            if(verified):\n",
        "                result.append(sequence[verified_subsequence[0]:verified_subsequence[1]])\n",
        "                if(verified_subsequence[0]-0)>0:\n",
        "                    subequence_to_check=sequence[0:verified_subsequence[0]]\n",
        "                    #since tokens before the starting position of the verified subsequence have already been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length-1))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                if(int(sequence[-1][1])-verified_subsequence[1])>0:\n",
        "                    subequence_to_check=sequence[(verified_subsequence[1]):]\n",
        "                    #since tokens following the end position of the verified subsequence have not been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                return result\n",
        "            else:\n",
        "                subsequence_length-=1\n",
        "        return result\n",
        "\n",
        "    # def flatten(self,mylist, outlist,ignore_types=(str, bytes, int, ne.NE_candidate)):\n",
        "    def flatten(self,mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "        if mylist !=[]:\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.flatten(item, outlist)\n",
        "                else:\n",
        "                    # if isinstance(item,ne.NE_candidate):\n",
        "                    #     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
        "                    #     item.reset_length()\n",
        "                    # else:\n",
        "                    if type(item)!= int:\n",
        "                        item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "            \n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "            # elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "            #     temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "            #     if(temp1):\n",
        "            #         temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "            #     temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "                #print(index, temp)\n",
        "            # elif \"...\" in word:\n",
        "            #     #print(\"here\")\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"...\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"...\")):\n",
        "            #             temp=list(map(lambda elem: elem+'...', temp))\n",
        "            #         else:\n",
        "            #            temp=list(map(lambda elem: elem+'...', temp[:-1]))+[temp[-1]]\n",
        "            #     # temp.append(temp1[-1])\n",
        "            # elif \"..\" in word:\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"..\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"..\")):\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp))\n",
        "            #         else:\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp[:-1]))+[temp[-1]]\n",
        "            #     #temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def get_Candidates(self, sequence, CTrie,flag):\n",
        "        #flag: debug_flag\n",
        "        candidateList=[]\n",
        "        left=0\n",
        "        start_node=CTrie\n",
        "        last_cand=\"NAN\"\n",
        "        last_cand_substr=\"\"\n",
        "        reset=False\n",
        "        right=0\n",
        "        while (right < len(sequence)):\n",
        "            # if(flag):\n",
        "            #     print(right)\n",
        "            if(reset):\n",
        "                start_node=CTrie\n",
        "                last_cand_substr=\"\"\n",
        "                left=right\n",
        "            curr_text=sequence[right][0]\n",
        "            curr_pos=[sequence[right][1]]\n",
        "            #normalized curr_text\n",
        "            curr=self.normalize(sequence[right][0])\n",
        "            cand_str=self.normalize(last_cand_substr+\" \"+curr)\n",
        "            cand_str_wPunct=(last_cand_substr+\" \"+curr_text).lower()\n",
        "            last_cand_sequence=sequence[left:(right+1)]\n",
        "            last_cand_text=' '.join(str(e[0]) for e in last_cand_sequence)\n",
        "            last_cand_text_norm=self.normalize(' '.join(str(e[0]) for e in last_cand_sequence))\n",
        "            if(flag):\n",
        "                print(\"==>\",cand_str,last_cand_text_norm)\n",
        "            if((cand_str==last_cand_text_norm)&((curr in start_node.path.keys())|(curr_text.lower() in start_node.path.keys()))):\n",
        "            #if (((curr in start_node.path.keys())&(cand_str==last_cand_text_norm))|(curr_text.lower() in start_node.path.keys())):\n",
        "                if flag:\n",
        "                    print(\"=>\",cand_str,last_cand_text)\n",
        "                reset=False\n",
        "                if (curr_text.lower() in start_node.path.keys()):\n",
        "                    if (start_node.path[curr_text.lower()].value_valid):\n",
        "                        last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                        last_cand_batch=start_node.path[curr_text.lower()].feature_list[-1]\n",
        "                        last_cand=last_cand_text\n",
        "                    elif(curr in start_node.path.keys()):\n",
        "                        if ((start_node.path[curr].value_valid)):\n",
        "                            last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                            last_cand=last_cand_text\n",
        "                            last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                        else:\n",
        "                            if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                                #print(\"hehe\",cand_str)\n",
        "                                right=left\n",
        "                                reset=True\n",
        "                    else:\n",
        "                        if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                            #print(\"hehe\",cand_str)\n",
        "                            right=left\n",
        "                            reset=True\n",
        "                elif ((start_node.path[curr].value_valid)&(cand_str==last_cand_text_norm)):\n",
        "                    # if flag:\n",
        "                    #     print(\"==\",last_cand_text)\n",
        "                    last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                    last_cand=last_cand_text\n",
        "                    last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                else:\n",
        "                    if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                        #print(\"hehe\",cand_str)\n",
        "                        right=left\n",
        "                        reset=True\n",
        "                if((curr_text.lower() in start_node.path.keys())&(cand_str==last_cand_text_norm)):\n",
        "                    start_node=start_node.path[curr_text.lower()]\n",
        "                    last_cand_substr=cand_str_wPunct\n",
        "                else:\n",
        "                    start_node=start_node.path[curr]\n",
        "                    last_cand_substr=cand_str\n",
        "            else:\n",
        "                #print(\"=>\",cand_str,last_cand_text)\n",
        "                if(last_cand!=\"NAN\"):\n",
        "                    candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "                    last_cand=\"NAN\"\n",
        "                    if(start_node!=CTrie):\n",
        "                        start_node=CTrie\n",
        "                        last_cand_substr=\"\"\n",
        "                        if curr in start_node.path.keys():\n",
        "                            # if(flag):\n",
        "                            #     print(\"here\",curr)\n",
        "                            reset=False\n",
        "                            if start_node.path[curr].value_valid:\n",
        "                                last_cand_text=curr_text\n",
        "                                last_cand_pos=curr_pos\n",
        "                                last_cand=last_cand_text\n",
        "                                last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                            left=right\n",
        "                            start_node=start_node.path[curr]\n",
        "                            last_cand_substr=curr\n",
        "                        else:\n",
        "                            reset=True\n",
        "                    else:\n",
        "                        reset=True\n",
        "                else:\n",
        "                    if(left<right):\n",
        "                        # if(flag):\n",
        "                        #     print(sequence[(left+1):(right+1)])\n",
        "                        #candidateList.extend(self.get_Candidates(sequence[(left+1):(right+1)], CTrie, flag))\n",
        "                        right=left\n",
        "                        # if(flag):\n",
        "                        #     print(\"++\",right)\n",
        "                    reset=True\n",
        "            right+=1\n",
        "        # if(flag):\n",
        "        #     print(last_cand)\n",
        "        if(last_cand!=\"NAN\"):\n",
        "            candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "        return candidateList\n",
        "\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        #self.data_frame_holder=self.data_frame_holder.append(df,ignore_index=True)\n",
        "        #self.data_frame_holder=self.data_frame_holder.reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def join_token_tuples(self,list_of_tuples):\n",
        "        #print(string.punctuation)\n",
        "        combined_str=(' '.join(tuple[0] for tuple in list_of_tuples)).lstrip(string.punctuation).rstrip(string.punctuation).strip()\n",
        "        combined_pos='*'.join(str(tuple[1]) for tuple in list_of_tuples)\n",
        "        combined_tuple=(combined_str,combined_pos,list_of_tuples[0][2],list_of_tuples[0][3],list_of_tuples[0][4],list_of_tuples[0][5],list_of_tuples[0][6])\n",
        "        return combined_tuple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def all_capitalized(self,candidate):\n",
        "        strip_op=candidate\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip())\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        prep_article_list=prep_list+article_list+self.phase2stopwordList\n",
        "        word_list=strip_op.split()\n",
        "        for i in range(len(word_list)):\n",
        "            word=word_list[i]\n",
        "            if((word[0].isupper())|(word[0].isdigit())):\n",
        "                continue\n",
        "            else:\n",
        "                if(word in prep_article_list):\n",
        "                    if (i!=0):\n",
        "                        continue\n",
        "                    else:\n",
        "                        return False\n",
        "                elif(word in conjoiner):\n",
        "                    continue\n",
        "                else:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def check_feature_update(self, candidate_tuple,non_discriminative_flag):\n",
        "        #print(candidate_tuple)\n",
        "        if(non_discriminative_flag):\n",
        "            return 7\n",
        "        candidateText=candidate_tuple[0]\n",
        "        position=candidate_tuple[1]\n",
        "        word_list=candidateText.split()\n",
        "        if candidateText.islower():\n",
        "            return 6\n",
        "        elif candidateText.isupper():\n",
        "            return 5\n",
        "        elif (len(word_list)==1):\n",
        "            #start-of-sentence-check\n",
        "            if self.all_capitalized(candidateText):\n",
        "                if(int(position[0])==0):\n",
        "                    return 4\n",
        "                else:\n",
        "                    return 2\n",
        "            else:\n",
        "                return 3\n",
        "        else:\n",
        "            if(self.all_capitalized(candidateText)):\n",
        "                return 2\n",
        "            else:\n",
        "                return 3\n",
        "\n",
        "\n",
        "    def update_Candidatedict(self,candidate_tuple,non_discriminative_flag,contextual_embedding_vector):\n",
        "        candidateText=candidate_tuple[0]\n",
        "        normalized_candidate=self.normalize(candidateText)\n",
        "        # print('adding:',normalized_candidate)\n",
        "\n",
        "        feature_list=[]\n",
        "        if(normalized_candidate in self.CandidateBase_dict.keys()):\n",
        "            feature_list=self.CandidateBase_dict[normalized_candidate]\n",
        "        else:\n",
        "            # feature_list=[0]*9 # only syntax\n",
        "            feature_list=[0]*109 # context embedding: 100\n",
        "\n",
        "            feature_list[0]=self.counter\n",
        "            feature_list[1]=len(normalized_candidate.split())\n",
        "\n",
        "        #syntax_feature to update\n",
        "        feature_to_update=self.check_feature_update(candidate_tuple,non_discriminative_flag)\n",
        "        feature_list[feature_to_update]+=1\n",
        "\n",
        "        # add up the context embedding features\n",
        "        # print(len(contextual_embedding_vector))\n",
        "        feature_list[8:-1]= np.add(feature_list[8:-1],contextual_embedding_vector).tolist()\n",
        "\n",
        "        #increment cumulative frequency\n",
        "        feature_list[-1]+=1\n",
        "        self.CandidateBase_dict[normalized_candidate]=feature_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def extract(self,tweetBaseInput,CTrie,phase2stopwordList,new_or_old):\n",
        "\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #output_queue\n",
        "            self.data_frame_holder_OQ=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.not_reintroduced=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.CandidateBase_dict= {}\n",
        "            self.ambiguous_candidate_distanceDict_prev={}\n",
        "            self.partition_dict={}\n",
        "            self.good_candidates=[]\n",
        "            self.bad_candidates=[]\n",
        "            self.ambiguous_candidates=[]\n",
        "\n",
        "            self.aggregator_incomplete_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.just_converted_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            #self.data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates'])\n",
        "            self.raw_tweets_for_others=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "\n",
        "            self.accuracy_tuples_prev_batch=[]\n",
        "            self.accuracy_vals=[]\n",
        "            \n",
        "            #frequency_w_decay related information\n",
        "            self.ambiguous_candidates_reintroduction_dict={}\n",
        "\n",
        "            #### other systems\n",
        "            self.accuracy_vals_stanford=[]\n",
        "            self.accuracy_vals_opencalai=[]\n",
        "            self.accuracy_vals_ritter=[]\n",
        "            self.accuracy_vals_neuroner=[]\n",
        "\n",
        "            self.number_of_seen_tweets_per_batch=[]\n",
        "        self.phase2stopwordList=phase2stopwordList\n",
        "        self.number_of_seen_tweets_per_batch.append(len(tweetBaseInput))\n",
        "\n",
        "\n",
        "        #data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase1_holder_holder=[]\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "        \n",
        "        #candidateBase_holder=[]\n",
        "\n",
        "        #this has to be changed to an append function since IPQ already has incomplete tweets from prev batch  \n",
        "        #print(len(tweetBaseInput))\n",
        "        #immediate_processingQueue = pd.concat([self.incomplete_tweets,TweetBase ])\n",
        "        #immediate_processingQueue.to_csv(\"impq.csv\", sep=',', encoding='utf-8')\n",
        "        \n",
        "\n",
        "\n",
        "        #print('In Phase 2',len(immediate_processingQueue))\n",
        "        #immediate_processingQueue=immediate_processingQueue.reset_index(drop=True)\n",
        "        combined_list_here=([]+list(cachedStopWords)+chat_word_list+day_list+month_list+article_list+prep_list)\n",
        "        combined_list_filtered=list(filter(lambda word: word not in (prep_list+article_list+month_list+self.phase2stopwordList), combined_list_here))\n",
        "        #--------------------------------------PHASE II---------------------------------------------------\n",
        "        for index, row in tweetBaseInput.iterrows():\n",
        "\n",
        "            #phase 1 candidates for one sentence\n",
        "            phase1_holder=[]\n",
        "\n",
        "            tweetText=str(row['TweetSentence'])\n",
        "            tweetWordList = row['tweetwordList']\n",
        "            sentID=str(row['sentID'])\n",
        "            tweetID=str(row['tweetID'])\n",
        "            phase1Candidates=str(row['phase1CandidatesWPositions'])\n",
        "            batch=int(row['entry_batch'])\n",
        "            contextual_embeddings_dict= {ind: embedding for ind, embedding in enumerate(row['contextual_embeddings'])}\n",
        "            \n",
        "            # print('====',tweetID,sentID)\n",
        "            # print('tweetWordList:',tweetWordList)\n",
        "            \n",
        "            non_discriminative_flag=False\n",
        "            phase1CandidatesList=[]\n",
        "\n",
        "            # print('phase1Candidates:',phase1Candidates)\n",
        "\n",
        "            if (phase1Candidates !='nan'):\n",
        "                phase1Raw=phase1Candidates.split(\"||\")\n",
        "                phase1Raw = list(filter(None, phase1Raw))\n",
        "\n",
        "\n",
        "                for entities_with_loc in phase1Raw:\n",
        "                    entity_to_store=entities_with_loc.split(\"::\")[0]\n",
        "                    #print(entity_to_store)\n",
        "                    position=entities_with_loc.split(\"::\")[1]\n",
        "                    #print(position)\n",
        "                    phase1_holder.append((entity_to_store,position))\n",
        "                    phase1_holder.clear()\n",
        "                    phase1CandidatesList.append(entity_to_store.lower())\n",
        "\n",
        "                phase1_holder_holder.append(copy.deepcopy(phase1_holder))\n",
        "                \n",
        "\n",
        "            else:\n",
        "                non_discriminative_flag=True\n",
        "                phase1_holder_holder.append([])\n",
        "\n",
        "            \n",
        "            # tweetWordList=self.getWords(tweetText)\n",
        "            # tweetWordList= [(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "            \n",
        "            tweetWordList_stopWords=list(filter (lambda word: ((((word[0].strip()).strip(string.punctuation)).lower() in combined_list_filtered)|(word[0].strip() in string.punctuation)|(word[0].startswith('@'))|(word[0].startswith('#'))), tweetWordList))\n",
        "\n",
        "            # phase 2 candidate tuples without stopwords for a sentence\n",
        "            c=[(y[0],str(y[1]),tweetID,sentID,'ne',batch,time) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "            #c=[(y[0],str(y[1])) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "\n",
        "            \n",
        "            sequences=[]\n",
        "            for k, g in groupby(enumerate(c), lambda element: element[0]-int(element[1][1])):\n",
        "                sequences.append(list(map(itemgetter(1), g)))\n",
        "\n",
        "            # print('phaseII candidates:')\n",
        "            ne_candidate_list=[]\n",
        "            for sequence in sequences:\n",
        "                seq_candidate_list=self.get_Candidates(sequence, CTrie,False)\n",
        "                if(seq_candidate_list):\n",
        "                    for candidate_tuple in seq_candidate_list:\n",
        "                        # print(candidate_tuple)\n",
        "\n",
        "                        # extract candidate token embeddings\n",
        "                        candidate_token_embeddings = torch.stack([contextual_embeddings_dict[int(position)] for position in candidate_tuple[1]])\n",
        "                        # print('candidate with token_embeddings:',candidate_tuple[0],candidate_token_embeddings.shape,len(candidate_tuple[1]))\n",
        "\n",
        "                        # !!necessary because this function during training receives [1,n,100] tensors that it squeezes; so might screw up 1-token sentences\n",
        "                        candidate_embedding = self.entity_phrase_embedder.getEmbedding(candidate_token_embeddings.unsqueeze(0))\n",
        "                        # print('candidate_embedding:',candidate_embedding.shape)\n",
        "\n",
        "                        #inserts into CandidateBase: Syntax and Context Feature Setting\n",
        "                        if not ((float(batch)<self.counter)&(candidate_tuple[-1]<self.counter)):\n",
        "                            self.update_Candidatedict(candidate_tuple,non_discriminative_flag,candidate_embedding.tolist())\n",
        "\n",
        "                    ne_candidate_list.extend(seq_candidate_list)\n",
        "\n",
        "            phase2_candidates=[self.normalize(e[0]) for e in ne_candidate_list]\n",
        "            phase2_candidates_unnormalized=[e[0] for e in ne_candidate_list]\n",
        "\n",
        "            phase2_candidates_holder.append(phase2_candidates)\n",
        "            phase2_unnormalized_candidates_holder.append(phase2_candidates_unnormalized)\n",
        "\n",
        "            dict1 = {'entry_batch':batch, 'tweetID':tweetID, 'sentID':sentID, 'TweetSentence':tweetText, 'phase1Candidates':phase1CandidatesList,'2nd Iteration Candidates':phase2_candidates,'2nd Iteration Candidates Unnormalized':phase2_candidates_unnormalized}\n",
        "\n",
        "            df_holder.append(dict1)\n",
        "            \n",
        "            #-------------------------------------------------------------------END of 1st iteration: RESCAN+CANDIDATE_UPDATION-----------------------------------------------------------\n",
        "\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_large_300d.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidateBase_dict_filtered={}\n",
        "        # for candidate in self.CandidateBase_dict:\n",
        "        #     if(candidate in candidate_list):\n",
        "        #         candidateBase_dict_filtered[candidate] = self.CandidateBase_dict[candidate]\n",
        "\n",
        "        # # candidateBase_dict_filtered = self.CandidateBase_dict\n",
        "\n",
        "        # candidate_featureBase_DF_filtered=pd.DataFrame.from_dict(candidateBase_dict_filtered, orient='index')\n",
        "        # candidate_featureBase_DF_filtered.columns=self.candidateBaseHeaders[1:]\n",
        "        # candidate_featureBase_DF_filtered.index.name=self.candidateBaseHeaders[0]\n",
        "        # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered.reset_index(drop=False)\n",
        "        # # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered[candidate_featureBase_DF_filtered['cumulative'] >= 5.0]\n",
        "\n",
        "        # print('before:', len(self.CandidateBase_dict),'after:',len(candidateBase_dict_filtered))\n",
        "        \n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "        # candidate_featureBase_DF_filtered['class'] = candidate_featureBase_DF_filtered.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # # candidate_featureBase_DF_filtered['class'] = 0\n",
        "\n",
        "        # candidate_featureBase_DF_filtered.to_csv(\"data/candidate_train_records_large_100d.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        candidate_featureBase_DF = pd.DataFrame.from_dict(self.CandidateBase_dict, orient='index')\n",
        "        candidate_featureBase_DF.columns = self.candidateBaseHeaders[1:]\n",
        "        candidate_featureBase_DF.index.name = self.candidateBaseHeaders[0]\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF.reset_index(drop=False)\n",
        "\n",
        "        return candidate_featureBase_DF,df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "\n",
        "        ## self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        ## self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets_for_current_batch)\n",
        "\n",
        "\n",
        "    def finish(self):\n",
        "        return self.accuracy_vals\n",
        "\n",
        "    def finish_other_systems(self):\n",
        "        stanford_f1=[]\n",
        "        stanford_precision=[]\n",
        "        stanford_recall=[]\n",
        "        print(\"*****************************************STANFORD***********************\")\n",
        "        for i in self.accuracy_vals_stanford:\n",
        "            stanford_f1.append(i[0])\n",
        "            stanford_precision.append(i[1])\n",
        "            stanford_recall.append(i[2])\n",
        "            # print(i)\n",
        "        print('stanford_f1:', stanford_f1)\n",
        "        print('stanford_precision:', stanford_precision)\n",
        "        print('stanford_recall:', stanford_recall)\n",
        "\n",
        "        print(sum(stanford_f1)/len(stanford_f1))\n",
        "        print(sum(stanford_precision)/len(stanford_precision))\n",
        "        print(sum(stanford_recall)/len(stanford_recall))\n",
        "\n",
        "        print(\"*****************************************Opencalai***********************\")\n",
        "        opencalai_f1=[]\n",
        "        opencalai_precision=[]\n",
        "        opencalai_recall=[]\n",
        "        for i in self.accuracy_vals_opencalai:\n",
        "            opencalai_f1.append(i[0])\n",
        "            opencalai_precision.append(i[1])\n",
        "            opencalai_recall.append(i[2])\n",
        "        print('opencalai_f1:', opencalai_f1)\n",
        "        print('opencalai_precision:', opencalai_precision)\n",
        "        print('opencalai_recall:', opencalai_recall)\n",
        "\n",
        "        print(sum(opencalai_f1)/len(opencalai_f1))\n",
        "        print(sum(opencalai_precision)/len(opencalai_precision))\n",
        "        print(sum(opencalai_recall)/len(opencalai_recall))\n",
        "        print(\"*****************************************Ritter***********************\")\n",
        "        ritter_f1=[]\n",
        "        ritter_precision=[]\n",
        "        ritter_recall=[]\n",
        "        for i in self.accuracy_vals_ritter:\n",
        "            ritter_f1.append(i[0])\n",
        "            ritter_precision.append(i[1])\n",
        "            ritter_recall.append(i[2])\n",
        "        print('ritter_f1:', ritter_f1)\n",
        "        print('ritter_precision:', ritter_precision)\n",
        "        print('ritter_recall:', ritter_recall)\n",
        "\n",
        "        print(sum(ritter_f1)/len(ritter_f1))\n",
        "        print(sum(ritter_precision)/len(ritter_precision))\n",
        "        print(sum(ritter_recall)/len(ritter_recall))\n",
        "        print(\"*****************************************Neuroner***********************\")\n",
        "        neuroner_f1=[]\n",
        "        neuroner_precision=[]\n",
        "        neuroner_recall=[]\n",
        "        for i in self.accuracy_vals_neuroner:\n",
        "            neuroner_f1.append(i[0])\n",
        "            neuroner_precision.append(i[1])\n",
        "            neuroner_recall.append(i[2])\n",
        "        print('neuroner_f1:', neuroner_f1)\n",
        "        print('neuroner_precision:', neuroner_precision)\n",
        "        print('neuroner_recall:',neuroner_recall)\n",
        "\n",
        "        print(sum(neuroner_f1)/len(neuroner_f1))\n",
        "        print(sum(neuroner_precision)/len(neuroner_precision))\n",
        "        print(sum(neuroner_recall)/len(neuroner_recall))\n",
        "\n",
        "        return (self.accuracy_vals_stanford,self.accuracy_vals_opencalai,self.accuracy_vals_ritter,self.accuracy_vals_neuroner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfCP0UkWHnXB"
      },
      "source": [
        "## **Running the Engine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TxMA6-ThnC8"
      },
      "source": [
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "tweets_unpartitoned=pd.read_csv(\"data/covid/covid_2K.csv\",sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3K/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade/roevwade.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika/pikapika.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity/ripcity.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billnye/billnye.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/wnut-test/wnut17test.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "#to train the Entity Classifiers\n",
        "# tweets_unpartitoned=pd.read_csv('data/deduplicated/deduplicated_test.csv',sep =';',keep_default_na=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBJlVu-gk74E",
        "outputId": "1ce5c7b8-1516-4015-a7eb-68f6815db042"
      },
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweets are in memory...\n",
            "1998 1998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRLdfhq-EHQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4346e7f-3f42-4638-a9a1-240577e4f4eb"
      },
      "source": [
        "# from executor import *\n",
        "# sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation = execute('emerging.test.conll')\n",
        "# # print(token_feature_tuple_list[:3])\n",
        "\n",
        "from executor import *\n",
        "\n",
        "# trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training()\n",
        "# sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation = execute('emerging.test.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "\n",
        "tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training('emerging.test.conll')\n",
        "# tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training('btc.emerging.test.conll')\n",
        "# tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training('deduplicated.emerging.test.conll')\n",
        "\n",
        "sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation = execute(tweets_test, labels_test, postag_test, trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4403 4403\n",
            "Argument List: emerging.test.conll\n",
            "data/covid/emerging.test.conll.preproc.url\n",
            "data/covid/emerging.test.conll.preproc.url.postag\n",
            "Loading tweets...\n",
            "Loading pos tags...\n",
            "3091 3091\n",
            "Loading twitter embeddings...\n",
            "Loading gazetteers embeddings...\n",
            "Building neural network...\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 00062: early stopping\n",
            "Saved model to disk\n",
            "got features...\n",
            "got xseq...\n",
            "Generating encodings...\n",
            "reached prediction\n",
            "got probabilities\n",
            "end of prediction\n",
            "starting crf predictor\n",
            "got features...\n",
            "got xseq...\n",
            "crf prediction done\n",
            "35899 4 35899\n",
            "35899 35899 35899 35899\n",
            "100\n",
            "tally: 3091 3091\n",
            "tally: 3091 3091 3091\n",
            "true_positive_count,false_positive_count,false_negative_count:\n",
            "459 252 308\n",
            "precision:  0.6455696202531646\n",
            "recall:  0.5984354628422425\n",
            "f_measure:  0.6211096075778079\n",
            "test time_taken: 12.01735234260559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1pcAMbnT70-"
      },
      "source": [
        "sentence_df_dict format: tweetID, sentID, sentence, list of extracted mentions, token wise 100-d features from DNN;;\n",
        "key-> serialized sentence #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-F2Bkz6V7CE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52286ded-f930-464e-c7b9-eeab1ba829c1"
      },
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))\n",
        "\n",
        "total_time=0\n",
        "g=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qszVo73c9-Re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29b04e1-6d4a-4a6b-ead2-b8b6db2ab068"
      },
      "source": [
        "#Running Phase I\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, sentence_df_dict_gaguilar, tweet_to_sentences_w_annotation, device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started Local NER Engine!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMQJ0J24y6Lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be53d1c8-85cf-4007-eba7-10f0c132c7ec"
      },
      "source": [
        "return_tuple = local_NER_Module.extract(g)\n",
        "\n",
        "tweet_base=return_tuple[0]\n",
        "contextual_embeddings=return_tuple[1]\n",
        "candidate_base=return_tuple[2]\n",
        "elapsedTime= return_tuple[4] - return_tuple[3]\n",
        "phase2stopwordList=return_tuple[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=return_tuple[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Local NER now\n",
            "2.4495418071746826 2.4495418071746826\n",
            "Produced 0\n",
            "**********************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0a_wcAFjV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cf5c6b-8a76-44b3-e5a7-5712ec22544b"
      },
      "source": [
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'deduplicated_test')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'venezuela')\n",
        "\n",
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'covid')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_3K')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'roevwade')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billdeblasio')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'pikapika')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billnye')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'ripcity')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'wnut17')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'btc')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Shape:  torch.Size([692, 101])\n",
            "Output Shape:  torch.Size([692])\n",
            "Epoch 1 : 0.7113622784614563 , 0.7014254331588745\n",
            "69 69\n",
            "precision: 0.875 recall: 0.2641509433962264 f1: 0.4057971014492754\n",
            "making this the checkpoint to save\n",
            "Epoch 2 : 0.6907460272312165 , 0.6835946440696716\n",
            "69 69\n",
            "precision: 0.9047619047619048 recall: 0.3584905660377358 f1: 0.5135135135135135\n",
            "making this the checkpoint to save\n",
            "Epoch 3 : 0.6713542312383651 , 0.6657890677452087\n",
            "69 69\n",
            "precision: 0.875 recall: 0.5283018867924528 f1: 0.6588235294117648\n",
            "making this the checkpoint to save\n",
            "Epoch 4 : 0.6525004148483277 , 0.6478211879730225\n",
            "69 69\n",
            "precision: 0.8648648648648649 recall: 0.6037735849056604 f1: 0.7111111111111112\n",
            "making this the checkpoint to save\n",
            "Epoch 5 : 0.6330786347389221 , 0.6305314302444458\n",
            "69 69\n",
            "precision: 0.8536585365853658 recall: 0.660377358490566 f1: 0.7446808510638298\n",
            "making this the checkpoint to save\n",
            "Epoch 6 : 0.6115579903125763 , 0.6120328307151794\n",
            "69 69\n",
            "precision: 0.8636363636363636 recall: 0.7169811320754716 f1: 0.7835051546391751\n",
            "making this the checkpoint to save\n",
            "Epoch 7 : 0.5936214923858643 , 0.5955771207809448\n",
            "69 69\n",
            "precision: 0.8297872340425532 recall: 0.7358490566037735 f1: 0.78\n",
            "making this the checkpoint to save\n",
            "Epoch 8 : 0.5746432781219483 , 0.5806373953819275\n",
            "69 69\n",
            "precision: 0.8333333333333334 recall: 0.7547169811320755 f1: 0.7920792079207922\n",
            "making this the checkpoint to save\n",
            "Epoch 9 : 0.555884213745594 , 0.5676106214523315\n",
            "69 69\n",
            "precision: 0.8367346938775511 recall: 0.7735849056603774 f1: 0.803921568627451\n",
            "making this the checkpoint to save\n",
            "Epoch 10 : 0.5402612492442131 , 0.5559741258621216\n",
            "69 69\n",
            "precision: 0.8367346938775511 recall: 0.7735849056603774 f1: 0.803921568627451\n",
            "=========\n",
            "making this the checkpoint to save\n",
            "Epoch 11 : 0.5277830556035041 , 0.5458456873893738\n",
            "69 69\n",
            "precision: 0.84 recall: 0.7924528301886793 f1: 0.8155339805825242\n",
            "making this the checkpoint to save\n",
            "Epoch 12 : 0.5114531084895134 , 0.5392230749130249\n",
            "69 69\n",
            "precision: 0.84 recall: 0.7924528301886793 f1: 0.8155339805825242\n",
            "making this the checkpoint to save\n",
            "Epoch 13 : 0.504160588979721 , 0.5327964425086975\n",
            "69 69\n",
            "precision: 0.84 recall: 0.7924528301886793 f1: 0.8155339805825242\n",
            "making this the checkpoint to save\n",
            "Epoch 14 : 0.49422583878040316 , 0.5278674364089966\n",
            "69 69\n",
            "precision: 0.84 recall: 0.7924528301886793 f1: 0.8155339805825242\n",
            "making this the checkpoint to save\n",
            "Epoch 15 : 0.48687827587127686 , 0.524357795715332\n",
            "69 69\n",
            "precision: 0.84 recall: 0.7924528301886793 f1: 0.8155339805825242\n",
            "making this the checkpoint to save\n",
            "Epoch 16 : 0.47553780674934387 , 0.5205724835395813\n",
            "69 69\n",
            "precision: 0.8461538461538461 recall: 0.8301886792452831 f1: 0.8380952380952382\n",
            "making this the checkpoint to save\n",
            "Epoch 17 : 0.46624941378831863 , 0.5178630948066711\n",
            "69 69\n",
            "precision: 0.8461538461538461 recall: 0.8301886792452831 f1: 0.8380952380952382\n",
            "making this the checkpoint to save\n",
            "Epoch 18 : 0.4635800734162331 , 0.516804575920105\n",
            "69 69\n",
            "precision: 0.8431372549019608 recall: 0.8113207547169812 f1: 0.826923076923077\n",
            "making this the checkpoint to save\n",
            "Epoch 19 : 0.46261838674545286 , 0.5128728747367859\n",
            "69 69\n",
            "precision: 0.8490566037735849 recall: 0.8490566037735849 f1: 0.8490566037735849\n",
            "making this the checkpoint to save\n",
            "Epoch 20 : 0.45255967825651167 , 0.5102827548980713\n",
            "69 69\n",
            "precision: 0.8518518518518519 recall: 0.8679245283018868 f1: 0.8598130841121496\n",
            "=========\n",
            "making this the checkpoint to save\n",
            "Epoch 21 : 0.4516967713832855 , 0.5091378092765808\n",
            "69 69\n",
            "precision: 0.8490566037735849 recall: 0.8490566037735849 f1: 0.8490566037735849\n",
            "making this the checkpoint to save\n",
            "Epoch 22 : 0.44830221980810164 , 0.5060895681381226\n",
            "69 69\n",
            "precision: 0.8518518518518519 recall: 0.8679245283018868 f1: 0.8598130841121496\n",
            "making this the checkpoint to save\n",
            "Epoch 23 : 0.4383490592241287 , 0.5039243102073669\n",
            "69 69\n",
            "precision: 0.8518518518518519 recall: 0.8679245283018868 f1: 0.8598130841121496\n",
            "making this the checkpoint to save\n",
            "Epoch 24 : 0.4289685435593128 , 0.5037499666213989\n",
            "69 69\n",
            "precision: 0.8518518518518519 recall: 0.8679245283018868 f1: 0.8598130841121496\n",
            "making this the checkpoint to save\n",
            "Epoch 25 : 0.4248913608491421 , 0.5006457567214966\n",
            "69 69\n",
            "precision: 0.8518518518518519 recall: 0.8679245283018868 f1: 0.8598130841121496\n",
            "making this the checkpoint to save\n",
            "Epoch 26 : 0.42481117099523547 , 0.49773335456848145\n",
            "69 69\n",
            "precision: 0.8518518518518519 recall: 0.8679245283018868 f1: 0.8598130841121496\n",
            "making this the checkpoint to save\n",
            "Epoch 27 : 0.4233392372727394 , 0.49616163969039917\n",
            "69 69\n",
            "precision: 0.8518518518518519 recall: 0.8679245283018868 f1: 0.8598130841121496\n",
            "making this the checkpoint to save\n",
            "Epoch 28 : 0.4215502694249153 , 0.49388450384140015\n",
            "69 69\n",
            "precision: 0.8363636363636363 recall: 0.8679245283018868 f1: 0.8518518518518519\n",
            "making this the checkpoint to save\n",
            "Epoch 29 : 0.41617853194475174 , 0.490673691034317\n",
            "69 69\n",
            "precision: 0.8363636363636363 recall: 0.8679245283018868 f1: 0.8518518518518519\n",
            "making this the checkpoint to save\n",
            "Epoch 30 : 0.4076408624649048 , 0.4885439872741699\n",
            "69 69\n",
            "precision: 0.8363636363636363 recall: 0.8679245283018868 f1: 0.8518518518518519\n",
            "=========\n",
            "making this the checkpoint to save\n",
            "Epoch 31 : 0.40466518998146056 , 0.48863983154296875\n",
            "69 69\n",
            "precision: 0.8363636363636363 recall: 0.8679245283018868 f1: 0.8518518518518519\n",
            "Epoch 32 : 0.4054646462202072 , 0.4858148694038391\n",
            "69 69\n",
            "precision: 0.8363636363636363 recall: 0.8679245283018868 f1: 0.8518518518518519\n",
            "making this the checkpoint to save\n",
            "Epoch 33 : 0.4007551521062851 , 0.4846374988555908\n",
            "69 69\n",
            "precision: 0.8363636363636363 recall: 0.8679245283018868 f1: 0.8518518518518519\n",
            "making this the checkpoint to save\n",
            "Epoch 34 : 0.4027652531862259 , 0.47919824719429016\n",
            "69 69\n",
            "precision: 0.8421052631578947 recall: 0.9056603773584906 f1: 0.8727272727272727\n",
            "making this the checkpoint to save\n",
            "Epoch 35 : 0.40121379494667053 , 0.4795069098472595\n",
            "69 69\n",
            "precision: 0.8392857142857143 recall: 0.8867924528301887 f1: 0.8623853211009174\n",
            "Epoch 36 : 0.39022538661956785 , 0.47529342770576477\n",
            "69 69\n",
            "precision: 0.8421052631578947 recall: 0.9056603773584906 f1: 0.8727272727272727\n",
            "making this the checkpoint to save\n",
            "Epoch 37 : 0.3890008211135864 , 0.47090470790863037\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 38 : 0.3877399869263172 , 0.47232210636138916\n",
            "69 69\n",
            "precision: 0.8421052631578947 recall: 0.9056603773584906 f1: 0.8727272727272727\n",
            "Epoch 39 : 0.3906675688922405 , 0.4688573181629181\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 40 : 0.3772452503442764 , 0.46943047642707825\n",
            "69 69\n",
            "precision: 0.8421052631578947 recall: 0.9056603773584906 f1: 0.8727272727272727\n",
            "=========\n",
            "Epoch 41 : 0.37348450124263766 , 0.46690574288368225\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 42 : 0.37582537680864336 , 0.4650620222091675\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 43 : 0.36803886890411375 , 0.4613618850708008\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 44 : 0.37727112472057345 , 0.46162378787994385\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "Epoch 45 : 0.36950728073716166 , 0.4582809805870056\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 46 : 0.3606792464852333 , 0.4555574059486389\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 47 : 0.3628715381026268 , 0.45722901821136475\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "Epoch 48 : 0.3580213442444801 , 0.451728492975235\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 49 : 0.35631044283509256 , 0.453094482421875\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "Epoch 50 : 0.35375386774539946 , 0.44968128204345703\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "=========\n",
            "making this the checkpoint to save\n",
            "Epoch 51 : 0.35283938571810725 , 0.4483681917190552\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 52 : 0.34980656430125234 , 0.45029404759407043\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "Epoch 53 : 0.3483834944665432 , 0.44634899497032166\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 54 : 0.3494549110531807 , 0.4444887638092041\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 55 : 0.3516165740787983 , 0.4443959593772888\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 56 : 0.34127780199050906 , 0.4433457851409912\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 57 : 0.34234062805771825 , 0.44269776344299316\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 58 : 0.3427145205438137 , 0.44010356068611145\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "making this the checkpoint to save\n",
            "Epoch 59 : 0.3346424959599972 , 0.4383654296398163\n",
            "69 69\n",
            "precision: 0.847457627118644 recall: 0.9433962264150944 f1: 0.8928571428571428\n",
            "making this the checkpoint to save\n",
            "Epoch 60 : 0.33373380079865456 , 0.44127336144447327\n",
            "69 69\n",
            "precision: 0.8448275862068966 recall: 0.9245283018867925 f1: 0.8828828828828829\n",
            "=========\n",
            "Epoch 61 : 0.33560811877250674 , 0.4377883970737457\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 62 : 0.3417940117418766 , 0.4368182122707367\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 63 : 0.3380794942378998 , 0.4339977204799652\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 64 : 0.3253980837762356 , 0.4373113811016083\n",
            "69 69\n",
            "precision: 0.847457627118644 recall: 0.9433962264150944 f1: 0.8928571428571428\n",
            "Epoch 65 : 0.32633528858423233 , 0.4366273880004883\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 66 : 0.32803144827485087 , 0.4327425956726074\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 67 : 0.3218322366476059 , 0.43166041374206543\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 68 : 0.3226725123822689 , 0.4320510923862457\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 69 : 0.32093931883573534 , 0.43256404995918274\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 70 : 0.3234112858772278 , 0.4293684661388397\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "=========\n",
            "making this the checkpoint to save\n",
            "Epoch 71 : 0.3248292125761509 , 0.431878000497818\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 72 : 0.3188311345875263 , 0.43132010102272034\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 73 : 0.31629754304885865 , 0.42812204360961914\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 74 : 0.31436624750494957 , 0.4283774495124817\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 75 : 0.3141500823199749 , 0.4279601573944092\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 76 : 0.31809975430369375 , 0.42854586243629456\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 77 : 0.3141066260635853 , 0.42679381370544434\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 78 : 0.3088275156915188 , 0.428143173456192\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 79 : 0.3081362500786781 , 0.4265812933444977\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 80 : 0.30541528686881064 , 0.42526775598526\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "=========\n",
            "making this the checkpoint to save\n",
            "Epoch 81 : 0.31060331910848615 , 0.42727020382881165\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 82 : 0.31563485562801363 , 0.4274677038192749\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 83 : 0.3043831467628479 , 0.42453813552856445\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "making this the checkpoint to save\n",
            "Epoch 84 : 0.3098862238228321 , 0.4274495542049408\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 85 : 0.2976182200014591 , 0.42587602138519287\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 86 : 0.2949333839118481 , 0.42606210708618164\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 87 : 0.29864999651908875 , 0.42655348777770996\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 88 : 0.29818521738052367 , 0.4269334375858307\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 89 : 0.2977351777255535 , 0.4264209568500519\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 90 : 0.2925029419362545 , 0.4246101379394531\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "=========\n",
            "Epoch 91 : 0.3000482223927975 , 0.42581912875175476\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 92 : 0.2914624117314816 , 0.42543303966522217\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 93 : 0.28753516264259815 , 0.4258715808391571\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 94 : 0.2854919407516718 , 0.42505133152008057\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 95 : 0.2889415882527828 , 0.42620837688446045\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 96 : 0.28951574116945267 , 0.42623278498649597\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 97 : 0.29180650040507317 , 0.4264388084411621\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 98 : 0.28364858850836755 , 0.42656210064888\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 99 : 0.28739266023039817 , 0.42598453164100647\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 100 : 0.2851617708802223 , 0.4257080554962158\n",
            "69 69\n",
            "precision: 0.8524590163934426 recall: 0.9811320754716981 f1: 0.912280701754386\n",
            "=========\n",
            "Epoch 101 : 0.2824076198041439 , 0.42664116621017456\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 102 : 0.2872209019958973 , 0.4252376854419708\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 103 : 0.28373115584254266 , 0.42666542530059814\n",
            "69 69\n",
            "precision: 0.85 recall: 0.9622641509433962 f1: 0.9026548672566371\n",
            "Epoch 104 : 0.2818365581333637 , 0.4260530471801758\n",
            "69 69\n",
            "precision: 0.8524590163934426 recall: 0.9811320754716981 f1: 0.912280701754386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjMQa7kpFlCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e27c390-4a0a-405a-d7d8-8c01227a1e6e"
      },
      "source": [
        "time_in = time.time()\n",
        "candidate_base_post_Phase2, complete_tweet_dataframe,time_out = global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n",
        "# time_out = time.time()\n",
        "\n",
        "print('time_taken', (time_out-time_in))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 -0.19692959244990274\n",
            "torch.Size([482, 1])\n",
            "torch.Size([482])\n",
            "0.04194395989179611 0.9996871948242188\n",
            "good_candidates: ['kylie', 'trump', 'north pole', 'tammy', 'u.s', 'fall river', 'massachusetts', 'mesa', 'italy', 'coronavirus', 'clear creek', 'olav', 'nyc-based', 'nhs', 'std', 'hoover', 'reagan', 'soulja boy', 'donald trump', 'louisiana', 'lauderdale', 'germany', 'scott street park', 'queenslanders', 'google', 'indonesia', 'usa', 'king taco', 'skittles', 'shrek', 'obama', 'dornoch', 'diablo', 'ge', 'tory lanez', 'up10tion', 'se london', 'kansas', 'nebraska', 'iowa', 'missouri', 'idaho', 'nevada', 'hite', 'teddy', 'leo duran', 'instagram', 'debbie downer', 'facetime', 'king floof', 'jj', 'bean', 'fauci', 'liverpool', 'gov', 'edwards', 'india', 'governors', 'baloch', 'st', 'louis', 'snapchat', 'philadelphia', 'bart', 'rogelio', 'god', 'phoebe', 'eva', 'white house', 'corbyn', 'alaskans', 'porter', 'centennial celebration', 'jimmy buffett', 'cedar', 'fca', 'lm', 'nyc', 'ohio', 'missouri city', 'alex', 'japan', 'south dakota', 'bayp', 'new jersey', 'govenors', 'smartphone', 'ron cook', 'johnny', 'wv', 'chicagoans', 'baghdad', 'jibc library', 'arlington', 'preorder system', 'mcdonald', 'abaco', 'kvia', 'matt damon', 'mars', 'ben', 'salop', 'tesco', 'arkansas', 'brad', 'queen izzy', 'costco', 'zoom', 'kentucky wesleyan dining', 'clarksville', 'bruins', 'senators', 'sasquatch', 'nl', 'canadians', 'ontario', 'cole', 'jasper', 'al green', 'nba', 'lynn', 'wellness center', 'new york city', 'caesar blevins', 'wake county', 'chemung county', 'lebro’s restaurant', 'denby dale', 'hy-vee', 'minn', 'tescos', 'winchester', 'kansas city area parks', 'nic', 'penguins', 'miami', 'ky', 'andy beshear', 'walmart', 'titans', 'translink', 'lake merritt', 'waldo', 'hartford', 'tolland', 'hamilton', 'twitter', 'cnn trump', 'central park', 'la', 'colorado', 'rachel', 'dana', 'heather', 'baron', 'chicago', 'dunfermline', 'frankurt', 'george', 'congress', 'new york', 'us governors', 'vp biden', \"giuseppe's brothers pizza\", 'beshear', 'minnesota', 'covid19', 'joey', 'robinson', 'sydney eastern suburbs', 'os', 'governor’s eo', 'luna', 'bengalis', 'bengal', 'youtube', 'luke skywalker', 'lauren meyers', 'dell med', 'michigan', 'city pulse', 'conch house', 'wyoming', 'cleveland', 'mario chain chomp', 'dick durbin', 'bonnie', 'travelodge', 'illinois', 'illinoisans', 'ifc', 'uk arboretum', 'buhari', 'south portland', 'wisconsin', 'tmz', 'newsom', 'jumping jehoshaphat', 'south korea', 'america', 'florida', 'doug', 'forbes', 'south kore', 'tom', 'lake charles louisiana', 'jon jones', 'iowans', 'boulder bookstore', 'kejriwal', 'tmc', 'akhtar hussain', 'west bengal', 'unacast', 'brother shaquille', 'kentucky', 'rana', 'iconic', 'downing street', 'kidsgrove', 'brickell', 'crawley stadi', 'tanjiro', 'mythbusters', 'chris', 'texas', 'virtual charter', 'nypd', 'new yorkers', 'sweden', 'south koreas', 'northwestern', 'eo', 'abney park', 'bronx', 'kerala', 'washington', 'lotus moment', 'kgh', 'harvard', 'cleveland clinic', 'o.c', 'godzilla', 'alabama', 'coles', 'msdnc', 'port', 'spain', 'ocean', 'nino', 'filipinos', 'roxi', 'ga', 'fla', 'morey', 'london', 'ivanka', 'melanie', 'barton springs spillway', 'ky governor beshear', 'madison', 'ireland', 'royal mail', 'fb', 'jon', 'ivey', 'sydney', 'kenya', 'swaziland/eswatini', 'barnhart', 'phillip', 'virginia governor', 'forest hills-social', 'tiger king', 'wsj', 'mercer', \"o'lakes state park\", 'broward county', 'virginia', 'mocha and licorice', 'canada', 'riverside park', 'governor cuomo', 'united states', 'china', 'nhl', 'netflix', 'matt', 'fl', 'kevin', 'jacobson park', 'signe', 'garda', 'mex city', 'greta', 'peter', 'becky', 'hubertus', 'chevy colorado', 'spacex', 'boca', 'roosters', 'theatre', 'boston', 'atv/dirt', 'georgia ave', 'michael', 'suzanne ross', 'ghana', 'rural midwest', 'pennsylvania', 'gelly', 'trick daddy', 'west ham', 'harry kane', 'wilson county', 'alert trump', 'humberside', 'il', 'gov beshear', 'albertans', 'hinshaw', 'torontonians', 'lego', 'orr', 'eric nam', 'czar trump', 'phillip morris', 'elizabeth city', 'boris johnson', 'nottinghamshire', 'africa', 'ring fit', 'bible', 'insta', 'lou', 'mill', 'hyderabad', 'spn', 'canadian blood services', 'atlantic', 'gop', 'kijiji', 'dc', 'pearson airport', 'amherst', 'grove city', 'rapunzel', 'somalis', 'utah', 'katie', 'soquel high school', 'ariana', 'frankie', 'carrier park', 'jimmy kimmel', 'melania trump', 'neponset', 'ultra', 'insomniac', 'broad st', 'winston-salem', 'pnwu', 'phillips', 'clarion', 'sydney airport', 'christians', 'maryland', 'mary', 'la county', 'sierra', 'lala', 'bajan', 'lubo', 'stewart high school', 'stardew valley', 'teagan', 'dadeville city council', 'seattle', 'pentagon', 'stockport', 'lockdown nz', 'dan mini', 'amish', 'hawaii', 'tom brady', 'pats', 'bucs', 'servame', 'courtney cole', 'scotland', 'td', 'georgia', 'island bay', 'candace', 'usb chargers', 'state park officials', 'cambridg', 'mcdonalds', 'woodlands', 'houston', 'village parks', 'melanie fournier', 'new orleans mayor', 'aldi', 'falls run park', 'reston', 'scott', 'allison smith', 'bywater', 'new orleans', 'quaranti', 'union county', 'bahamians', 'chanyeol', 'plymouth', 'los angeles', 'west hollywood', 'beverly hills', 'sam raimi', 'raleigh', 'house party', 'finley fox', 'limerick', 'richmond', 'borno', 'abubakar', 'el-kanemi', 'city of vidalia', 'art rooney ii', 'lila higgins', 'aa', 'mbu athletic']\n",
            "bad_candidates: ['loved ones', 'government', 'hoomans', 'world', 'state', 'county', 'ca', 'cdc', 'us', 'office', 'sla', 'house', 'skylar', 'kai', 'nigeria', 'lockdown', 'police', 'earpods', 'publix', 'meyers', 'federal', 'zak', 'wh', 'cynthia', 'social-distancing', 'swedes', 'muslim', 'americans', 'charlie', 'clover', 'virtual book', 'state parks', 'south china', 'stl', 'governor', 'braves', 'karen', 'exit row']\n",
            "ambiguous_candidates: ['uk', 'kwan 1', 'cnn', 'canadiens', 'ny', 'dominos', 'wa', 'pa', 'gwynith paltrow', 'npr', 'shehu']\n",
            "['south china']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1002: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1003: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1004: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0 0 0\n",
            "77 36 36\n",
            "334 [] ['chicagoans']\n",
            "0 1 0\n",
            "77 37 36\n",
            "335 [] []\n",
            "0 0 0\n",
            "77 37 36\n",
            "336 [] []\n",
            "0 0 0\n",
            "77 37 36\n",
            "337 [] []\n",
            "0 0 0\n",
            "77 37 36\n",
            "338 [] []\n",
            "0 0 0\n",
            "77 37 36\n",
            "339 [] []\n",
            "0 0 0\n",
            "77 37 36\n",
            "340 ['baghdad'] ['baghdad']\n",
            "1 0 0\n",
            "78 37 36\n",
            "341 ['jibc'] ['jibc library']\n",
            "0 1 1\n",
            "78 38 37\n",
            "342 ['arlington'] ['arlington', 'preorder system']\n",
            "1 1 0\n",
            "79 39 37\n",
            "343 [] []\n",
            "0 0 0\n",
            "79 39 37\n",
            "344 [] []\n",
            "0 0 0\n",
            "79 39 37\n",
            "345 [\"mcdonald's\", 'cnn'] ['mcdonald']\n",
            "1 0 1\n",
            "80 39 38\n",
            "346 [] []\n",
            "0 0 0\n",
            "80 39 38\n",
            "347 ['abaco'] ['abaco']\n",
            "1 0 0\n",
            "81 39 38\n",
            "348 [] []\n",
            "0 0 0\n",
            "81 39 38\n",
            "349 [] []\n",
            "0 0 0\n",
            "81 39 38\n",
            "350 [] []\n",
            "0 0 0\n",
            "81 39 38\n",
            "351 [] []\n",
            "0 0 0\n",
            "81 39 38\n",
            "352 [] []\n",
            "0 0 0\n",
            "81 39 38\n",
            "353 ['president trump', 'kvia news'] ['trump', 'kvia']\n",
            "0 2 2\n",
            "81 41 40\n",
            "354 [] []\n",
            "0 0 0\n",
            "81 41 40\n",
            "355 [] []\n",
            "0 0 0\n",
            "81 41 40\n",
            "356 [] []\n",
            "0 0 0\n",
            "81 41 40\n",
            "357 [] []\n",
            "0 0 0\n",
            "81 41 40\n",
            "358 [] []\n",
            "0 0 0\n",
            "81 41 40\n",
            "359 ['matt damon', 'mars'] ['matt damon', 'mars']\n",
            "2 0 0\n",
            "83 41 40\n",
            "360 ['ben'] ['ben']\n",
            "1 0 0\n",
            "84 41 40\n",
            "361 [] []\n",
            "0 0 0\n",
            "84 41 40\n",
            "362 [] []\n",
            "0 0 0\n",
            "84 41 40\n",
            "363 [] []\n",
            "0 0 0\n",
            "84 41 40\n",
            "364 [] []\n",
            "0 0 0\n",
            "84 41 40\n",
            "365 [] []\n",
            "0 0 0\n",
            "84 41 40\n",
            "366 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "85 41 40\n",
            "367 [] []\n",
            "0 0 0\n",
            "85 41 40\n",
            "368 [] ['salop']\n",
            "0 1 0\n",
            "85 42 40\n",
            "369 ['tesco'] ['tesco']\n",
            "1 0 0\n",
            "86 42 40\n",
            "370 ['arkansas'] ['arkansas']\n",
            "1 0 0\n",
            "87 42 40\n",
            "371 [] []\n",
            "0 0 0\n",
            "87 42 40\n",
            "372 ['brad', 'izzy'] ['brad', 'queen izzy']\n",
            "1 1 1\n",
            "88 43 41\n",
            "373 [] []\n",
            "0 0 0\n",
            "88 43 41\n",
            "374 [] []\n",
            "0 0 0\n",
            "88 43 41\n",
            "375 ['wake forest'] []\n",
            "0 0 1\n",
            "88 43 42\n",
            "376 [] []\n",
            "0 0 0\n",
            "88 43 42\n",
            "377 ['costco'] ['costco']\n",
            "1 0 0\n",
            "89 43 42\n",
            "378 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "90 43 42\n",
            "379 [] []\n",
            "0 0 0\n",
            "90 43 42\n",
            "380 [] []\n",
            "0 0 0\n",
            "90 43 42\n",
            "381 ['zoom'] ['zoom']\n",
            "1 0 0\n",
            "91 43 42\n",
            "382 ['tvh'] []\n",
            "0 0 1\n",
            "91 43 43\n",
            "383 [] []\n",
            "0 0 0\n",
            "91 43 43\n",
            "384 ['kentucky wesleyan'] ['kentucky wesleyan dining']\n",
            "0 1 1\n",
            "91 44 44\n",
            "385 [] []\n",
            "0 0 0\n",
            "91 44 44\n",
            "386 [] []\n",
            "0 0 0\n",
            "91 44 44\n",
            "387 ['trump', 'cnn'] ['trump']\n",
            "1 0 1\n",
            "92 44 45\n",
            "388 [] []\n",
            "0 0 0\n",
            "92 44 45\n",
            "389 ['clarksville'] ['clarksville']\n",
            "1 0 0\n",
            "93 44 45\n",
            "390 [] []\n",
            "0 0 0\n",
            "93 44 45\n",
            "391 [] []\n",
            "0 0 0\n",
            "93 44 45\n",
            "392 ['fauci', 'trump'] ['fauci', 'trump']\n",
            "2 0 0\n",
            "95 44 45\n",
            "393 ['bruins', 'canadiens ecqf', 'bruins', 'senators'] ['bruins', 'bruins', 'senators']\n",
            "3 0 1\n",
            "98 44 46\n",
            "394 [] []\n",
            "0 0 0\n",
            "98 44 46\n",
            "395 [] []\n",
            "0 0 0\n",
            "98 44 46\n",
            "396 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "99 44 46\n",
            "397 [] ['sasquatch']\n",
            "0 1 0\n",
            "99 45 46\n",
            "398 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "399 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "400 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "401 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "402 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "403 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "404 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "405 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "406 [] []\n",
            "0 0 0\n",
            "99 45 46\n",
            "407 ['social embeddedness network conference', 'zoom'] ['zoom']\n",
            "1 0 1\n",
            "100 45 47\n",
            "408 [] ['nl']\n",
            "0 1 0\n",
            "100 46 47\n",
            "409 [] []\n",
            "0 0 0\n",
            "100 46 47\n",
            "410 [] []\n",
            "0 0 0\n",
            "100 46 47\n",
            "411 ['print shop', 'covid-19'] []\n",
            "0 0 2\n",
            "100 46 49\n",
            "412 [] []\n",
            "0 0 0\n",
            "100 46 49\n",
            "413 [] []\n",
            "0 0 0\n",
            "100 46 49\n",
            "414 [] []\n",
            "0 0 0\n",
            "100 46 49\n",
            "415 [] []\n",
            "0 0 0\n",
            "100 46 49\n",
            "416 [] []\n",
            "0 0 0\n",
            "100 46 49\n",
            "417 [] []\n",
            "0 0 0\n",
            "100 46 49\n",
            "418 [] []\n",
            "0 0 0\n",
            "100 46 49\n",
            "419 [] ['canadians']\n",
            "0 1 0\n",
            "100 47 49\n",
            "420 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "421 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "422 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "423 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "424 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "425 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "426 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "427 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "428 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "429 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "430 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "431 [] []\n",
            "0 0 0\n",
            "100 47 49\n",
            "432 [] ['nyc-based']\n",
            "0 1 0\n",
            "100 48 49\n",
            "433 [] []\n",
            "0 0 0\n",
            "100 48 49\n",
            "434 [] []\n",
            "0 0 0\n",
            "100 48 49\n",
            "435 [] []\n",
            "0 0 0\n",
            "100 48 49\n",
            "436 [] []\n",
            "0 0 0\n",
            "100 48 49\n",
            "437 ['pokédex'] []\n",
            "0 0 1\n",
            "100 48 50\n",
            "438 [] []\n",
            "0 0 0\n",
            "100 48 50\n",
            "439 [] []\n",
            "0 0 0\n",
            "100 48 50\n",
            "440 [] []\n",
            "0 0 0\n",
            "100 48 50\n",
            "441 [] []\n",
            "0 0 0\n",
            "100 48 50\n",
            "442 ['kulikuli'] []\n",
            "0 0 1\n",
            "100 48 51\n",
            "443 [] []\n",
            "0 0 0\n",
            "100 48 51\n",
            "444 [] []\n",
            "0 0 0\n",
            "100 48 51\n",
            "445 [] []\n",
            "0 0 0\n",
            "100 48 51\n",
            "446 ['ontario'] ['ontario']\n",
            "1 0 0\n",
            "101 48 51\n",
            "447 [] []\n",
            "0 0 0\n",
            "101 48 51\n",
            "448 ['covid-19'] []\n",
            "0 0 1\n",
            "101 48 52\n",
            "449 [] ['nyc-based']\n",
            "0 1 0\n",
            "101 49 52\n",
            "450 [] []\n",
            "0 0 0\n",
            "101 49 52\n",
            "451 ['cole'] ['cole']\n",
            "1 0 0\n",
            "102 49 52\n",
            "452 ['jasper'] ['jasper']\n",
            "1 0 0\n",
            "103 49 52\n",
            "453 [] []\n",
            "0 0 0\n",
            "103 49 52\n",
            "454 ['al green', 'nba'] ['al green', 'nba']\n",
            "2 0 0\n",
            "105 49 52\n",
            "455 [] []\n",
            "0 0 0\n",
            "105 49 52\n",
            "456 [] []\n",
            "0 0 0\n",
            "105 49 52\n",
            "457 [] []\n",
            "0 0 0\n",
            "105 49 52\n",
            "458 [] []\n",
            "0 0 0\n",
            "105 49 52\n",
            "459 [] ['lynn']\n",
            "0 1 0\n",
            "105 50 52\n",
            "460 [] []\n",
            "0 0 0\n",
            "105 50 52\n",
            "461 [] []\n",
            "0 0 0\n",
            "105 50 52\n",
            "462 ['the athletic and wellness center'] ['wellness center']\n",
            "0 1 1\n",
            "105 51 53\n",
            "463 [] []\n",
            "0 0 0\n",
            "105 51 53\n",
            "464 [] []\n",
            "0 0 0\n",
            "105 51 53\n",
            "465 [] []\n",
            "0 0 0\n",
            "105 51 53\n",
            "466 [] []\n",
            "0 0 0\n",
            "105 51 53\n",
            "467 [] []\n",
            "0 0 0\n",
            "105 51 53\n",
            "468 ['donald trump'] ['donald trump']\n",
            "1 0 0\n",
            "106 51 53\n",
            "469 ['new york city', 'ny'] ['new york city']\n",
            "1 0 1\n",
            "107 51 54\n",
            "470 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "471 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "472 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "473 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "474 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "475 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "476 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "477 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "478 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "479 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "480 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "481 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "482 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "483 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "484 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "485 [] []\n",
            "0 0 0\n",
            "107 51 54\n",
            "486 ['tik tok'] []\n",
            "0 0 1\n",
            "107 51 55\n",
            "487 [] []\n",
            "0 0 0\n",
            "107 51 55\n",
            "488 ['caesar blevins'] ['caesar blevins']\n",
            "1 0 0\n",
            "108 51 55\n",
            "489 [] []\n",
            "0 0 0\n",
            "108 51 55\n",
            "490 [] []\n",
            "0 0 0\n",
            "108 51 55\n",
            "491 [] []\n",
            "0 0 0\n",
            "108 51 55\n",
            "492 [] ['wake county']\n",
            "0 1 0\n",
            "108 52 55\n",
            "493 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "494 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "495 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "496 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "497 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "498 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "499 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "500 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "501 [] []\n",
            "0 0 0\n",
            "108 52 55\n",
            "502 ['coronavirus', 'chemung'] ['coronavirus', 'chemung county']\n",
            "1 1 1\n",
            "109 53 56\n",
            "503 [] []\n",
            "0 0 0\n",
            "109 53 56\n",
            "504 ['lebro’s restaurant'] ['lebro’s restaurant']\n",
            "1 0 0\n",
            "110 53 56\n",
            "505 [] []\n",
            "0 0 0\n",
            "110 53 56\n",
            "506 [] []\n",
            "0 0 0\n",
            "110 53 56\n",
            "507 [] []\n",
            "0 0 0\n",
            "110 53 56\n",
            "508 [] []\n",
            "0 0 0\n",
            "110 53 56\n",
            "509 [] []\n",
            "0 0 0\n",
            "110 53 56\n",
            "510 ['denby dale'] ['denby dale']\n",
            "1 0 0\n",
            "111 53 56\n",
            "511 [] []\n",
            "0 0 0\n",
            "111 53 56\n",
            "512 [] ['hy-vee', 'minn']\n",
            "0 2 0\n",
            "111 55 56\n",
            "513 ['tesco'] ['tescos']\n",
            "0 1 1\n",
            "111 56 57\n",
            "514 [] []\n",
            "0 0 0\n",
            "111 56 57\n",
            "515 [] []\n",
            "0 0 0\n",
            "111 56 57\n",
            "516 [] []\n",
            "0 0 0\n",
            "111 56 57\n",
            "517 ['covid-19', 'us'] ['us']\n",
            "1 0 1\n",
            "112 56 58\n",
            "518 [] []\n",
            "0 0 0\n",
            "112 56 58\n",
            "519 [] []\n",
            "0 0 0\n",
            "112 56 58\n",
            "520 [] []\n",
            "0 0 0\n",
            "112 56 58\n",
            "521 [] []\n",
            "0 0 0\n",
            "112 56 58\n",
            "522 [] ['winchester']\n",
            "0 1 0\n",
            "112 57 58\n",
            "523 [] []\n",
            "0 0 0\n",
            "112 57 58\n",
            "524 [] []\n",
            "0 0 0\n",
            "112 57 58\n",
            "525 ['trotro'] []\n",
            "0 0 1\n",
            "112 57 59\n",
            "526 ['instagram'] ['instagram']\n",
            "1 0 0\n",
            "113 57 59\n",
            "527 ['ny', 'nyc'] ['nyc']\n",
            "1 0 1\n",
            "114 57 60\n",
            "528 ['pickleball bust', 'kansas city'] ['kansas city area parks', 'nic']\n",
            "0 2 2\n",
            "114 59 62\n",
            "529 [] []\n",
            "0 0 0\n",
            "114 59 62\n",
            "530 [] []\n",
            "0 0 0\n",
            "114 59 62\n",
            "531 [] []\n",
            "0 0 0\n",
            "114 59 62\n",
            "532 [] []\n",
            "0 0 0\n",
            "114 59 62\n",
            "533 [] ['nhs']\n",
            "0 1 0\n",
            "114 60 62\n",
            "534 [] []\n",
            "0 0 0\n",
            "114 60 62\n",
            "535 [] []\n",
            "0 0 0\n",
            "114 60 62\n",
            "536 [] []\n",
            "0 0 0\n",
            "114 60 62\n",
            "537 [] []\n",
            "0 0 0\n",
            "114 60 62\n",
            "538 ['penguins'] ['penguins']\n",
            "1 0 0\n",
            "115 60 62\n",
            "539 ['miami'] ['miami']\n",
            "1 0 0\n",
            "116 60 62\n",
            "540 [] []\n",
            "0 0 0\n",
            "116 60 62\n",
            "541 [] []\n",
            "0 0 0\n",
            "116 60 62\n",
            "542 ['trump'] ['trump']\n",
            "1 0 0\n",
            "117 60 62\n",
            "543 ['white house', 'coronavirus'] ['white house', 'coronavirus']\n",
            "2 0 0\n",
            "119 60 62\n",
            "544 [] []\n",
            "0 0 0\n",
            "119 60 62\n",
            "545 [] []\n",
            "0 0 0\n",
            "119 60 62\n",
            "546 [] []\n",
            "0 0 0\n",
            "119 60 62\n",
            "547 [] []\n",
            "0 0 0\n",
            "119 60 62\n",
            "548 ['andy beshear'] ['ky', 'gov', 'andy beshear']\n",
            "1 2 0\n",
            "120 62 62\n",
            "549 ['walmart', 'covid-19'] ['walmart']\n",
            "1 0 1\n",
            "121 62 63\n",
            "550 [] ['nyc-based']\n",
            "0 1 0\n",
            "121 63 63\n",
            "551 [] []\n",
            "0 0 0\n",
            "121 63 63\n",
            "552 [] []\n",
            "0 0 0\n",
            "121 63 63\n",
            "553 [] ['titans']\n",
            "0 1 0\n",
            "121 64 63\n",
            "554 [] []\n",
            "0 0 0\n",
            "121 64 63\n",
            "555 [] []\n",
            "0 0 0\n",
            "121 64 63\n",
            "556 [] []\n",
            "0 0 0\n",
            "121 64 63\n",
            "557 [] []\n",
            "0 0 0\n",
            "121 64 63\n",
            "558 ['trump', 'us', 'coronavirus'] ['trump', 'us', 'coronavirus']\n",
            "3 0 0\n",
            "124 64 63\n",
            "559 [] ['translink']\n",
            "0 1 0\n",
            "124 65 63\n",
            "560 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "561 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "562 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "563 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "564 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "565 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "566 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "567 [] []\n",
            "0 0 0\n",
            "124 65 63\n",
            "568 ['lake merritt'] ['lake merritt']\n",
            "1 0 0\n",
            "125 65 63\n",
            "569 ['johnny'] ['johnny']\n",
            "1 0 0\n",
            "126 65 63\n",
            "570 [] []\n",
            "0 0 0\n",
            "126 65 63\n",
            "571 ['waldo'] ['waldo']\n",
            "1 0 0\n",
            "127 65 63\n",
            "572 [] []\n",
            "0 0 0\n",
            "127 65 63\n",
            "573 ['hartford', 'tolland'] ['hartford', 'tolland']\n",
            "2 0 0\n",
            "129 65 63\n",
            "574 [] []\n",
            "0 0 0\n",
            "129 65 63\n",
            "575 [] []\n",
            "0 0 0\n",
            "129 65 63\n",
            "576 ['hamilton'] ['hamilton']\n",
            "1 0 0\n",
            "130 65 63\n",
            "577 [] []\n",
            "0 0 0\n",
            "130 65 63\n",
            "578 [] []\n",
            "0 0 0\n",
            "130 65 63\n",
            "579 [] []\n",
            "0 0 0\n",
            "130 65 63\n",
            "580 [] []\n",
            "0 0 0\n",
            "130 65 63\n",
            "581 [] []\n",
            "0 0 0\n",
            "130 65 63\n",
            "582 ['twitter'] ['twitter']\n",
            "1 0 0\n",
            "131 65 63\n",
            "583 ['trump'] ['cnn trump']\n",
            "0 1 1\n",
            "131 66 64\n",
            "584 ['central park'] ['central park']\n",
            "1 0 0\n",
            "132 66 64\n",
            "585 [] []\n",
            "0 0 0\n",
            "132 66 64\n",
            "586 [] []\n",
            "0 0 0\n",
            "132 66 64\n",
            "587 [] []\n",
            "0 0 0\n",
            "132 66 64\n",
            "588 [] []\n",
            "0 0 0\n",
            "132 66 64\n",
            "589 [] ['la']\n",
            "0 1 0\n",
            "132 67 64\n",
            "590 [] []\n",
            "0 0 0\n",
            "132 67 64\n",
            "591 ['colorado'] ['colorado']\n",
            "1 0 0\n",
            "133 67 64\n",
            "592 [] ['god']\n",
            "0 1 0\n",
            "133 68 64\n",
            "593 [] []\n",
            "0 0 0\n",
            "133 68 64\n",
            "594 ['rachel'] ['rachel']\n",
            "1 0 0\n",
            "134 68 64\n",
            "595 [] []\n",
            "0 0 0\n",
            "134 68 64\n",
            "596 [] []\n",
            "0 0 0\n",
            "134 68 64\n",
            "597 ['dana'] ['dana']\n",
            "1 0 0\n",
            "135 68 64\n",
            "598 [] []\n",
            "0 0 0\n",
            "135 68 64\n",
            "599 [] ['heather']\n",
            "0 1 0\n",
            "135 69 64\n",
            "600 ['coronavirus', 'baron'] ['coronavirus', 'baron']\n",
            "2 0 0\n",
            "137 69 64\n",
            "601 [] []\n",
            "0 0 0\n",
            "137 69 64\n",
            "602 [] []\n",
            "0 0 0\n",
            "137 69 64\n",
            "603 ['summit-hill', 'grand'] []\n",
            "0 0 2\n",
            "137 69 66\n",
            "604 [] []\n",
            "0 0 0\n",
            "137 69 66\n",
            "605 ['chicago'] ['chicago']\n",
            "1 0 0\n",
            "138 69 66\n",
            "606 ['coronavirus'] ['coronavirus', 'dunfermline']\n",
            "1 1 0\n",
            "139 70 66\n",
            "607 [] []\n",
            "0 0 0\n",
            "139 70 66\n",
            "608 [] []\n",
            "0 0 0\n",
            "139 70 66\n",
            "609 [] []\n",
            "0 0 0\n",
            "139 70 66\n",
            "610 [] []\n",
            "0 0 0\n",
            "139 70 66\n",
            "611 [] []\n",
            "0 0 0\n",
            "139 70 66\n",
            "612 [] []\n",
            "0 0 0\n",
            "139 70 66\n",
            "613 [] []\n",
            "0 0 0\n",
            "139 70 66\n",
            "614 ['nyc'] ['nyc', 'coronavirus']\n",
            "1 1 0\n",
            "140 71 66\n",
            "615 [] []\n",
            "0 0 0\n",
            "140 71 66\n",
            "616 ['frankurt'] ['frankurt']\n",
            "1 0 0\n",
            "141 71 66\n",
            "617 ['george'] ['george']\n",
            "1 0 0\n",
            "142 71 66\n",
            "618 [] []\n",
            "0 0 0\n",
            "142 71 66\n",
            "619 [] []\n",
            "0 0 0\n",
            "142 71 66\n",
            "620 [] []\n",
            "0 0 0\n",
            "142 71 66\n",
            "621 [] []\n",
            "0 0 0\n",
            "142 71 66\n",
            "622 [] []\n",
            "0 0 0\n",
            "142 71 66\n",
            "623 [] []\n",
            "0 0 0\n",
            "142 71 66\n",
            "624 [] []\n",
            "0 0 0\n",
            "142 71 66\n",
            "625 [] ['congress']\n",
            "0 1 0\n",
            "142 72 66\n",
            "626 ['new york'] ['new york']\n",
            "1 0 0\n",
            "143 72 66\n",
            "627 [] []\n",
            "0 0 0\n",
            "143 72 66\n",
            "628 [] []\n",
            "0 0 0\n",
            "143 72 66\n",
            "629 [] []\n",
            "0 0 0\n",
            "143 72 66\n",
            "630 [] []\n",
            "0 0 0\n",
            "143 72 66\n",
            "631 ['trump', 'us'] ['trump', 'us governors']\n",
            "1 1 1\n",
            "144 73 67\n",
            "632 [] []\n",
            "0 0 0\n",
            "144 73 67\n",
            "633 [] []\n",
            "0 0 0\n",
            "144 73 67\n",
            "634 ['biden'] ['vp biden']\n",
            "0 1 1\n",
            "144 74 68\n",
            "635 [] [\"giuseppe's brothers pizza\"]\n",
            "0 1 0\n",
            "144 75 68\n",
            "636 [] []\n",
            "0 0 0\n",
            "144 75 68\n",
            "637 [] []\n",
            "0 0 0\n",
            "144 75 68\n",
            "638 [] []\n",
            "0 0 0\n",
            "144 75 68\n",
            "639 [] []\n",
            "0 0 0\n",
            "144 75 68\n",
            "640 [] []\n",
            "0 0 0\n",
            "144 75 68\n",
            "641 ['covid-19'] []\n",
            "0 0 1\n",
            "144 75 69\n",
            "642 [] []\n",
            "0 0 0\n",
            "144 75 69\n",
            "643 [] []\n",
            "0 0 0\n",
            "144 75 69\n",
            "644 [] []\n",
            "0 0 0\n",
            "144 75 69\n",
            "645 [] []\n",
            "0 0 0\n",
            "144 75 69\n",
            "646 [] ['coronavirus']\n",
            "0 1 0\n",
            "144 76 69\n",
            "647 ['beshear'] ['beshear']\n",
            "1 0 0\n",
            "145 76 69\n",
            "648 ['minnesota'] ['minnesota']\n",
            "1 0 0\n",
            "146 76 69\n",
            "649 [] []\n",
            "0 0 0\n",
            "146 76 69\n",
            "650 ['covid19'] ['covid19']\n",
            "1 0 0\n",
            "147 76 69\n",
            "651 [] []\n",
            "0 0 0\n",
            "147 76 69\n",
            "652 ['joey', 'robinson'] ['joey', 'robinson']\n",
            "2 0 0\n",
            "149 76 69\n",
            "653 ['st.louis', 'philadelphia'] ['st', 'louis', 'philadelphia']\n",
            "1 2 1\n",
            "150 78 70\n",
            "654 [] []\n",
            "0 0 0\n",
            "150 78 70\n",
            "655 [] []\n",
            "0 0 0\n",
            "150 78 70\n",
            "656 ['sydney'] ['sydney eastern suburbs', 'os']\n",
            "0 2 1\n",
            "150 80 71\n",
            "657 [] []\n",
            "0 0 0\n",
            "150 80 71\n",
            "658 [] []\n",
            "0 0 0\n",
            "150 80 71\n",
            "659 [] []\n",
            "0 0 0\n",
            "150 80 71\n",
            "660 [] []\n",
            "0 0 0\n",
            "150 80 71\n",
            "661 [] ['governor’s eo']\n",
            "0 1 0\n",
            "150 81 71\n",
            "662 [] ['luna']\n",
            "0 1 0\n",
            "150 82 71\n",
            "663 [] []\n",
            "0 0 0\n",
            "150 82 71\n",
            "664 [] []\n",
            "0 0 0\n",
            "150 82 71\n",
            "665 ['bengalis', 'bengal'] ['bengalis', 'bengal']\n",
            "2 0 0\n",
            "152 82 71\n",
            "666 [] []\n",
            "0 0 0\n",
            "152 82 71\n",
            "667 ['trump'] ['trump']\n",
            "1 0 0\n",
            "153 82 71\n",
            "668 [] []\n",
            "0 0 0\n",
            "153 82 71\n",
            "669 [] []\n",
            "0 0 0\n",
            "153 82 71\n",
            "670 [] []\n",
            "0 0 0\n",
            "153 82 71\n",
            "671 ['u.s.', 'coronavirus'] ['u.s', 'coronavirus']\n",
            "2 0 0\n",
            "155 82 71\n",
            "672 [] []\n",
            "0 0 0\n",
            "155 82 71\n",
            "673 ['nyc'] ['nyc']\n",
            "1 0 0\n",
            "156 82 71\n",
            "674 [] []\n",
            "0 0 0\n",
            "156 82 71\n",
            "675 ['trump'] ['trump', 'coronavirus']\n",
            "1 1 0\n",
            "157 83 71\n",
            "676 [] []\n",
            "0 0 0\n",
            "157 83 71\n",
            "677 [] []\n",
            "0 0 0\n",
            "157 83 71\n",
            "678 [] []\n",
            "0 0 0\n",
            "157 83 71\n",
            "679 ['robboranxradio'] []\n",
            "0 0 1\n",
            "157 83 72\n",
            "680 [] []\n",
            "0 0 0\n",
            "157 83 72\n",
            "681 ['trump', 'coronavirus'] ['trump', 'white house', 'coronavirus']\n",
            "2 1 0\n",
            "159 84 72\n",
            "682 [] []\n",
            "0 0 0\n",
            "159 84 72\n",
            "683 [] []\n",
            "0 0 0\n",
            "159 84 72\n",
            "684 [] []\n",
            "0 0 0\n",
            "159 84 72\n",
            "685 ['youtube'] ['youtube']\n",
            "1 0 0\n",
            "160 84 72\n",
            "686 [] []\n",
            "0 0 0\n",
            "160 84 72\n",
            "687 ['us'] ['us']\n",
            "1 0 0\n",
            "161 84 72\n",
            "688 [] []\n",
            "0 0 0\n",
            "161 84 72\n",
            "689 [] []\n",
            "0 0 0\n",
            "161 84 72\n",
            "690 [] []\n",
            "0 0 0\n",
            "161 84 72\n",
            "691 ['luke skywalker'] ['luke skywalker']\n",
            "1 0 0\n",
            "162 84 72\n",
            "692 [] []\n",
            "0 0 0\n",
            "162 84 72\n",
            "693 ['lauren meyers', 'dell'] ['lauren meyers', 'dell med']\n",
            "1 1 1\n",
            "163 85 73\n",
            "694 [] []\n",
            "0 0 0\n",
            "163 85 73\n",
            "695 [] []\n",
            "0 0 0\n",
            "163 85 73\n",
            "696 ['tedros adhanom ghebreyesus'] []\n",
            "0 0 1\n",
            "163 85 74\n",
            "697 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "164 85 74\n",
            "698 [] []\n",
            "0 0 0\n",
            "164 85 74\n",
            "699 [] []\n",
            "0 0 0\n",
            "164 85 74\n",
            "700 [] []\n",
            "0 0 0\n",
            "164 85 74\n",
            "701 ['michigan', 'city pulse'] ['michigan', 'city pulse']\n",
            "2 0 0\n",
            "166 85 74\n",
            "702 ['conch house'] ['conch house']\n",
            "1 0 0\n",
            "167 85 74\n",
            "703 ['tory lanez'] ['tory lanez']\n",
            "1 0 0\n",
            "168 85 74\n",
            "704 [] []\n",
            "0 0 0\n",
            "168 85 74\n",
            "705 [] []\n",
            "0 0 0\n",
            "168 85 74\n",
            "706 [] []\n",
            "0 0 0\n",
            "168 85 74\n",
            "707 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "169 85 74\n",
            "708 [] []\n",
            "0 0 0\n",
            "169 85 74\n",
            "709 [] []\n",
            "0 0 0\n",
            "169 85 74\n",
            "710 [] []\n",
            "0 0 0\n",
            "169 85 74\n",
            "711 ['wyoming'] ['wyoming']\n",
            "1 0 0\n",
            "170 85 74\n",
            "712 [] []\n",
            "0 0 0\n",
            "170 85 74\n",
            "713 ['cleveland'] ['cleveland']\n",
            "1 0 0\n",
            "171 85 74\n",
            "714 [] []\n",
            "0 0 0\n",
            "171 85 74\n",
            "715 [] []\n",
            "0 0 0\n",
            "171 85 74\n",
            "716 [] []\n",
            "0 0 0\n",
            "171 85 74\n",
            "717 [] []\n",
            "0 0 0\n",
            "171 85 74\n",
            "718 [] []\n",
            "0 0 0\n",
            "171 85 74\n",
            "719 [] []\n",
            "0 0 0\n",
            "171 85 74\n",
            "720 [] []\n",
            "0 0 0\n",
            "171 85 74\n",
            "721 ['super mario chain chomp toy'] ['mario chain chomp']\n",
            "0 1 1\n",
            "171 86 75\n",
            "722 [] []\n",
            "0 0 0\n",
            "171 86 75\n",
            "723 ['sal'] []\n",
            "0 0 1\n",
            "171 86 76\n",
            "724 [] []\n",
            "0 0 0\n",
            "171 86 76\n",
            "725 [] []\n",
            "0 0 0\n",
            "171 86 76\n",
            "726 [] []\n",
            "0 0 0\n",
            "171 86 76\n",
            "727 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "172 86 76\n",
            "728 ['trump'] ['trump', 'governors']\n",
            "1 1 0\n",
            "173 87 76\n",
            "729 ['dick durbin', 'bonnie'] ['dick durbin', 'bonnie']\n",
            "2 0 0\n",
            "175 87 76\n",
            "730 ['travelodge'] ['travelodge']\n",
            "1 0 0\n",
            "176 87 76\n",
            "731 [] []\n",
            "0 0 0\n",
            "176 87 76\n",
            "732 [] []\n",
            "0 0 0\n",
            "176 87 76\n",
            "733 ['dick durbin', 'coronavirus', 'illinois'] ['dick durbin', 'coronavirus', 'illinois', 'illinoisans']\n",
            "3 1 0\n",
            "179 88 76\n",
            "734 [] []\n",
            "0 0 0\n",
            "179 88 76\n",
            "735 [] []\n",
            "0 0 0\n",
            "179 88 76\n",
            "736 ['bosslogic'] []\n",
            "0 0 1\n",
            "179 88 77\n",
            "737 [] []\n",
            "0 0 0\n",
            "179 88 77\n",
            "738 [] []\n",
            "0 0 0\n",
            "179 88 77\n",
            "739 ['white house'] ['white house']\n",
            "1 0 0\n",
            "180 88 77\n",
            "740 ['ifc'] ['ifc']\n",
            "1 0 0\n",
            "181 88 77\n",
            "741 [] []\n",
            "0 0 0\n",
            "181 88 77\n",
            "742 ['covid-19'] ['covid19']\n",
            "0 1 1\n",
            "181 89 78\n",
            "743 ['uk'] ['uk arboretum']\n",
            "0 1 1\n",
            "181 90 79\n",
            "744 [] []\n",
            "0 0 0\n",
            "181 90 79\n",
            "745 [] []\n",
            "0 0 0\n",
            "181 90 79\n",
            "746 [] []\n",
            "0 0 0\n",
            "181 90 79\n",
            "747 ['horspath youth u9s'] []\n",
            "0 0 1\n",
            "181 90 80\n",
            "748 [] ['god', 'buhari']\n",
            "0 2 0\n",
            "181 92 80\n",
            "749 ['south portland'] ['south portland']\n",
            "1 0 0\n",
            "182 92 80\n",
            "750 ['wisconsin', 'coronavirus'] ['wisconsin', 'coronavirus']\n",
            "2 0 0\n",
            "184 92 80\n",
            "751 [] ['tmz']\n",
            "0 1 0\n",
            "184 93 80\n",
            "752 [] []\n",
            "0 0 0\n",
            "184 93 80\n",
            "753 [] []\n",
            "0 0 0\n",
            "184 93 80\n",
            "754 [] []\n",
            "0 0 0\n",
            "184 93 80\n",
            "755 [] []\n",
            "0 0 0\n",
            "184 93 80\n",
            "756 [] []\n",
            "0 0 0\n",
            "184 93 80\n",
            "757 [] []\n",
            "0 0 0\n",
            "184 93 80\n",
            "758 ['ca', 'newsom'] ['newsom']\n",
            "1 0 1\n",
            "185 93 81\n",
            "759 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "760 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "761 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "762 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "763 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "764 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "765 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "766 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "767 [] []\n",
            "0 0 0\n",
            "185 93 81\n",
            "768 ['jehoshaphat'] ['jumping jehoshaphat']\n",
            "0 1 1\n",
            "185 94 82\n",
            "769 [] []\n",
            "0 0 0\n",
            "185 94 82\n",
            "770 [] []\n",
            "0 0 0\n",
            "185 94 82\n",
            "771 [] []\n",
            "0 0 0\n",
            "185 94 82\n",
            "772 ['covid-19'] []\n",
            "0 0 1\n",
            "185 94 83\n",
            "773 ['instagram'] ['instagram']\n",
            "1 0 0\n",
            "186 94 83\n",
            "774 [] []\n",
            "0 0 0\n",
            "186 94 83\n",
            "775 [] []\n",
            "0 0 0\n",
            "186 94 83\n",
            "776 [] []\n",
            "0 0 0\n",
            "186 94 83\n",
            "777 [] []\n",
            "0 0 0\n",
            "186 94 83\n",
            "778 ['south korea'] ['south korea']\n",
            "1 0 0\n",
            "187 94 83\n",
            "779 ['wake forest'] []\n",
            "0 0 1\n",
            "187 94 84\n",
            "780 [] []\n",
            "0 0 0\n",
            "187 94 84\n",
            "781 [] []\n",
            "0 0 0\n",
            "187 94 84\n",
            "782 [] []\n",
            "0 0 0\n",
            "187 94 84\n",
            "783 [] []\n",
            "0 0 0\n",
            "187 94 84\n",
            "784 ['super mario chain chomp toy'] ['mario chain chomp']\n",
            "0 1 1\n",
            "187 95 85\n",
            "785 [] []\n",
            "0 0 0\n",
            "187 95 85\n",
            "786 [] []\n",
            "0 0 0\n",
            "187 95 85\n",
            "787 [] []\n",
            "0 0 0\n",
            "187 95 85\n",
            "788 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "188 95 85\n",
            "789 [] []\n",
            "0 0 0\n",
            "188 95 85\n",
            "790 [] []\n",
            "0 0 0\n",
            "188 95 85\n",
            "791 [] []\n",
            "0 0 0\n",
            "188 95 85\n",
            "792 [] []\n",
            "0 0 0\n",
            "188 95 85\n",
            "793 [] []\n",
            "0 0 0\n",
            "188 95 85\n",
            "794 [] []\n",
            "0 0 0\n",
            "188 95 85\n",
            "795 ['america', 'florida'] ['america', 'florida']\n",
            "2 0 0\n",
            "190 95 85\n",
            "796 [] []\n",
            "0 0 0\n",
            "190 95 85\n",
            "797 ['doug'] ['doug']\n",
            "1 0 0\n",
            "191 95 85\n",
            "798 [] []\n",
            "0 0 0\n",
            "191 95 85\n",
            "799 [] []\n",
            "0 0 0\n",
            "191 95 85\n",
            "800 [] []\n",
            "0 0 0\n",
            "191 95 85\n",
            "801 [] []\n",
            "0 0 0\n",
            "191 95 85\n",
            "802 ['south korea', 'forbes'] ['south korea', 'forbes', 'south kore']\n",
            "2 1 0\n",
            "193 96 85\n",
            "803 [] []\n",
            "0 0 0\n",
            "193 96 85\n",
            "804 [] []\n",
            "0 0 0\n",
            "193 96 85\n",
            "805 [] []\n",
            "0 0 0\n",
            "193 96 85\n",
            "806 [] []\n",
            "0 0 0\n",
            "193 96 85\n",
            "807 [] []\n",
            "0 0 0\n",
            "193 96 85\n",
            "808 [] []\n",
            "0 0 0\n",
            "193 96 85\n",
            "809 ['tom'] ['tom']\n",
            "1 0 0\n",
            "194 96 85\n",
            "810 [] ['god']\n",
            "0 1 0\n",
            "194 97 85\n",
            "811 [] []\n",
            "0 0 0\n",
            "194 97 85\n",
            "812 [] []\n",
            "0 0 0\n",
            "194 97 85\n",
            "813 ['lake charles louisiana'] ['lake charles louisiana']\n",
            "1 0 0\n",
            "195 97 85\n",
            "814 [] []\n",
            "0 0 0\n",
            "195 97 85\n",
            "815 [] []\n",
            "0 0 0\n",
            "195 97 85\n",
            "816 ['covid19'] ['covid19']\n",
            "1 0 0\n",
            "196 97 85\n",
            "817 [] []\n",
            "0 0 0\n",
            "196 97 85\n",
            "818 [] []\n",
            "0 0 0\n",
            "196 97 85\n",
            "819 [] []\n",
            "0 0 0\n",
            "196 97 85\n",
            "820 [] ['god']\n",
            "0 1 0\n",
            "196 98 85\n",
            "821 [] []\n",
            "0 0 0\n",
            "196 98 85\n",
            "822 [] []\n",
            "0 0 0\n",
            "196 98 85\n",
            "823 [] []\n",
            "0 0 0\n",
            "196 98 85\n",
            "824 [] []\n",
            "0 0 0\n",
            "196 98 85\n",
            "825 ['trump'] ['governors', 'trump']\n",
            "1 1 0\n",
            "197 99 85\n",
            "826 [] ['jon jones']\n",
            "0 1 0\n",
            "197 100 85\n",
            "827 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "828 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "829 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "830 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "831 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "832 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "833 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "834 [] []\n",
            "0 0 0\n",
            "197 100 85\n",
            "835 ['us'] ['us']\n",
            "1 0 0\n",
            "198 100 85\n",
            "836 [] ['iowans']\n",
            "0 1 0\n",
            "198 101 85\n",
            "837 ['boulder bookstore'] ['boulder bookstore']\n",
            "1 0 0\n",
            "199 101 85\n",
            "838 [] []\n",
            "0 0 0\n",
            "199 101 85\n",
            "839 ['trump'] ['trump', 'governors']\n",
            "1 1 0\n",
            "200 102 85\n",
            "840 [] []\n",
            "0 0 0\n",
            "200 102 85\n",
            "841 ['sally'] []\n",
            "0 0 1\n",
            "200 102 86\n",
            "842 [] []\n",
            "0 0 0\n",
            "200 102 86\n",
            "843 [] ['nhs']\n",
            "0 1 0\n",
            "200 103 86\n",
            "844 ['arizonans', 'coronavirus'] ['coronavirus']\n",
            "1 0 1\n",
            "201 103 87\n",
            "845 [] []\n",
            "0 0 0\n",
            "201 103 87\n",
            "846 ['obama'] ['obama']\n",
            "1 0 0\n",
            "202 103 87\n",
            "847 [] []\n",
            "0 0 0\n",
            "202 103 87\n",
            "848 [] []\n",
            "0 0 0\n",
            "202 103 87\n",
            "849 [] []\n",
            "0 0 0\n",
            "202 103 87\n",
            "850 ['wyoming'] ['wyoming']\n",
            "1 0 0\n",
            "203 103 87\n",
            "851 ['kejriwal'] ['kejriwal']\n",
            "1 0 0\n",
            "204 103 87\n",
            "852 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "853 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "854 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "855 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "856 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "857 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "858 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "859 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "860 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "861 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "862 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "863 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "864 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "865 [] []\n",
            "0 0 0\n",
            "204 103 87\n",
            "866 ['india', 'akhtar hussain', 'west bengal'] ['india', 'tmc', 'akhtar hussain', 'west bengal']\n",
            "3 1 0\n",
            "207 104 87\n",
            "867 ['unacast', 'u.s.'] ['unacast', 'u.s']\n",
            "2 0 0\n",
            "209 104 87\n",
            "868 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "210 104 87\n",
            "869 [] []\n",
            "0 0 0\n",
            "210 104 87\n",
            "870 ['shaquille'] ['brother shaquille']\n",
            "0 1 1\n",
            "210 105 88\n",
            "871 ['trump'] ['trump']\n",
            "1 0 0\n",
            "211 105 88\n",
            "872 [] []\n",
            "0 0 0\n",
            "211 105 88\n",
            "873 [] []\n",
            "0 0 0\n",
            "211 105 88\n",
            "874 [] []\n",
            "0 0 0\n",
            "211 105 88\n",
            "875 [] []\n",
            "0 0 0\n",
            "211 105 88\n",
            "876 ['kentucky'] ['kentucky']\n",
            "1 0 0\n",
            "212 105 88\n",
            "877 [] []\n",
            "0 0 0\n",
            "212 105 88\n",
            "878 ['lady', 'rana'] ['rana']\n",
            "1 0 1\n",
            "213 105 89\n",
            "879 [] []\n",
            "0 0 0\n",
            "213 105 89\n",
            "880 [] []\n",
            "0 0 0\n",
            "213 105 89\n",
            "881 [] []\n",
            "0 0 0\n",
            "213 105 89\n",
            "882 [] []\n",
            "0 0 0\n",
            "213 105 89\n",
            "883 [] ['iconic']\n",
            "0 1 0\n",
            "213 106 89\n",
            "884 ['usa', 'covid-19'] ['usa']\n",
            "1 0 1\n",
            "214 106 90\n",
            "885 ['covid-19'] []\n",
            "0 0 1\n",
            "214 106 91\n",
            "886 ['downing street'] ['downing street']\n",
            "1 0 0\n",
            "215 106 91\n",
            "887 [] []\n",
            "0 0 0\n",
            "215 106 91\n",
            "888 [] []\n",
            "0 0 0\n",
            "215 106 91\n",
            "889 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "216 106 91\n",
            "890 ['kidsgrove'] ['kidsgrove']\n",
            "1 0 0\n",
            "217 106 91\n",
            "891 [] []\n",
            "0 0 0\n",
            "217 106 91\n",
            "892 ['brickell'] ['brickell']\n",
            "1 0 0\n",
            "218 106 91\n",
            "893 [] []\n",
            "0 0 0\n",
            "218 106 91\n",
            "894 [] []\n",
            "0 0 0\n",
            "218 106 91\n",
            "895 ['covid19'] ['covid19']\n",
            "1 0 0\n",
            "219 106 91\n",
            "896 ['corona'] []\n",
            "0 0 1\n",
            "219 106 92\n",
            "897 ['arkansas'] ['arkansas']\n",
            "1 0 0\n",
            "220 106 92\n",
            "898 [] []\n",
            "0 0 0\n",
            "220 106 92\n",
            "899 ['crawley'] ['crawley stadi']\n",
            "0 1 1\n",
            "220 107 93\n",
            "900 [] []\n",
            "0 0 0\n",
            "220 107 93\n",
            "901 [] []\n",
            "0 0 0\n",
            "220 107 93\n",
            "902 [] []\n",
            "0 0 0\n",
            "220 107 93\n",
            "903 ['tanjiro'] ['tanjiro']\n",
            "1 0 0\n",
            "221 107 93\n",
            "904 [] []\n",
            "0 0 0\n",
            "221 107 93\n",
            "905 [] []\n",
            "0 0 0\n",
            "221 107 93\n",
            "906 [] []\n",
            "0 0 0\n",
            "221 107 93\n",
            "907 [] []\n",
            "0 0 0\n",
            "221 107 93\n",
            "908 ['sprouts', 'whole foods', 'philadelphia'] ['philadelphia']\n",
            "1 0 2\n",
            "222 107 95\n",
            "909 [] []\n",
            "0 0 0\n",
            "222 107 95\n",
            "910 [] []\n",
            "0 0 0\n",
            "222 107 95\n",
            "911 [] []\n",
            "0 0 0\n",
            "222 107 95\n",
            "912 [] []\n",
            "0 0 0\n",
            "222 107 95\n",
            "913 [] []\n",
            "0 0 0\n",
            "222 107 95\n",
            "914 [] []\n",
            "0 0 0\n",
            "222 107 95\n",
            "915 ['mythbusters'] ['mythbusters']\n",
            "1 0 0\n",
            "223 107 95\n",
            "916 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "917 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "918 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "919 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "920 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "921 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "922 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "923 [] []\n",
            "0 0 0\n",
            "223 107 95\n",
            "924 ['chris'] ['chris']\n",
            "1 0 0\n",
            "224 107 95\n",
            "925 ['texas'] ['texas']\n",
            "1 0 0\n",
            "225 107 95\n",
            "926 [] []\n",
            "0 0 0\n",
            "225 107 95\n",
            "927 [] []\n",
            "0 0 0\n",
            "225 107 95\n",
            "928 [] []\n",
            "0 0 0\n",
            "225 107 95\n",
            "929 ['oregon'] ['virtual charter']\n",
            "0 1 1\n",
            "225 108 96\n",
            "930 [] []\n",
            "0 0 0\n",
            "225 108 96\n",
            "931 [] []\n",
            "0 0 0\n",
            "225 108 96\n",
            "932 [] []\n",
            "0 0 0\n",
            "225 108 96\n",
            "933 [] ['trump']\n",
            "0 1 0\n",
            "225 109 96\n",
            "934 ['dominos'] []\n",
            "0 0 1\n",
            "225 109 97\n",
            "935 [] []\n",
            "0 0 0\n",
            "225 109 97\n",
            "936 [] []\n",
            "0 0 0\n",
            "225 109 97\n",
            "937 [] []\n",
            "0 0 0\n",
            "225 109 97\n",
            "938 [] []\n",
            "0 0 0\n",
            "225 109 97\n",
            "939 [] []\n",
            "0 0 0\n",
            "225 109 97\n",
            "940 ['trump'] ['trump']\n",
            "1 0 0\n",
            "226 109 97\n",
            "941 ['nypd', 'new york', 'coronavirus', 'abc news'] ['nypd', 'new yorkers', 'coronavirus']\n",
            "2 1 2\n",
            "228 110 99\n",
            "942 [] []\n",
            "0 0 0\n",
            "228 110 99\n",
            "943 ['sweden'] ['sweden']\n",
            "1 0 0\n",
            "229 110 99\n",
            "944 ['south korea', 'covid19'] ['south koreas', 'covid19']\n",
            "1 1 1\n",
            "230 111 100\n",
            "945 [] []\n",
            "0 0 0\n",
            "230 111 100\n",
            "946 ['trump', 'coronavirus'] ['trump', 'coronavirus']\n",
            "2 0 0\n",
            "232 111 100\n",
            "947 [] []\n",
            "0 0 0\n",
            "232 111 100\n",
            "948 [] []\n",
            "0 0 0\n",
            "232 111 100\n",
            "949 [] []\n",
            "0 0 0\n",
            "232 111 100\n",
            "950 [] []\n",
            "0 0 0\n",
            "232 111 100\n",
            "951 [] []\n",
            "0 0 0\n",
            "232 111 100\n",
            "952 [] []\n",
            "0 0 0\n",
            "232 111 100\n",
            "953 ['northwestern energy'] ['northwestern']\n",
            "0 1 1\n",
            "232 112 101\n",
            "954 [] []\n",
            "0 0 0\n",
            "232 112 101\n",
            "955 ['walmart'] ['walmart']\n",
            "1 0 0\n",
            "233 112 101\n",
            "956 ['italy', 'coronavirus'] ['italy', 'coronavirus']\n",
            "2 0 0\n",
            "235 112 101\n",
            "957 [] []\n",
            "0 0 0\n",
            "235 112 101\n",
            "958 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "236 112 101\n",
            "959 [] []\n",
            "0 0 0\n",
            "236 112 101\n",
            "960 [] []\n",
            "0 0 0\n",
            "236 112 101\n",
            "961 [] ['eo']\n",
            "0 1 0\n",
            "236 113 101\n",
            "962 ['abney park'] ['abney park']\n",
            "1 0 0\n",
            "237 113 101\n",
            "963 [] []\n",
            "0 0 0\n",
            "237 113 101\n",
            "964 [] []\n",
            "0 0 0\n",
            "237 113 101\n",
            "965 [] []\n",
            "0 0 0\n",
            "237 113 101\n",
            "966 [] []\n",
            "0 0 0\n",
            "237 113 101\n",
            "967 [] []\n",
            "0 0 0\n",
            "237 113 101\n",
            "968 [] []\n",
            "0 0 0\n",
            "237 113 101\n",
            "969 ['uk'] []\n",
            "0 0 1\n",
            "237 113 102\n",
            "970 [] []\n",
            "0 0 0\n",
            "237 113 102\n",
            "971 ['trump'] ['trump']\n",
            "1 0 0\n",
            "238 113 102\n",
            "972 ['bronx'] ['bronx']\n",
            "1 0 0\n",
            "239 113 102\n",
            "973 [] []\n",
            "0 0 0\n",
            "239 113 102\n",
            "974 [] []\n",
            "0 0 0\n",
            "239 113 102\n",
            "975 [] []\n",
            "0 0 0\n",
            "239 113 102\n",
            "976 [] []\n",
            "0 0 0\n",
            "239 113 102\n",
            "977 [] []\n",
            "0 0 0\n",
            "239 113 102\n",
            "978 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "240 113 102\n",
            "979 [] []\n",
            "0 0 0\n",
            "240 113 102\n",
            "980 ['kerala'] ['kerala']\n",
            "1 0 0\n",
            "241 113 102\n",
            "981 ['covid-19'] []\n",
            "0 0 1\n",
            "241 113 103\n",
            "982 [] []\n",
            "0 0 0\n",
            "241 113 103\n",
            "983 [] []\n",
            "0 0 0\n",
            "241 113 103\n",
            "984 [] []\n",
            "0 0 0\n",
            "241 113 103\n",
            "985 ['covid 19', 'washington state'] ['washington']\n",
            "0 1 2\n",
            "241 114 105\n",
            "986 ['lotus moment'] ['lotus moment']\n",
            "1 0 0\n",
            "242 114 105\n",
            "987 [] []\n",
            "0 0 0\n",
            "242 114 105\n",
            "988 [] []\n",
            "0 0 0\n",
            "242 114 105\n",
            "989 ['kgh'] ['kgh']\n",
            "1 0 0\n",
            "243 114 105\n",
            "990 [] []\n",
            "0 0 0\n",
            "243 114 105\n",
            "991 [] []\n",
            "0 0 0\n",
            "243 114 105\n",
            "992 [] []\n",
            "0 0 0\n",
            "243 114 105\n",
            "993 [] []\n",
            "0 0 0\n",
            "243 114 105\n",
            "994 ['harvard'] ['harvard']\n",
            "1 0 0\n",
            "244 114 105\n",
            "995 [] []\n",
            "0 0 0\n",
            "244 114 105\n",
            "996 [] []\n",
            "0 0 0\n",
            "244 114 105\n",
            "997 [] []\n",
            "0 0 0\n",
            "244 114 105\n",
            "998 [] []\n",
            "0 0 0\n",
            "244 114 105\n",
            "999 ['sweden'] ['sweden']\n",
            "1 0 0\n",
            "245 114 105\n",
            "1000 [] []\n",
            "0 0 0\n",
            "245 114 105\n",
            "1001 [] []\n",
            "0 0 0\n",
            "245 114 105\n",
            "1002 [] []\n",
            "0 0 0\n",
            "245 114 105\n",
            "1003 [] []\n",
            "0 0 0\n",
            "245 114 105\n",
            "1004 [] []\n",
            "0 0 0\n",
            "245 114 105\n",
            "1005 ['trump', 'twitter'] ['trump', 'twitter']\n",
            "2 0 0\n",
            "247 114 105\n",
            "1006 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "248 114 105\n",
            "1007 [] []\n",
            "0 0 0\n",
            "248 114 105\n",
            "1008 ['ptsd'] []\n",
            "0 0 1\n",
            "248 114 106\n",
            "1009 [] []\n",
            "0 0 0\n",
            "248 114 106\n",
            "1010 [] []\n",
            "0 0 0\n",
            "248 114 106\n",
            "1011 [] []\n",
            "0 0 0\n",
            "248 114 106\n",
            "1012 [] []\n",
            "0 0 0\n",
            "248 114 106\n",
            "1013 [] []\n",
            "0 0 0\n",
            "248 114 106\n",
            "1014 ['easter'] []\n",
            "0 0 1\n",
            "248 114 107\n",
            "1015 [] []\n",
            "0 0 0\n",
            "248 114 107\n",
            "1016 ['nyc'] ['nyc']\n",
            "1 0 0\n",
            "249 114 107\n",
            "1017 [] []\n",
            "0 0 0\n",
            "249 114 107\n",
            "1018 [] []\n",
            "0 0 0\n",
            "249 114 107\n",
            "1019 [] []\n",
            "0 0 0\n",
            "249 114 107\n",
            "1020 ['cleveland'] ['cleveland clinic']\n",
            "0 1 1\n",
            "249 115 108\n",
            "1021 [] []\n",
            "0 0 0\n",
            "249 115 108\n",
            "1022 ['godzilla'] ['o.c', 'godzilla']\n",
            "1 1 0\n",
            "250 116 108\n",
            "1023 [] []\n",
            "0 0 0\n",
            "250 116 108\n",
            "1024 [] []\n",
            "0 0 0\n",
            "250 116 108\n",
            "1025 [] []\n",
            "0 0 0\n",
            "250 116 108\n",
            "1026 [] []\n",
            "0 0 0\n",
            "250 116 108\n",
            "1027 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "251 116 108\n",
            "1028 ['alabama'] ['alabama']\n",
            "1 0 0\n",
            "252 116 108\n",
            "1029 [] []\n",
            "0 0 0\n",
            "252 116 108\n",
            "1030 [] []\n",
            "0 0 0\n",
            "252 116 108\n",
            "1031 [] []\n",
            "0 0 0\n",
            "252 116 108\n",
            "1032 [] []\n",
            "0 0 0\n",
            "252 116 108\n",
            "1033 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "253 116 108\n",
            "1034 [] []\n",
            "0 0 0\n",
            "253 116 108\n",
            "1035 [] []\n",
            "0 0 0\n",
            "253 116 108\n",
            "1036 [] []\n",
            "0 0 0\n",
            "253 116 108\n",
            "1037 ['basketball'] []\n",
            "0 0 1\n",
            "253 116 109\n",
            "1038 [] []\n",
            "0 0 0\n",
            "253 116 109\n",
            "1039 [] []\n",
            "0 0 0\n",
            "253 116 109\n",
            "1040 [] []\n",
            "0 0 0\n",
            "253 116 109\n",
            "1041 [] []\n",
            "0 0 0\n",
            "253 116 109\n",
            "1042 [] []\n",
            "0 0 0\n",
            "253 116 109\n",
            "1043 [] []\n",
            "0 0 0\n",
            "253 116 109\n",
            "1044 ['coles'] ['coles']\n",
            "1 0 0\n",
            "254 116 109\n",
            "1045 [] []\n",
            "0 0 0\n",
            "254 116 109\n",
            "1046 ['msdnc'] ['msdnc']\n",
            "1 0 0\n",
            "255 116 109\n",
            "1047 ['spain', 'ttt news'] ['port', 'spain']\n",
            "1 1 1\n",
            "256 117 110\n",
            "1048 ['coronavirus'] ['coronavirus', 'ocean']\n",
            "1 1 0\n",
            "257 118 110\n",
            "1049 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "258 118 110\n",
            "1050 [] []\n",
            "0 0 0\n",
            "258 118 110\n",
            "1051 ['mstu'] []\n",
            "0 0 1\n",
            "258 118 111\n",
            "1052 [] []\n",
            "0 0 0\n",
            "258 118 111\n",
            "1053 [] []\n",
            "0 0 0\n",
            "258 118 111\n",
            "1054 [] []\n",
            "0 0 0\n",
            "258 118 111\n",
            "1055 [] []\n",
            "0 0 0\n",
            "258 118 111\n",
            "1056 ['ontario'] ['ontario']\n",
            "1 0 0\n",
            "259 118 111\n",
            "1057 [] []\n",
            "0 0 0\n",
            "259 118 111\n",
            "1058 [] []\n",
            "0 0 0\n",
            "259 118 111\n",
            "1059 [] []\n",
            "0 0 0\n",
            "259 118 111\n",
            "1060 ['charlie puth'] []\n",
            "0 0 1\n",
            "259 118 112\n",
            "1061 [] []\n",
            "0 0 0\n",
            "259 118 112\n",
            "1062 [] []\n",
            "0 0 0\n",
            "259 118 112\n",
            "1063 ['trump', 'us'] ['trump', 'us governors']\n",
            "1 1 1\n",
            "260 119 113\n",
            "1064 [] []\n",
            "0 0 0\n",
            "260 119 113\n",
            "1065 [] []\n",
            "0 0 0\n",
            "260 119 113\n",
            "1066 ['clover', \"st. patrick's day\"] ['st']\n",
            "0 1 2\n",
            "260 120 115\n",
            "1067 ['nino'] ['nino']\n",
            "1 0 0\n",
            "261 120 115\n",
            "1068 [] []\n",
            "0 0 0\n",
            "261 120 115\n",
            "1069 [] []\n",
            "0 0 0\n",
            "261 120 115\n",
            "1070 [] []\n",
            "0 0 0\n",
            "261 120 115\n",
            "1071 [] []\n",
            "0 0 0\n",
            "261 120 115\n",
            "1072 ['filipinos'] ['filipinos']\n",
            "1 0 0\n",
            "262 120 115\n",
            "1073 ['miami'] ['miami']\n",
            "1 0 0\n",
            "263 120 115\n",
            "1074 ['americans'] ['smartphone']\n",
            "0 1 1\n",
            "263 121 116\n",
            "1075 [] []\n",
            "0 0 0\n",
            "263 121 116\n",
            "1076 [] []\n",
            "0 0 0\n",
            "263 121 116\n",
            "1077 [] []\n",
            "0 0 0\n",
            "263 121 116\n",
            "1078 [] []\n",
            "0 0 0\n",
            "263 121 116\n",
            "1079 [] []\n",
            "0 0 0\n",
            "263 121 116\n",
            "1080 ['roxi'] ['roxi']\n",
            "1 0 0\n",
            "264 121 116\n",
            "1081 [] []\n",
            "0 0 0\n",
            "264 121 116\n",
            "1082 [] []\n",
            "0 0 0\n",
            "264 121 116\n",
            "1083 ['ga', 'fla'] ['ga', 'fla']\n",
            "2 0 0\n",
            "266 121 116\n",
            "1084 ['morey'] ['morey']\n",
            "1 0 0\n",
            "267 121 116\n",
            "1085 [] ['twitter']\n",
            "0 1 0\n",
            "267 122 116\n",
            "1086 ['london'] ['london']\n",
            "1 0 0\n",
            "268 122 116\n",
            "1087 ['donald trump', 'ivanka', 'melanie'] ['donald trump', 'ivanka', 'melanie']\n",
            "3 0 0\n",
            "271 122 116\n",
            "1088 [] []\n",
            "0 0 0\n",
            "271 122 116\n",
            "1089 [] []\n",
            "0 0 0\n",
            "271 122 116\n",
            "1090 [] []\n",
            "0 0 0\n",
            "271 122 116\n",
            "1091 ['trump'] ['trump', 'coronavirus']\n",
            "1 1 0\n",
            "272 123 116\n",
            "1092 ['corona'] []\n",
            "0 0 1\n",
            "272 123 117\n",
            "1093 [] []\n",
            "0 0 0\n",
            "272 123 117\n",
            "1094 [] []\n",
            "0 0 0\n",
            "272 123 117\n",
            "1095 [] []\n",
            "0 0 0\n",
            "272 123 117\n",
            "1096 [] []\n",
            "0 0 0\n",
            "272 123 117\n",
            "1097 ['fangirl'] []\n",
            "0 0 1\n",
            "272 123 118\n",
            "1098 [] []\n",
            "0 0 0\n",
            "272 123 118\n",
            "1099 ['obama'] ['obama']\n",
            "1 0 0\n",
            "273 123 118\n",
            "1100 [] []\n",
            "0 0 0\n",
            "273 123 118\n",
            "1101 [] []\n",
            "0 0 0\n",
            "273 123 118\n",
            "1102 [] []\n",
            "0 0 0\n",
            "273 123 118\n",
            "1103 ['barton springs spillway'] ['barton springs spillway']\n",
            "1 0 0\n",
            "274 123 118\n",
            "1104 [] []\n",
            "0 0 0\n",
            "274 123 118\n",
            "1105 ['us'] ['us']\n",
            "1 0 0\n",
            "275 123 118\n",
            "1106 [] []\n",
            "0 0 0\n",
            "275 123 118\n",
            "1107 [] []\n",
            "0 0 0\n",
            "275 123 118\n",
            "1108 [] []\n",
            "0 0 0\n",
            "275 123 118\n",
            "1109 [] []\n",
            "0 0 0\n",
            "275 123 118\n",
            "1110 [] []\n",
            "0 0 0\n",
            "275 123 118\n",
            "1111 ['fascism'] []\n",
            "0 0 1\n",
            "275 123 119\n",
            "1112 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1113 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1114 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1115 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1116 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1117 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1118 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1119 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1120 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1121 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1122 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1123 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1124 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1125 [] []\n",
            "0 0 0\n",
            "275 123 119\n",
            "1126 ['ky', 'beshear', 'beshear'] ['ky governor beshear', 'beshear']\n",
            "1 1 2\n",
            "276 124 121\n",
            "1127 [] []\n",
            "0 0 0\n",
            "276 124 121\n",
            "1128 [] []\n",
            "0 0 0\n",
            "276 124 121\n",
            "1129 ['madison'] ['madison']\n",
            "1 0 0\n",
            "277 124 121\n",
            "1130 ['ireland'] ['ireland']\n",
            "1 0 0\n",
            "278 124 121\n",
            "1131 [] []\n",
            "0 0 0\n",
            "278 124 121\n",
            "1132 [] []\n",
            "0 0 0\n",
            "278 124 121\n",
            "1133 [] []\n",
            "0 0 0\n",
            "278 124 121\n",
            "1134 [] []\n",
            "0 0 0\n",
            "278 124 121\n",
            "1135 ['royal mail'] ['royal mail']\n",
            "1 0 0\n",
            "279 124 121\n",
            "1136 [] []\n",
            "0 0 0\n",
            "279 124 121\n",
            "1137 [] []\n",
            "0 0 0\n",
            "279 124 121\n",
            "1138 [] []\n",
            "0 0 0\n",
            "279 124 121\n",
            "1139 [] []\n",
            "0 0 0\n",
            "279 124 121\n",
            "1140 [] []\n",
            "0 0 0\n",
            "279 124 121\n",
            "1141 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "280 124 121\n",
            "1142 [] []\n",
            "0 0 0\n",
            "280 124 121\n",
            "1143 [] []\n",
            "0 0 0\n",
            "280 124 121\n",
            "1144 [] ['fb']\n",
            "0 1 0\n",
            "280 125 121\n",
            "1145 [] []\n",
            "0 0 0\n",
            "280 125 121\n",
            "1146 ['germaphobe'] []\n",
            "0 0 1\n",
            "280 125 122\n",
            "1147 [] []\n",
            "0 0 0\n",
            "280 125 122\n",
            "1148 [] []\n",
            "0 0 0\n",
            "280 125 122\n",
            "1149 ['jon'] ['jon']\n",
            "1 0 0\n",
            "281 125 122\n",
            "1150 [] []\n",
            "0 0 0\n",
            "281 125 122\n",
            "1151 [] []\n",
            "0 0 0\n",
            "281 125 122\n",
            "1152 [] []\n",
            "0 0 0\n",
            "281 125 122\n",
            "1153 ['ivey'] ['gov', 'ivey']\n",
            "1 1 0\n",
            "282 126 122\n",
            "1154 ['sydney'] ['sydney']\n",
            "1 0 0\n",
            "283 126 122\n",
            "1155 ['kenya', 'coronavirus'] ['kenya', 'coronavirus']\n",
            "2 0 0\n",
            "285 126 122\n",
            "1156 [] []\n",
            "0 0 0\n",
            "285 126 122\n",
            "1157 ['swaziland', 'eswatini'] ['swaziland/eswatini']\n",
            "0 1 2\n",
            "285 127 124\n",
            "1158 [] []\n",
            "0 0 0\n",
            "285 127 124\n",
            "1159 [] []\n",
            "0 0 0\n",
            "285 127 124\n",
            "1160 [] []\n",
            "0 0 0\n",
            "285 127 124\n",
            "1161 [] []\n",
            "0 0 0\n",
            "285 127 124\n",
            "1162 [] []\n",
            "0 0 0\n",
            "285 127 124\n",
            "1163 [] []\n",
            "0 0 0\n",
            "285 127 124\n",
            "1164 [] []\n",
            "0 0 0\n",
            "285 127 124\n",
            "1165 ['millennials'] []\n",
            "0 0 1\n",
            "285 127 125\n",
            "1166 [] []\n",
            "0 0 0\n",
            "285 127 125\n",
            "1167 [] []\n",
            "0 0 0\n",
            "285 127 125\n",
            "1168 [] []\n",
            "0 0 0\n",
            "285 127 125\n",
            "1169 [] []\n",
            "0 0 0\n",
            "285 127 125\n",
            "1170 ['barnhart'] ['barnhart']\n",
            "1 0 0\n",
            "286 127 125\n",
            "1171 ['phillip', 'virginia'] ['phillip', 'virginia governor']\n",
            "1 1 1\n",
            "287 128 126\n",
            "1172 [] []\n",
            "0 0 0\n",
            "287 128 126\n",
            "1173 ['forest hills'] ['forest hills-social']\n",
            "0 1 1\n",
            "287 129 127\n",
            "1174 [] []\n",
            "0 0 0\n",
            "287 129 127\n",
            "1175 [] []\n",
            "0 0 0\n",
            "287 129 127\n",
            "1176 ['nami'] []\n",
            "0 0 1\n",
            "287 129 128\n",
            "1177 ['tiger king', 'peaky blinders', 'coronavirus'] ['tiger king', 'coronavirus']\n",
            "2 0 1\n",
            "289 129 129\n",
            "1178 [] []\n",
            "0 0 0\n",
            "289 129 129\n",
            "1179 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "290 129 129\n",
            "1180 [] []\n",
            "0 0 0\n",
            "290 129 129\n",
            "1181 [] []\n",
            "0 0 0\n",
            "290 129 129\n",
            "1182 [] []\n",
            "0 0 0\n",
            "290 129 129\n",
            "1183 [] []\n",
            "0 0 0\n",
            "290 129 129\n",
            "1184 [] []\n",
            "0 0 0\n",
            "290 129 129\n",
            "1185 [] []\n",
            "0 0 0\n",
            "290 129 129\n",
            "1186 ['st.louis'] ['st', 'louis']\n",
            "0 2 1\n",
            "290 131 130\n",
            "1187 [] []\n",
            "0 0 0\n",
            "290 131 130\n",
            "1188 [] ['god']\n",
            "0 1 0\n",
            "290 132 130\n",
            "1189 [] []\n",
            "0 0 0\n",
            "290 132 130\n",
            "1190 ['u.s.'] ['u.s', 'wsj']\n",
            "1 1 0\n",
            "291 133 130\n",
            "1191 ['coronavirus', 'nyc'] ['coronavirus', 'nyc']\n",
            "2 0 0\n",
            "293 133 130\n",
            "1192 [] []\n",
            "0 0 0\n",
            "293 133 130\n",
            "1193 [] []\n",
            "0 0 0\n",
            "293 133 130\n",
            "1194 ['mercer county', 'coronavirus'] ['mercer', 'coronavirus']\n",
            "1 1 1\n",
            "294 134 131\n",
            "1195 [] ['nyc-based']\n",
            "0 1 0\n",
            "294 135 131\n",
            "1196 [] []\n",
            "0 0 0\n",
            "294 135 131\n",
            "1197 [] []\n",
            "0 0 0\n",
            "294 135 131\n",
            "1198 ['beshear'] ['beshear']\n",
            "1 0 0\n",
            "295 135 131\n",
            "1199 [] []\n",
            "0 0 0\n",
            "295 135 131\n",
            "1200 [] []\n",
            "0 0 0\n",
            "295 135 131\n",
            "1201 [] []\n",
            "0 0 0\n",
            "295 135 131\n",
            "1202 [] ['nyc-based']\n",
            "0 1 0\n",
            "295 136 131\n",
            "1203 [] []\n",
            "0 0 0\n",
            "295 136 131\n",
            "1204 [] []\n",
            "0 0 0\n",
            "295 136 131\n",
            "1205 [] []\n",
            "0 0 0\n",
            "295 136 131\n",
            "1206 [\"o'lakes state park\"] [\"o'lakes state park\"]\n",
            "1 0 0\n",
            "296 136 131\n",
            "1207 [] []\n",
            "0 0 0\n",
            "296 136 131\n",
            "1208 [] ['god']\n",
            "0 1 0\n",
            "296 137 131\n",
            "1209 [] []\n",
            "0 0 0\n",
            "296 137 131\n",
            "1210 ['broward'] ['broward county', 'fla']\n",
            "0 2 1\n",
            "296 139 132\n",
            "1211 [] []\n",
            "0 0 0\n",
            "296 139 132\n",
            "1212 [] []\n",
            "0 0 0\n",
            "296 139 132\n",
            "1213 [] []\n",
            "0 0 0\n",
            "296 139 132\n",
            "1214 [] []\n",
            "0 0 0\n",
            "296 139 132\n",
            "1215 ['virginia'] ['virginia']\n",
            "1 0 0\n",
            "297 139 132\n",
            "1216 ['trump', 'coronavirus'] ['trump', 'coronavirus']\n",
            "2 0 0\n",
            "299 139 132\n",
            "1217 [] []\n",
            "0 0 0\n",
            "299 139 132\n",
            "1218 [] []\n",
            "0 0 0\n",
            "299 139 132\n",
            "1219 [] []\n",
            "0 0 0\n",
            "299 139 132\n",
            "1220 [] []\n",
            "0 0 0\n",
            "299 139 132\n",
            "1221 [] []\n",
            "0 0 0\n",
            "299 139 132\n",
            "1222 ['mocha', 'licorice'] ['mocha and licorice']\n",
            "0 1 2\n",
            "299 140 134\n",
            "1223 [] []\n",
            "0 0 0\n",
            "299 140 134\n",
            "1224 [] []\n",
            "0 0 0\n",
            "299 140 134\n",
            "1225 [] []\n",
            "0 0 0\n",
            "299 140 134\n",
            "1226 ['canada'] ['canada']\n",
            "1 0 0\n",
            "300 140 134\n",
            "1227 [] []\n",
            "0 0 0\n",
            "300 140 134\n",
            "1228 ['instagram'] ['instagram']\n",
            "1 0 0\n",
            "301 140 134\n",
            "1229 [] []\n",
            "0 0 0\n",
            "301 140 134\n",
            "1230 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "302 140 134\n",
            "1231 ['nypd', 'riverside park'] ['nypd', 'riverside park']\n",
            "2 0 0\n",
            "304 140 134\n",
            "1232 ['trump', 'governor cuomo'] ['governor cuomo']\n",
            "1 0 1\n",
            "305 140 135\n",
            "1233 [] []\n",
            "0 0 0\n",
            "305 140 135\n",
            "1234 ['united states'] ['united states']\n",
            "1 0 0\n",
            "306 140 135\n",
            "1235 [] []\n",
            "0 0 0\n",
            "306 140 135\n",
            "1236 ['covid-19', 'tabasco'] []\n",
            "0 0 2\n",
            "306 140 137\n",
            "1237 ['u.s.', 'china', 'coronavirus', 'trump'] ['u.s', 'china', 'coronavirus', 'trump']\n",
            "4 0 0\n",
            "310 140 137\n",
            "1238 [] []\n",
            "0 0 0\n",
            "310 140 137\n",
            "1239 [] []\n",
            "0 0 0\n",
            "310 140 137\n",
            "1240 [] []\n",
            "0 0 0\n",
            "310 140 137\n",
            "1241 [] ['nhl']\n",
            "0 1 0\n",
            "310 141 137\n",
            "1242 [] []\n",
            "0 0 0\n",
            "310 141 137\n",
            "1243 [] []\n",
            "0 0 0\n",
            "310 141 137\n",
            "1244 [] []\n",
            "0 0 0\n",
            "310 141 137\n",
            "1245 ['netflix'] ['netflix']\n",
            "1 0 0\n",
            "311 141 137\n",
            "1246 ['matt', 'florida'] ['matt', 'florida']\n",
            "2 0 0\n",
            "313 141 137\n",
            "1247 [] ['fl']\n",
            "0 1 0\n",
            "313 142 137\n",
            "1248 ['oak'] []\n",
            "0 0 1\n",
            "313 142 138\n",
            "1249 ['kentucky', 'coronavirus'] ['kentucky', 'coronavirus']\n",
            "2 0 0\n",
            "315 142 138\n",
            "1250 [] []\n",
            "0 0 0\n",
            "315 142 138\n",
            "1251 ['wa'] []\n",
            "0 0 1\n",
            "315 142 139\n",
            "1252 ['florida'] ['florida']\n",
            "1 0 0\n",
            "316 142 139\n",
            "1253 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1254 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1255 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1256 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1257 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1258 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1259 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1260 [] []\n",
            "0 0 0\n",
            "316 142 139\n",
            "1261 ['kevin'] ['kevin']\n",
            "1 0 0\n",
            "317 142 139\n",
            "1262 ['fatbeam'] []\n",
            "0 0 1\n",
            "317 142 140\n",
            "1263 [] []\n",
            "0 0 0\n",
            "317 142 140\n",
            "1264 [] []\n",
            "0 0 0\n",
            "317 142 140\n",
            "1265 [] []\n",
            "0 0 0\n",
            "317 142 140\n",
            "1266 [] []\n",
            "0 0 0\n",
            "317 142 140\n",
            "1267 [] []\n",
            "0 0 0\n",
            "317 142 140\n",
            "1268 ['jacobson park'] ['jacobson park']\n",
            "1 0 0\n",
            "318 142 140\n",
            "1269 [] []\n",
            "0 0 0\n",
            "318 142 140\n",
            "1270 [] []\n",
            "0 0 0\n",
            "318 142 140\n",
            "1271 [] []\n",
            "0 0 0\n",
            "318 142 140\n",
            "1272 [] []\n",
            "0 0 0\n",
            "318 142 140\n",
            "1273 [] []\n",
            "0 0 0\n",
            "318 142 140\n",
            "1274 [] []\n",
            "0 0 0\n",
            "318 142 140\n",
            "1275 [] []\n",
            "0 0 0\n",
            "318 142 140\n",
            "1276 ['signe', 'solo'] ['signe']\n",
            "1 0 1\n",
            "319 142 141\n",
            "1277 [] []\n",
            "0 0 0\n",
            "319 142 141\n",
            "1278 ['garda'] ['garda']\n",
            "1 0 0\n",
            "320 142 141\n",
            "1279 ['broward'] ['broward county', 'fla']\n",
            "0 2 1\n",
            "320 144 142\n",
            "1280 [] []\n",
            "0 0 0\n",
            "320 144 142\n",
            "1281 ['mex city'] ['mex city']\n",
            "1 0 0\n",
            "321 144 142\n",
            "1282 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1283 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1284 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1285 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1286 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1287 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1288 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1289 [] []\n",
            "0 0 0\n",
            "321 144 142\n",
            "1290 ['greta'] ['greta']\n",
            "1 0 0\n",
            "322 144 142\n",
            "1291 ['peter gott'] ['peter']\n",
            "0 1 1\n",
            "322 145 143\n",
            "1292 [] []\n",
            "0 0 0\n",
            "322 145 143\n",
            "1293 [] []\n",
            "0 0 0\n",
            "322 145 143\n",
            "1294 [] []\n",
            "0 0 0\n",
            "322 145 143\n",
            "1295 ['vektroid'] []\n",
            "0 0 1\n",
            "322 145 144\n",
            "1296 [] []\n",
            "0 0 0\n",
            "322 145 144\n",
            "1297 [] []\n",
            "0 0 0\n",
            "322 145 144\n",
            "1298 [] []\n",
            "0 0 0\n",
            "322 145 144\n",
            "1299 [] []\n",
            "0 0 0\n",
            "322 145 144\n",
            "1300 [] []\n",
            "0 0 0\n",
            "322 145 144\n",
            "1301 [] []\n",
            "0 0 0\n",
            "322 145 144\n",
            "1302 ['becky', 'hubertus', 'chevy colorado'] ['becky', 'hubertus', 'chevy colorado']\n",
            "3 0 0\n",
            "325 145 144\n",
            "1303 [] []\n",
            "0 0 0\n",
            "325 145 144\n",
            "1304 [] []\n",
            "0 0 0\n",
            "325 145 144\n",
            "1305 [] []\n",
            "0 0 0\n",
            "325 145 144\n",
            "1306 [] []\n",
            "0 0 0\n",
            "325 145 144\n",
            "1307 ['spacex', 'boca'] ['spacex', 'boca']\n",
            "2 0 0\n",
            "327 145 144\n",
            "1308 [] []\n",
            "0 0 0\n",
            "327 145 144\n",
            "1309 [] []\n",
            "0 0 0\n",
            "327 145 144\n",
            "1310 ['trump'] ['trump']\n",
            "1 0 0\n",
            "328 145 144\n",
            "1311 ['u.s.', 'china', 'coronavirus', 'trump'] ['u.s', 'china', 'coronavirus', 'trump']\n",
            "4 0 0\n",
            "332 145 144\n",
            "1312 ['trump', 'coronavirus'] ['trump', 'governors', 'coronavirus']\n",
            "2 1 0\n",
            "334 146 144\n",
            "1313 [] []\n",
            "0 0 0\n",
            "334 146 144\n",
            "1314 [] []\n",
            "0 0 0\n",
            "334 146 144\n",
            "1315 [] []\n",
            "0 0 0\n",
            "334 146 144\n",
            "1316 [] []\n",
            "0 0 0\n",
            "334 146 144\n",
            "1317 ['us', 'covid-19'] ['us']\n",
            "1 0 1\n",
            "335 146 145\n",
            "1318 [] []\n",
            "0 0 0\n",
            "335 146 145\n",
            "1319 ['roosters'] ['roosters']\n",
            "1 0 0\n",
            "336 146 145\n",
            "1320 ['new jersey'] ['new jersey']\n",
            "1 0 0\n",
            "337 146 145\n",
            "1321 ['trump'] ['trump']\n",
            "1 0 0\n",
            "338 146 145\n",
            "1322 [] []\n",
            "0 0 0\n",
            "338 146 145\n",
            "1323 [] []\n",
            "0 0 0\n",
            "338 146 145\n",
            "1324 [] ['theatre']\n",
            "0 1 0\n",
            "338 147 145\n",
            "1325 [] []\n",
            "0 0 0\n",
            "338 147 145\n",
            "1326 ['covid 19'] []\n",
            "0 0 1\n",
            "338 147 146\n",
            "1327 [] []\n",
            "0 0 0\n",
            "338 147 146\n",
            "1328 [] []\n",
            "0 0 0\n",
            "338 147 146\n",
            "1329 ['ny', 'boston'] ['boston']\n",
            "1 0 1\n",
            "339 147 147\n",
            "1330 [] []\n",
            "0 0 0\n",
            "339 147 147\n",
            "1331 ['soulja boy'] ['soulja boy']\n",
            "1 0 0\n",
            "340 147 147\n",
            "1332 [] []\n",
            "0 0 0\n",
            "340 147 147\n",
            "1333 ['atv', 'georgia ave'] ['atv/dirt', 'georgia ave']\n",
            "1 1 1\n",
            "341 148 148\n",
            "1334 [] []\n",
            "0 0 0\n",
            "341 148 148\n",
            "1335 [] []\n",
            "0 0 0\n",
            "341 148 148\n",
            "1336 [] []\n",
            "0 0 0\n",
            "341 148 148\n",
            "1337 [] []\n",
            "0 0 0\n",
            "341 148 148\n",
            "1338 [] []\n",
            "0 0 0\n",
            "341 148 148\n",
            "1339 [] []\n",
            "0 0 0\n",
            "341 148 148\n",
            "1340 [] []\n",
            "0 0 0\n",
            "341 148 148\n",
            "1341 ['michael'] ['michael']\n",
            "1 0 0\n",
            "342 148 148\n",
            "1342 ['suzanne ross'] ['suzanne ross']\n",
            "1 0 0\n",
            "343 148 148\n",
            "1343 [] []\n",
            "0 0 0\n",
            "343 148 148\n",
            "1344 [] []\n",
            "0 0 0\n",
            "343 148 148\n",
            "1345 ['coronavirus', 'us', 'south china morning post'] ['coronavirus', 'us', 'china']\n",
            "2 1 1\n",
            "345 149 149\n",
            "1346 [] []\n",
            "0 0 0\n",
            "345 149 149\n",
            "1347 [] []\n",
            "0 0 0\n",
            "345 149 149\n",
            "1348 ['ghana'] ['ghana']\n",
            "1 0 0\n",
            "346 149 149\n",
            "1349 ['midwest'] ['rural midwest']\n",
            "0 1 1\n",
            "346 150 150\n",
            "1350 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1351 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1352 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1353 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1354 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1355 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1356 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1357 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1358 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1359 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1360 [] []\n",
            "0 0 0\n",
            "346 150 150\n",
            "1361 ['muslim', 'masjid', 'ny'] []\n",
            "0 0 3\n",
            "346 150 153\n",
            "1362 ['dem dey'] []\n",
            "0 0 1\n",
            "346 150 154\n",
            "1363 ['indonesia'] ['indonesia']\n",
            "1 0 0\n",
            "347 150 154\n",
            "1364 ['coronavirus', 'covid-19'] ['coronavirus']\n",
            "1 0 1\n",
            "348 150 155\n",
            "1365 ['pennsylvania'] ['pennsylvania']\n",
            "1 0 0\n",
            "349 150 155\n",
            "1366 [] []\n",
            "0 0 0\n",
            "349 150 155\n",
            "1367 [] []\n",
            "0 0 0\n",
            "349 150 155\n",
            "1368 [] []\n",
            "0 0 0\n",
            "349 150 155\n",
            "1369 [] []\n",
            "0 0 0\n",
            "349 150 155\n",
            "1370 [] []\n",
            "0 0 0\n",
            "349 150 155\n",
            "1371 [] ['gelly']\n",
            "0 1 0\n",
            "349 151 155\n",
            "1372 [] []\n",
            "0 0 0\n",
            "349 151 155\n",
            "1373 [] ['trick daddy']\n",
            "0 1 0\n",
            "349 152 155\n",
            "1374 [] []\n",
            "0 0 0\n",
            "349 152 155\n",
            "1375 [] []\n",
            "0 0 0\n",
            "349 152 155\n",
            "1376 ['west ham', 'harry kane'] ['west ham', 'harry kane']\n",
            "2 0 0\n",
            "351 152 155\n",
            "1377 ['kentucky'] ['kentucky', 'coronavirus']\n",
            "1 1 0\n",
            "352 153 155\n",
            "1378 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1379 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1380 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1381 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1382 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1383 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1384 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1385 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1386 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1387 [] []\n",
            "0 0 0\n",
            "352 153 155\n",
            "1388 ['trump', 'coronavirus'] ['trump', 'coronavirus']\n",
            "2 0 0\n",
            "354 153 155\n",
            "1389 [] []\n",
            "0 0 0\n",
            "354 153 155\n",
            "1390 [] []\n",
            "0 0 0\n",
            "354 153 155\n",
            "1391 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "355 153 155\n",
            "1392 [] []\n",
            "0 0 0\n",
            "355 153 155\n",
            "1393 [] []\n",
            "0 0 0\n",
            "355 153 155\n",
            "1394 [] []\n",
            "0 0 0\n",
            "355 153 155\n",
            "1395 [] []\n",
            "0 0 0\n",
            "355 153 155\n",
            "1396 ['dey'] []\n",
            "0 0 1\n",
            "355 153 156\n",
            "1397 [] []\n",
            "0 0 0\n",
            "355 153 156\n",
            "1398 [] []\n",
            "0 0 0\n",
            "355 153 156\n",
            "1399 ['canada'] ['canada']\n",
            "1 0 0\n",
            "356 153 156\n",
            "1400 [] []\n",
            "0 0 0\n",
            "356 153 156\n",
            "1401 [] []\n",
            "0 0 0\n",
            "356 153 156\n",
            "1402 [] []\n",
            "0 0 0\n",
            "356 153 156\n",
            "1403 [] []\n",
            "0 0 0\n",
            "356 153 156\n",
            "1404 [] []\n",
            "0 0 0\n",
            "356 153 156\n",
            "1405 [] []\n",
            "0 0 0\n",
            "356 153 156\n",
            "1406 ['wilson'] ['wilson county']\n",
            "0 1 1\n",
            "356 154 157\n",
            "1407 ['fox news', 'trump'] ['alert trump', 'coronavirus']\n",
            "0 2 2\n",
            "356 156 159\n",
            "1408 [] []\n",
            "0 0 0\n",
            "356 156 159\n",
            "1409 ['super mario chain chomp toy'] ['mario chain chomp']\n",
            "0 1 1\n",
            "356 157 160\n",
            "1410 ['humberside'] ['humberside']\n",
            "1 0 0\n",
            "357 157 160\n",
            "1411 [] []\n",
            "0 0 0\n",
            "357 157 160\n",
            "1412 ['jehovah'] []\n",
            "0 0 1\n",
            "357 157 161\n",
            "1413 [] []\n",
            "0 0 0\n",
            "357 157 161\n",
            "1414 [] []\n",
            "0 0 0\n",
            "357 157 161\n",
            "1415 [] []\n",
            "0 0 0\n",
            "357 157 161\n",
            "1416 [] []\n",
            "0 0 0\n",
            "357 157 161\n",
            "1417 ['ny', 'nj', 'pa', 'mi', 'la', 'wa', 'ca', 'ma', 'fl', 'il'] ['il']\n",
            "1 0 9\n",
            "358 157 170\n",
            "1418 ['america'] ['america']\n",
            "1 0 0\n",
            "359 157 170\n",
            "1419 ['gov beshear'] ['gov beshear']\n",
            "1 0 0\n",
            "360 157 170\n",
            "1420 [] []\n",
            "0 0 0\n",
            "360 157 170\n",
            "1421 [] []\n",
            "0 0 0\n",
            "360 157 170\n",
            "1422 [] []\n",
            "0 0 0\n",
            "360 157 170\n",
            "1423 ['fauci', 'trump'] ['fauci', 'trump']\n",
            "2 0 0\n",
            "362 157 170\n",
            "1424 [] []\n",
            "0 0 0\n",
            "362 157 170\n",
            "1425 [] []\n",
            "0 0 0\n",
            "362 157 170\n",
            "1426 [\"mcdonald's\", 'cnn'] ['mcdonald']\n",
            "1 0 1\n",
            "363 157 171\n",
            "1427 ['tv'] ['st']\n",
            "0 1 1\n",
            "363 158 172\n",
            "1428 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "364 158 172\n",
            "1429 [] []\n",
            "0 0 0\n",
            "364 158 172\n",
            "1430 ['albertans'] ['albertans']\n",
            "1 0 0\n",
            "365 158 172\n",
            "1431 [] []\n",
            "0 0 0\n",
            "365 158 172\n",
            "1432 ['nevada'] ['nevada']\n",
            "1 0 0\n",
            "366 158 172\n",
            "1433 ['hinshaw'] ['hinshaw']\n",
            "1 0 0\n",
            "367 158 172\n",
            "1434 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1435 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1436 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1437 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1438 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1439 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1440 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1441 [] []\n",
            "0 0 0\n",
            "367 158 172\n",
            "1442 ['gwynith paltrow'] []\n",
            "0 0 1\n",
            "367 158 173\n",
            "1443 [] []\n",
            "0 0 0\n",
            "367 158 173\n",
            "1444 [] []\n",
            "0 0 0\n",
            "367 158 173\n",
            "1445 ['torontonians'] ['torontonians']\n",
            "1 0 0\n",
            "368 158 173\n",
            "1446 [] []\n",
            "0 0 0\n",
            "368 158 173\n",
            "1447 [] []\n",
            "0 0 0\n",
            "368 158 173\n",
            "1448 [] []\n",
            "0 0 0\n",
            "368 158 173\n",
            "1449 ['lego'] ['lego']\n",
            "1 0 0\n",
            "369 158 173\n",
            "1450 [] ['nyc-based']\n",
            "0 1 0\n",
            "369 159 173\n",
            "1451 ['orr', 'cdc'] ['orr']\n",
            "1 0 1\n",
            "370 159 174\n",
            "1452 [] []\n",
            "0 0 0\n",
            "370 159 174\n",
            "1453 [] []\n",
            "0 0 0\n",
            "370 159 174\n",
            "1454 ['eric nam'] ['eric nam']\n",
            "1 0 0\n",
            "371 159 174\n",
            "1455 [] []\n",
            "0 0 0\n",
            "371 159 174\n",
            "1456 ['blackwell’s'] []\n",
            "0 0 1\n",
            "371 159 175\n",
            "1457 [] []\n",
            "0 0 0\n",
            "371 159 175\n",
            "1458 [] []\n",
            "0 0 0\n",
            "371 159 175\n",
            "1459 [] []\n",
            "0 0 0\n",
            "371 159 175\n",
            "1460 [] []\n",
            "0 0 0\n",
            "371 159 175\n",
            "1461 [] []\n",
            "0 0 0\n",
            "371 159 175\n",
            "1462 ['h&s'] []\n",
            "0 0 1\n",
            "371 159 176\n",
            "1463 [] []\n",
            "0 0 0\n",
            "371 159 176\n",
            "1464 [] []\n",
            "0 0 0\n",
            "371 159 176\n",
            "1465 ['npr'] []\n",
            "0 0 1\n",
            "371 159 177\n",
            "1466 ['czar trump'] ['trump', 'czar trump']\n",
            "1 1 0\n",
            "372 160 177\n",
            "1467 [] []\n",
            "0 0 0\n",
            "372 160 177\n",
            "1468 [] []\n",
            "0 0 0\n",
            "372 160 177\n",
            "1469 [] []\n",
            "0 0 0\n",
            "372 160 177\n",
            "1470 [] []\n",
            "0 0 0\n",
            "372 160 177\n",
            "1471 [] []\n",
            "0 0 0\n",
            "372 160 177\n",
            "1472 ['phillip morris', 'kentucky fried chicken', 'elizabeth city', 'kbp foods'] ['phillip morris', 'kentucky', 'elizabeth city']\n",
            "2 1 2\n",
            "374 161 179\n",
            "1473 [] []\n",
            "0 0 0\n",
            "374 161 179\n",
            "1474 ['boris johnson'] ['boris johnson']\n",
            "1 0 0\n",
            "375 161 179\n",
            "1475 ['thumb cramps'] []\n",
            "0 0 1\n",
            "375 161 180\n",
            "1476 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1477 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1478 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1479 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1480 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1481 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1482 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1483 [] []\n",
            "0 0 0\n",
            "375 161 180\n",
            "1484 ['nottinghamshire', 'nhs'] ['nottinghamshire', 'nhs']\n",
            "2 0 0\n",
            "377 161 180\n",
            "1485 [] []\n",
            "0 0 0\n",
            "377 161 180\n",
            "1486 ['kentucky'] ['kentucky']\n",
            "1 0 0\n",
            "378 161 180\n",
            "1487 [] []\n",
            "0 0 0\n",
            "378 161 180\n",
            "1488 [] []\n",
            "0 0 0\n",
            "378 161 180\n",
            "1489 [] []\n",
            "0 0 0\n",
            "378 161 180\n",
            "1490 [] []\n",
            "0 0 0\n",
            "378 161 180\n",
            "1491 [] []\n",
            "0 0 0\n",
            "378 161 180\n",
            "1492 ['zoom'] ['zoom']\n",
            "1 0 0\n",
            "379 161 180\n",
            "1493 ['africa'] ['africa']\n",
            "1 0 0\n",
            "380 161 180\n",
            "1494 [] []\n",
            "0 0 0\n",
            "380 161 180\n",
            "1495 ['ring fit'] ['ring fit']\n",
            "1 0 0\n",
            "381 161 180\n",
            "1496 ['liverpool'] ['liverpool']\n",
            "1 0 0\n",
            "382 161 180\n",
            "1497 [] []\n",
            "0 0 0\n",
            "382 161 180\n",
            "1498 [] []\n",
            "0 0 0\n",
            "382 161 180\n",
            "1499 [] []\n",
            "0 0 0\n",
            "382 161 180\n",
            "1500 [] []\n",
            "0 0 0\n",
            "382 161 180\n",
            "1501 [] []\n",
            "0 0 0\n",
            "382 161 180\n",
            "1502 [] []\n",
            "0 0 0\n",
            "382 161 180\n",
            "1503 ['bible'] ['bible']\n",
            "1 0 0\n",
            "383 161 180\n",
            "1504 ['america'] ['america']\n",
            "1 0 0\n",
            "384 161 180\n",
            "1505 ['canada', 'italy'] ['canada', 'italy']\n",
            "2 0 0\n",
            "386 161 180\n",
            "1506 ['covid-19'] []\n",
            "0 0 1\n",
            "386 161 181\n",
            "1507 [] []\n",
            "0 0 0\n",
            "386 161 181\n",
            "1508 [] []\n",
            "0 0 0\n",
            "386 161 181\n",
            "1509 [] []\n",
            "0 0 0\n",
            "386 161 181\n",
            "1510 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "387 161 181\n",
            "1511 [] []\n",
            "0 0 0\n",
            "387 161 181\n",
            "1512 [] []\n",
            "0 0 0\n",
            "387 161 181\n",
            "1513 [] []\n",
            "0 0 0\n",
            "387 161 181\n",
            "1514 [] []\n",
            "0 0 0\n",
            "387 161 181\n",
            "1515 ['trump'] ['trump']\n",
            "1 0 0\n",
            "388 161 181\n",
            "1516 [] []\n",
            "0 0 0\n",
            "388 161 181\n",
            "1517 ['tuna'] ['insta']\n",
            "0 1 1\n",
            "388 162 182\n",
            "1518 ['ontario', 'new york'] ['new york']\n",
            "1 0 1\n",
            "389 162 183\n",
            "1519 [] []\n",
            "0 0 0\n",
            "389 162 183\n",
            "1520 [] ['lou']\n",
            "0 1 0\n",
            "389 163 183\n",
            "1521 [] []\n",
            "0 0 0\n",
            "389 163 183\n",
            "1522 ['baseball'] []\n",
            "0 0 1\n",
            "389 163 184\n",
            "1523 [] []\n",
            "0 0 0\n",
            "389 163 184\n",
            "1524 [] []\n",
            "0 0 0\n",
            "389 163 184\n",
            "1525 [] ['mill']\n",
            "0 1 0\n",
            "389 164 184\n",
            "1526 [] []\n",
            "0 0 0\n",
            "389 164 184\n",
            "1527 [] ['hyderabad']\n",
            "0 1 0\n",
            "389 165 184\n",
            "1528 [] []\n",
            "0 0 0\n",
            "389 165 184\n",
            "1529 [] []\n",
            "0 0 0\n",
            "389 165 184\n",
            "1530 [] []\n",
            "0 0 0\n",
            "389 165 184\n",
            "1531 [] ['spn']\n",
            "0 1 0\n",
            "389 166 184\n",
            "1532 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1533 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1534 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1535 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1536 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1537 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1538 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1539 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1540 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1541 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1542 [] []\n",
            "0 0 0\n",
            "389 166 184\n",
            "1543 [] ['canadian blood services']\n",
            "0 1 0\n",
            "389 167 184\n",
            "1544 [] []\n",
            "0 0 0\n",
            "389 167 184\n",
            "1545 ['america'] ['america']\n",
            "1 0 0\n",
            "390 167 184\n",
            "1546 [] []\n",
            "0 0 0\n",
            "390 167 184\n",
            "1547 [] []\n",
            "0 0 0\n",
            "390 167 184\n",
            "1548 ['atlantic'] ['atlantic']\n",
            "1 0 0\n",
            "391 167 184\n",
            "1549 [] []\n",
            "0 0 0\n",
            "391 167 184\n",
            "1550 [] []\n",
            "0 0 0\n",
            "391 167 184\n",
            "1551 ['us', 'trump'] ['us', 'trump']\n",
            "2 0 0\n",
            "393 167 184\n",
            "1552 ['american'] []\n",
            "0 0 1\n",
            "393 167 185\n",
            "1553 [] []\n",
            "0 0 0\n",
            "393 167 185\n",
            "1554 [] []\n",
            "0 0 0\n",
            "393 167 185\n",
            "1555 [] []\n",
            "0 0 0\n",
            "393 167 185\n",
            "1556 [] []\n",
            "0 0 0\n",
            "393 167 185\n",
            "1557 [] []\n",
            "0 0 0\n",
            "393 167 185\n",
            "1558 [] []\n",
            "0 0 0\n",
            "393 167 185\n",
            "1559 [] ['coronavirus']\n",
            "0 1 0\n",
            "393 168 185\n",
            "1560 [] []\n",
            "0 0 0\n",
            "393 168 185\n",
            "1561 ['golf'] []\n",
            "0 0 1\n",
            "393 168 186\n",
            "1562 [] []\n",
            "0 0 0\n",
            "393 168 186\n",
            "1563 [] []\n",
            "0 0 0\n",
            "393 168 186\n",
            "1564 ['christian moral'] []\n",
            "0 0 1\n",
            "393 168 187\n",
            "1565 [] []\n",
            "0 0 0\n",
            "393 168 187\n",
            "1566 ['christian moral'] []\n",
            "0 0 1\n",
            "393 168 188\n",
            "1567 [] []\n",
            "0 0 0\n",
            "393 168 188\n",
            "1568 [] ['coronavirus']\n",
            "0 1 0\n",
            "393 169 188\n",
            "1569 ['gop'] ['gop']\n",
            "1 0 0\n",
            "394 169 188\n",
            "1570 ['btu'] []\n",
            "0 0 1\n",
            "394 169 189\n",
            "1571 ['kijiji autos'] ['kijiji']\n",
            "0 1 1\n",
            "394 170 190\n",
            "1572 [] []\n",
            "0 0 0\n",
            "394 170 190\n",
            "1573 ['covid'] []\n",
            "0 0 1\n",
            "394 170 191\n",
            "1574 [] []\n",
            "0 0 0\n",
            "394 170 191\n",
            "1575 [] []\n",
            "0 0 0\n",
            "394 170 191\n",
            "1576 [] ['nyc-based']\n",
            "0 1 0\n",
            "394 171 191\n",
            "1577 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1578 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1579 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1580 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1581 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1582 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1583 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1584 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1585 [] []\n",
            "0 0 0\n",
            "394 171 191\n",
            "1586 ['dc', 'ny'] ['dc']\n",
            "1 0 1\n",
            "395 171 192\n",
            "1587 ['trump', 'us'] ['trump', 'us', 'coronavirus']\n",
            "2 1 0\n",
            "397 172 192\n",
            "1588 ['pearson airport'] ['pearson airport']\n",
            "1 0 0\n",
            "398 172 192\n",
            "1589 [] []\n",
            "0 0 0\n",
            "398 172 192\n",
            "1590 [] []\n",
            "0 0 0\n",
            "398 172 192\n",
            "1591 [] []\n",
            "0 0 0\n",
            "398 172 192\n",
            "1592 [] []\n",
            "0 0 0\n",
            "398 172 192\n",
            "1593 [] []\n",
            "0 0 0\n",
            "398 172 192\n",
            "1594 [] []\n",
            "0 0 0\n",
            "398 172 192\n",
            "1595 ['amherst', 'amherst'] ['amherst', 'amherst']\n",
            "2 0 0\n",
            "400 172 192\n",
            "1596 [] ['grove city']\n",
            "0 1 0\n",
            "400 173 192\n",
            "1597 [] []\n",
            "0 0 0\n",
            "400 173 192\n",
            "1598 [] ['rapunzel']\n",
            "0 1 0\n",
            "400 174 192\n",
            "1599 [] ['somalis']\n",
            "0 1 0\n",
            "400 175 192\n",
            "1600 [] []\n",
            "0 0 0\n",
            "400 175 192\n",
            "1601 ['utah'] ['utah']\n",
            "1 0 0\n",
            "401 175 192\n",
            "1602 ['itv2'] []\n",
            "0 0 1\n",
            "401 175 193\n",
            "1603 [] []\n",
            "0 0 0\n",
            "401 175 193\n",
            "1604 [] []\n",
            "0 0 0\n",
            "401 175 193\n",
            "1605 [] []\n",
            "0 0 0\n",
            "401 175 193\n",
            "1606 [] []\n",
            "0 0 0\n",
            "401 175 193\n",
            "1607 ['nyc'] ['nyc']\n",
            "1 0 0\n",
            "402 175 193\n",
            "1608 ['katie'] ['katie']\n",
            "1 0 0\n",
            "403 175 193\n",
            "1609 ['queso', 'margs', 'rosie'] []\n",
            "0 0 3\n",
            "403 175 196\n",
            "1610 [] []\n",
            "0 0 0\n",
            "403 175 196\n",
            "1611 [] []\n",
            "0 0 0\n",
            "403 175 196\n",
            "1612 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "404 175 196\n",
            "1613 ['soquel'] ['soquel high school']\n",
            "0 1 1\n",
            "404 176 197\n",
            "1614 [] []\n",
            "0 0 0\n",
            "404 176 197\n",
            "1615 ['ariana', 'frankie'] ['ariana', 'frankie']\n",
            "2 0 0\n",
            "406 176 197\n",
            "1616 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1617 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1618 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1619 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1620 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1621 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1622 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1623 [] []\n",
            "0 0 0\n",
            "406 176 197\n",
            "1624 ['coronavirus', 'vox'] ['coronavirus']\n",
            "1 0 1\n",
            "407 176 198\n",
            "1625 ['carrier park'] ['carrier park']\n",
            "1 0 0\n",
            "408 176 198\n",
            "1626 [] []\n",
            "0 0 0\n",
            "408 176 198\n",
            "1627 [] ['god']\n",
            "0 1 0\n",
            "408 177 198\n",
            "1628 [] []\n",
            "0 0 0\n",
            "408 177 198\n",
            "1629 [] []\n",
            "0 0 0\n",
            "408 177 198\n",
            "1630 [] []\n",
            "0 0 0\n",
            "408 177 198\n",
            "1631 ['uk'] []\n",
            "0 0 1\n",
            "408 177 199\n",
            "1632 ['lotta balls', 'jimmy kimmel', 'melania', 'trump'] ['jimmy kimmel', 'melania trump']\n",
            "1 1 3\n",
            "409 178 202\n",
            "1633 [] []\n",
            "0 0 0\n",
            "409 178 202\n",
            "1634 ['neponset'] ['neponset']\n",
            "1 0 0\n",
            "410 178 202\n",
            "1635 [] []\n",
            "0 0 0\n",
            "410 178 202\n",
            "1636 [] []\n",
            "0 0 0\n",
            "410 178 202\n",
            "1637 ['trump', 'trump'] ['trump', 'trump']\n",
            "2 0 0\n",
            "412 178 202\n",
            "1638 [] []\n",
            "0 0 0\n",
            "412 178 202\n",
            "1639 [] []\n",
            "0 0 0\n",
            "412 178 202\n",
            "1640 ['uk'] []\n",
            "0 0 1\n",
            "412 178 203\n",
            "1641 [] ['mythbusters']\n",
            "0 1 0\n",
            "412 179 203\n",
            "1642 [] []\n",
            "0 0 0\n",
            "412 179 203\n",
            "1643 [] ['nyc-based']\n",
            "0 1 0\n",
            "412 180 203\n",
            "1644 [] []\n",
            "0 0 0\n",
            "412 180 203\n",
            "1645 [] []\n",
            "0 0 0\n",
            "412 180 203\n",
            "1646 [] []\n",
            "0 0 0\n",
            "412 180 203\n",
            "1647 [] ['ultra', 'insomniac']\n",
            "0 2 0\n",
            "412 182 203\n",
            "1648 [] ['nyc-based']\n",
            "0 1 0\n",
            "412 183 203\n",
            "1649 [] []\n",
            "0 0 0\n",
            "412 183 203\n",
            "1650 [] []\n",
            "0 0 0\n",
            "412 183 203\n",
            "1651 [] []\n",
            "0 0 0\n",
            "412 183 203\n",
            "1652 [] []\n",
            "0 0 0\n",
            "412 183 203\n",
            "1653 ['west ham'] ['west ham']\n",
            "1 0 0\n",
            "413 183 203\n",
            "1654 [] []\n",
            "0 0 0\n",
            "413 183 203\n",
            "1655 [] []\n",
            "0 0 0\n",
            "413 183 203\n",
            "1656 ['uk', 'nhs'] ['nhs', 'st']\n",
            "1 1 1\n",
            "414 184 204\n",
            "1657 ['broad st'] ['broad st']\n",
            "1 0 0\n",
            "415 184 204\n",
            "1658 [] []\n",
            "0 0 0\n",
            "415 184 204\n",
            "1659 [] []\n",
            "0 0 0\n",
            "415 184 204\n",
            "1660 ['ny'] []\n",
            "0 0 1\n",
            "415 184 205\n",
            "1661 ['winston-salem'] ['winston-salem']\n",
            "1 0 0\n",
            "416 184 205\n",
            "1662 ['pnwu'] ['pnwu']\n",
            "1 0 0\n",
            "417 184 205\n",
            "1663 [] []\n",
            "0 0 0\n",
            "417 184 205\n",
            "1664 [] []\n",
            "0 0 0\n",
            "417 184 205\n",
            "1665 ['covid-19'] []\n",
            "0 0 1\n",
            "417 184 206\n",
            "1666 ['phillips'] ['phillips']\n",
            "1 0 0\n",
            "418 184 206\n",
            "1667 [] []\n",
            "0 0 0\n",
            "418 184 206\n",
            "1668 [] []\n",
            "0 0 0\n",
            "418 184 206\n",
            "1669 [] []\n",
            "0 0 0\n",
            "418 184 206\n",
            "1670 ['clarion'] ['clarion']\n",
            "1 0 0\n",
            "419 184 206\n",
            "1671 ['border farce', 'sydney'] ['sydney airport']\n",
            "0 1 2\n",
            "419 185 208\n",
            "1672 [] []\n",
            "0 0 0\n",
            "419 185 208\n",
            "1673 ['muslim'] []\n",
            "0 0 1\n",
            "419 185 209\n",
            "1674 [] []\n",
            "0 0 0\n",
            "419 185 209\n",
            "1675 [] []\n",
            "0 0 0\n",
            "419 185 209\n",
            "1676 [] []\n",
            "0 0 0\n",
            "419 185 209\n",
            "1677 [] []\n",
            "0 0 0\n",
            "419 185 209\n",
            "1678 ['christians'] ['christians']\n",
            "1 0 0\n",
            "420 185 209\n",
            "1679 [] []\n",
            "0 0 0\n",
            "420 185 209\n",
            "1680 [] []\n",
            "0 0 0\n",
            "420 185 209\n",
            "1681 ['maryland'] ['maryland']\n",
            "1 0 0\n",
            "421 185 209\n",
            "1682 ['mary'] ['mary']\n",
            "1 0 0\n",
            "422 185 209\n",
            "1683 [] []\n",
            "0 0 0\n",
            "422 185 209\n",
            "1684 ['la', 'covid-19'] ['la county']\n",
            "0 1 2\n",
            "422 186 211\n",
            "1685 [] []\n",
            "0 0 0\n",
            "422 186 211\n",
            "1686 [] []\n",
            "0 0 0\n",
            "422 186 211\n",
            "1687 [] []\n",
            "0 0 0\n",
            "422 186 211\n",
            "1688 [] []\n",
            "0 0 0\n",
            "422 186 211\n",
            "1689 ['covid-19'] []\n",
            "0 0 1\n",
            "422 186 212\n",
            "1690 [] []\n",
            "0 0 0\n",
            "422 186 212\n",
            "1691 ['sierra'] ['sierra']\n",
            "1 0 0\n",
            "423 186 212\n",
            "1692 [] ['lala']\n",
            "0 1 0\n",
            "423 187 212\n",
            "1693 [] []\n",
            "0 0 0\n",
            "423 187 212\n",
            "1694 [] []\n",
            "0 0 0\n",
            "423 187 212\n",
            "1695 [] []\n",
            "0 0 0\n",
            "423 187 212\n",
            "1696 [] []\n",
            "0 0 0\n",
            "423 187 212\n",
            "1697 ['trump'] ['trump']\n",
            "1 0 0\n",
            "424 187 212\n",
            "1698 [] []\n",
            "0 0 0\n",
            "424 187 212\n",
            "1699 ['tiktoc'] []\n",
            "0 0 1\n",
            "424 187 213\n",
            "1700 [] []\n",
            "0 0 0\n",
            "424 187 213\n",
            "1701 [] []\n",
            "0 0 0\n",
            "424 187 213\n",
            "1702 ['zoom'] ['zoom']\n",
            "1 0 0\n",
            "425 187 213\n",
            "1703 [] ['st']\n",
            "0 1 0\n",
            "425 188 213\n",
            "1704 ['bajan'] ['bajan']\n",
            "1 0 0\n",
            "426 188 213\n",
            "1705 [] []\n",
            "0 0 0\n",
            "426 188 213\n",
            "1706 ['mega rally'] []\n",
            "0 0 1\n",
            "426 188 214\n",
            "1707 [] []\n",
            "0 0 0\n",
            "426 188 214\n",
            "1708 [] []\n",
            "0 0 0\n",
            "426 188 214\n",
            "1709 [] []\n",
            "0 0 0\n",
            "426 188 214\n",
            "1710 [] []\n",
            "0 0 0\n",
            "426 188 214\n",
            "1711 [] []\n",
            "0 0 0\n",
            "426 188 214\n",
            "1712 [] []\n",
            "0 0 0\n",
            "426 188 214\n",
            "1713 [] ['lubo']\n",
            "0 1 0\n",
            "426 189 214\n",
            "1714 [] []\n",
            "0 0 0\n",
            "426 189 214\n",
            "1715 [] []\n",
            "0 0 0\n",
            "426 189 214\n",
            "1716 [] ['stewart high school']\n",
            "0 1 0\n",
            "426 190 214\n",
            "1717 [] []\n",
            "0 0 0\n",
            "426 190 214\n",
            "1718 [] []\n",
            "0 0 0\n",
            "426 190 214\n",
            "1719 ['stardew'] ['stardew valley']\n",
            "0 1 1\n",
            "426 191 215\n",
            "1720 [] []\n",
            "0 0 0\n",
            "426 191 215\n",
            "1721 [] []\n",
            "0 0 0\n",
            "426 191 215\n",
            "1722 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "427 191 215\n",
            "1723 ['teagan'] ['teagan']\n",
            "1 0 0\n",
            "428 191 215\n",
            "1724 [] []\n",
            "0 0 0\n",
            "428 191 215\n",
            "1725 [] []\n",
            "0 0 0\n",
            "428 191 215\n",
            "1726 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "429 191 215\n",
            "1727 [] []\n",
            "0 0 0\n",
            "429 191 215\n",
            "1728 [] []\n",
            "0 0 0\n",
            "429 191 215\n",
            "1729 ['usa'] ['usa']\n",
            "1 0 0\n",
            "430 191 215\n",
            "1730 ['dadeville city council'] ['dadeville city council']\n",
            "1 0 0\n",
            "431 191 215\n",
            "1731 ['seattle'] ['seattle']\n",
            "1 0 0\n",
            "432 191 215\n",
            "1732 ['trump'] ['trump', 'governors']\n",
            "1 1 0\n",
            "433 192 215\n",
            "1733 [] []\n",
            "0 0 0\n",
            "433 192 215\n",
            "1734 [] []\n",
            "0 0 0\n",
            "433 192 215\n",
            "1735 ['pentagon'] ['pentagon']\n",
            "1 0 0\n",
            "434 192 215\n",
            "1736 ['stockport'] ['stockport']\n",
            "1 0 0\n",
            "435 192 215\n",
            "1737 ['instagram'] ['instagram']\n",
            "1 0 0\n",
            "436 192 215\n",
            "1738 [] []\n",
            "0 0 0\n",
            "436 192 215\n",
            "1739 [] []\n",
            "0 0 0\n",
            "436 192 215\n",
            "1740 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "437 192 215\n",
            "1741 ['nz'] ['lockdown nz']\n",
            "0 1 1\n",
            "437 193 216\n",
            "1742 ['dan mini'] ['dan mini']\n",
            "1 0 0\n",
            "438 193 216\n",
            "1743 ['chinese', 'igg test'] []\n",
            "0 0 2\n",
            "438 193 218\n",
            "1744 [] []\n",
            "0 0 0\n",
            "438 193 218\n",
            "1745 ['covid19', 'igm', 'igg', 'china'] ['covid19', 'china']\n",
            "2 0 2\n",
            "440 193 220\n",
            "1746 ['amish country'] ['amish']\n",
            "0 1 1\n",
            "440 194 221\n",
            "1747 [] []\n",
            "0 0 0\n",
            "440 194 221\n",
            "1748 ['costco'] ['costco']\n",
            "1 0 0\n",
            "441 194 221\n",
            "1749 ['ge'] ['ge']\n",
            "1 0 0\n",
            "442 194 221\n",
            "1750 [] []\n",
            "0 0 0\n",
            "442 194 221\n",
            "1751 [] []\n",
            "0 0 0\n",
            "442 194 221\n",
            "1752 ['uk', 'cv-19'] []\n",
            "0 0 2\n",
            "442 194 223\n",
            "1753 [] []\n",
            "0 0 0\n",
            "442 194 223\n",
            "1754 [] []\n",
            "0 0 0\n",
            "442 194 223\n",
            "1755 [] []\n",
            "0 0 0\n",
            "442 194 223\n",
            "1756 [] []\n",
            "0 0 0\n",
            "442 194 223\n",
            "1757 [] []\n",
            "0 0 0\n",
            "442 194 223\n",
            "1758 [] []\n",
            "0 0 0\n",
            "442 194 223\n",
            "1759 [] []\n",
            "0 0 0\n",
            "442 194 223\n",
            "1760 ['hawaii'] ['hawaii']\n",
            "1 0 0\n",
            "443 194 223\n",
            "1761 ['gearjunkie'] []\n",
            "0 0 1\n",
            "443 194 224\n",
            "1762 ['tom brady'] ['tom brady', 'pats', 'bucs']\n",
            "1 2 0\n",
            "444 196 224\n",
            "1763 ['courtney cole'] ['servame', 'courtney cole']\n",
            "1 1 0\n",
            "445 197 224\n",
            "1764 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1765 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1766 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1767 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1768 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1769 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1770 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1771 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1772 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1773 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1774 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1775 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1776 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1777 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1778 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1779 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1780 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1781 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1782 [] []\n",
            "0 0 0\n",
            "445 197 224\n",
            "1783 [] ['scotland']\n",
            "0 1 0\n",
            "445 198 224\n",
            "1784 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "446 198 224\n",
            "1785 ['td', 'georgia'] ['td', 'georgia']\n",
            "2 0 0\n",
            "448 198 224\n",
            "1786 ['island bay'] ['island bay']\n",
            "1 0 0\n",
            "449 198 224\n",
            "1787 [] []\n",
            "0 0 0\n",
            "449 198 224\n",
            "1788 [] []\n",
            "0 0 0\n",
            "449 198 224\n",
            "1789 ['candace'] ['candace']\n",
            "1 0 0\n",
            "450 198 224\n",
            "1790 [] []\n",
            "0 0 0\n",
            "450 198 224\n",
            "1791 ['narcissist', 'white house'] ['white house']\n",
            "1 0 1\n",
            "451 198 225\n",
            "1792 [] []\n",
            "0 0 0\n",
            "451 198 225\n",
            "1793 [] []\n",
            "0 0 0\n",
            "451 198 225\n",
            "1794 [] []\n",
            "0 0 0\n",
            "451 198 225\n",
            "1795 ['psa'] []\n",
            "0 0 1\n",
            "451 198 226\n",
            "1796 [] []\n",
            "0 0 0\n",
            "451 198 226\n",
            "1797 ['mastamiller'] ['usb chargers']\n",
            "0 1 1\n",
            "451 199 227\n",
            "1798 [] []\n",
            "0 0 0\n",
            "451 199 227\n",
            "1799 [] []\n",
            "0 0 0\n",
            "451 199 227\n",
            "1800 [] ['state park officials']\n",
            "0 1 0\n",
            "451 200 227\n",
            "1801 [] []\n",
            "0 0 0\n",
            "451 200 227\n",
            "1802 [] ['insta']\n",
            "0 1 0\n",
            "451 201 227\n",
            "1803 ['naisc'] []\n",
            "0 0 1\n",
            "451 201 228\n",
            "1804 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "452 201 228\n",
            "1805 ['cambridg'] ['cambridg']\n",
            "1 0 0\n",
            "453 201 228\n",
            "1806 ['coronavirus', 'ireland'] ['coronavirus', 'ireland']\n",
            "2 0 0\n",
            "455 201 228\n",
            "1807 [] []\n",
            "0 0 0\n",
            "455 201 228\n",
            "1808 [] []\n",
            "0 0 0\n",
            "455 201 228\n",
            "1809 ['covid19'] ['covid19']\n",
            "1 0 0\n",
            "456 201 228\n",
            "1810 [] ['nyc-based']\n",
            "0 1 0\n",
            "456 202 228\n",
            "1811 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "457 202 228\n",
            "1812 [] []\n",
            "0 0 0\n",
            "457 202 228\n",
            "1813 [] []\n",
            "0 0 0\n",
            "457 202 228\n",
            "1814 [] []\n",
            "0 0 0\n",
            "457 202 228\n",
            "1815 [] []\n",
            "0 0 0\n",
            "457 202 228\n",
            "1816 ['india'] ['india']\n",
            "1 0 0\n",
            "458 202 228\n",
            "1817 ['mcdonalds'] ['mcdonalds']\n",
            "1 0 0\n",
            "459 202 228\n",
            "1818 [] ['st']\n",
            "0 1 0\n",
            "459 203 228\n",
            "1819 [] []\n",
            "0 0 0\n",
            "459 203 228\n",
            "1820 ['covad19'] []\n",
            "0 0 1\n",
            "459 203 229\n",
            "1821 [] []\n",
            "0 0 0\n",
            "459 203 229\n",
            "1822 [] []\n",
            "0 0 0\n",
            "459 203 229\n",
            "1823 [] []\n",
            "0 0 0\n",
            "459 203 229\n",
            "1824 ['super mario chain chomp'] ['mario chain chomp']\n",
            "0 1 1\n",
            "459 204 230\n",
            "1825 ['bros'] []\n",
            "0 0 1\n",
            "459 204 231\n",
            "1826 [] []\n",
            "0 0 0\n",
            "459 204 231\n",
            "1827 ['itv2'] []\n",
            "0 0 1\n",
            "459 204 232\n",
            "1828 [] []\n",
            "0 0 0\n",
            "459 204 232\n",
            "1829 [] []\n",
            "0 0 0\n",
            "459 204 232\n",
            "1830 [] []\n",
            "0 0 0\n",
            "459 204 232\n",
            "1831 ['woodlands'] ['woodlands']\n",
            "1 0 0\n",
            "460 204 232\n",
            "1832 ['houston'] ['houston']\n",
            "1 0 0\n",
            "461 204 232\n",
            "1833 [] []\n",
            "0 0 0\n",
            "461 204 232\n",
            "1834 [] []\n",
            "0 0 0\n",
            "461 204 232\n",
            "1835 [] []\n",
            "0 0 0\n",
            "461 204 232\n",
            "1836 ['america', 'covid-19'] ['america']\n",
            "1 0 1\n",
            "462 204 233\n",
            "1837 ['new yorkers'] ['new yorkers']\n",
            "1 0 0\n",
            "463 204 233\n",
            "1838 [] []\n",
            "0 0 0\n",
            "463 204 233\n",
            "1839 [] []\n",
            "0 0 0\n",
            "463 204 233\n",
            "1840 [] []\n",
            "0 0 0\n",
            "463 204 233\n",
            "1841 ['village park', 'village park'] ['village parks', 'village parks']\n",
            "0 2 2\n",
            "463 206 235\n",
            "1842 ['trump', 'coronavirus'] ['trump', 'coronavirus']\n",
            "2 0 0\n",
            "465 206 235\n",
            "1843 ['iss'] []\n",
            "0 0 1\n",
            "465 206 236\n",
            "1844 ['us'] ['us']\n",
            "1 0 0\n",
            "466 206 236\n",
            "1845 [] []\n",
            "0 0 0\n",
            "466 206 236\n",
            "1846 [] []\n",
            "0 0 0\n",
            "466 206 236\n",
            "1847 [] []\n",
            "0 0 0\n",
            "466 206 236\n",
            "1848 [] ['nyc-based']\n",
            "0 1 0\n",
            "466 207 236\n",
            "1849 [] []\n",
            "0 0 0\n",
            "466 207 236\n",
            "1850 [] []\n",
            "0 0 0\n",
            "466 207 236\n",
            "1851 [] []\n",
            "0 0 0\n",
            "466 207 236\n",
            "1852 [] []\n",
            "0 0 0\n",
            "466 207 236\n",
            "1853 ['melanie fournier', 'covid-19'] ['melanie fournier']\n",
            "1 0 1\n",
            "467 207 237\n",
            "1854 [] []\n",
            "0 0 0\n",
            "467 207 237\n",
            "1855 [] []\n",
            "0 0 0\n",
            "467 207 237\n",
            "1856 [] []\n",
            "0 0 0\n",
            "467 207 237\n",
            "1857 [] []\n",
            "0 0 0\n",
            "467 207 237\n",
            "1858 [] []\n",
            "0 0 0\n",
            "467 207 237\n",
            "1859 [] []\n",
            "0 0 0\n",
            "467 207 237\n",
            "1860 ['trump', 'coronavirus'] ['trump', 'coronavirus']\n",
            "2 0 0\n",
            "469 207 237\n",
            "1861 ['beashear'] ['gov']\n",
            "0 1 1\n",
            "469 208 238\n",
            "1862 [] []\n",
            "0 0 0\n",
            "469 208 238\n",
            "1863 ['e-karaoke'] []\n",
            "0 0 1\n",
            "469 208 239\n",
            "1864 ['new orlean'] ['new orleans mayor']\n",
            "0 1 1\n",
            "469 209 240\n",
            "1865 [] []\n",
            "0 0 0\n",
            "469 209 240\n",
            "1866 [] []\n",
            "0 0 0\n",
            "469 209 240\n",
            "1867 [] ['aldi']\n",
            "0 1 0\n",
            "469 210 240\n",
            "1868 [] []\n",
            "0 0 0\n",
            "469 210 240\n",
            "1869 [] []\n",
            "0 0 0\n",
            "469 210 240\n",
            "1870 [] []\n",
            "0 0 0\n",
            "469 210 240\n",
            "1871 ['cowvid-19'] []\n",
            "0 0 1\n",
            "469 210 241\n",
            "1872 [] []\n",
            "0 0 0\n",
            "469 210 241\n",
            "1873 [] []\n",
            "0 0 0\n",
            "469 210 241\n",
            "1874 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "470 210 241\n",
            "1875 [] []\n",
            "0 0 0\n",
            "470 210 241\n",
            "1876 ['falls run park'] ['falls run park']\n",
            "1 0 0\n",
            "471 210 241\n",
            "1877 ['trump'] ['trump']\n",
            "1 0 0\n",
            "472 210 241\n",
            "1878 ['reston national golf'] ['reston']\n",
            "0 1 1\n",
            "472 211 242\n",
            "1879 [] []\n",
            "0 0 0\n",
            "472 211 242\n",
            "1880 ['florida'] ['florida']\n",
            "1 0 0\n",
            "473 211 242\n",
            "1881 [] []\n",
            "0 0 0\n",
            "473 211 242\n",
            "1882 [] []\n",
            "0 0 0\n",
            "473 211 242\n",
            "1883 [] []\n",
            "0 0 0\n",
            "473 211 242\n",
            "1884 [] []\n",
            "0 0 0\n",
            "473 211 242\n",
            "1885 ['dr. scott'] ['scott']\n",
            "0 1 1\n",
            "473 212 243\n",
            "1886 ['allison smith'] ['allison smith']\n",
            "1 0 0\n",
            "474 212 243\n",
            "1887 [] []\n",
            "0 0 0\n",
            "474 212 243\n",
            "1888 [] []\n",
            "0 0 0\n",
            "474 212 243\n",
            "1889 [] []\n",
            "0 0 0\n",
            "474 212 243\n",
            "1890 ['florida', 'doh', 'cdc'] ['florida']\n",
            "1 0 2\n",
            "475 212 245\n",
            "1891 [] []\n",
            "0 0 0\n",
            "475 212 245\n",
            "1892 ['cdc'] []\n",
            "0 0 1\n",
            "475 212 246\n",
            "1893 ['slick skillet serenaders', 'new orleans'] ['fb', 'bywater', 'new orleans']\n",
            "1 2 1\n",
            "476 214 247\n",
            "1894 [] []\n",
            "0 0 0\n",
            "476 214 247\n",
            "1895 [] ['quaranti']\n",
            "0 1 0\n",
            "476 215 247\n",
            "1896 ['trump'] ['trump', 'coronavirus']\n",
            "1 1 0\n",
            "477 216 247\n",
            "1897 ['coronavirus', 'union county'] ['coronavirus', 'union county']\n",
            "2 0 0\n",
            "479 216 247\n",
            "1898 ['super mario chain chomp toy'] ['mario chain chomp']\n",
            "0 1 1\n",
            "479 217 248\n",
            "1899 ['coronavirus', 'union county'] ['coronavirus', 'union county']\n",
            "2 0 0\n",
            "481 217 248\n",
            "1900 [] ['bahamians']\n",
            "0 1 0\n",
            "481 218 248\n",
            "1901 [] []\n",
            "0 0 0\n",
            "481 218 248\n",
            "1902 [] []\n",
            "0 0 0\n",
            "481 218 248\n",
            "1903 ['coronavirus', 'ocean'] ['coronavirus', 'ocean']\n",
            "2 0 0\n",
            "483 218 248\n",
            "1904 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1905 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1906 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1907 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1908 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1909 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1910 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1911 [] []\n",
            "0 0 0\n",
            "483 218 248\n",
            "1912 ['chanyeol'] ['chanyeol']\n",
            "1 0 0\n",
            "484 218 248\n",
            "1913 [] []\n",
            "0 0 0\n",
            "484 218 248\n",
            "1914 ['ky', 'andy beshear'] ['ky', 'gov', 'andy beshear']\n",
            "2 1 0\n",
            "486 219 248\n",
            "1915 [] ['plymouth']\n",
            "0 1 0\n",
            "486 220 248\n",
            "1916 [] []\n",
            "0 0 0\n",
            "486 220 248\n",
            "1917 [] []\n",
            "0 0 0\n",
            "486 220 248\n",
            "1918 [] []\n",
            "0 0 0\n",
            "486 220 248\n",
            "1919 [] []\n",
            "0 0 0\n",
            "486 220 248\n",
            "1920 [] []\n",
            "0 0 0\n",
            "486 220 248\n",
            "1921 [] []\n",
            "0 0 0\n",
            "486 220 248\n",
            "1922 [] []\n",
            "0 0 0\n",
            "486 220 248\n",
            "1923 ['trump'] ['trump']\n",
            "1 0 0\n",
            "487 220 248\n",
            "1924 [] []\n",
            "0 0 0\n",
            "487 220 248\n",
            "1925 ['los angeles', 'west hollywood', 'beverly hills'] ['los angeles', 'west hollywood', 'beverly hills']\n",
            "3 0 0\n",
            "490 220 248\n",
            "1926 [] []\n",
            "0 0 0\n",
            "490 220 248\n",
            "1927 [] []\n",
            "0 0 0\n",
            "490 220 248\n",
            "1928 [] []\n",
            "0 0 0\n",
            "490 220 248\n",
            "1929 [] []\n",
            "0 0 0\n",
            "490 220 248\n",
            "1930 [] []\n",
            "0 0 0\n",
            "490 220 248\n",
            "1931 ['sam raimi'] ['sam raimi']\n",
            "1 0 0\n",
            "491 220 248\n",
            "1932 ['beshear'] ['beshear']\n",
            "1 0 0\n",
            "492 220 248\n",
            "1933 ['raleigh'] ['raleigh']\n",
            "1 0 0\n",
            "493 220 248\n",
            "1934 ['facetime'] ['god', 'facetime']\n",
            "1 1 0\n",
            "494 221 248\n",
            "1935 [] []\n",
            "0 0 0\n",
            "494 221 248\n",
            "1936 ['tds'] []\n",
            "0 0 1\n",
            "494 221 249\n",
            "1937 [] []\n",
            "0 0 0\n",
            "494 221 249\n",
            "1938 [] []\n",
            "0 0 0\n",
            "494 221 249\n",
            "1939 [] []\n",
            "0 0 0\n",
            "494 221 249\n",
            "1940 ['covid-19'] []\n",
            "0 0 1\n",
            "494 221 250\n",
            "1941 [] []\n",
            "0 0 0\n",
            "494 221 250\n",
            "1942 [] []\n",
            "0 0 0\n",
            "494 221 250\n",
            "1943 ['d.c. cyber warrior'] []\n",
            "0 0 1\n",
            "494 221 251\n",
            "1944 [] []\n",
            "0 0 0\n",
            "494 221 251\n",
            "1945 [] []\n",
            "0 0 0\n",
            "494 221 251\n",
            "1946 [] []\n",
            "0 0 0\n",
            "494 221 251\n",
            "1947 [] []\n",
            "0 0 0\n",
            "494 221 251\n",
            "1948 ['zoom'] ['house party', 'zoom']\n",
            "1 1 0\n",
            "495 222 251\n",
            "1949 [] []\n",
            "0 0 0\n",
            "495 222 251\n",
            "1950 [] []\n",
            "0 0 0\n",
            "495 222 251\n",
            "1951 [] []\n",
            "0 0 0\n",
            "495 222 251\n",
            "1952 ['basketball'] []\n",
            "0 0 1\n",
            "495 222 252\n",
            "1953 [] ['finley fox']\n",
            "0 1 0\n",
            "495 223 252\n",
            "1954 [] []\n",
            "0 0 0\n",
            "495 223 252\n",
            "1955 [] []\n",
            "0 0 0\n",
            "495 223 252\n",
            "1956 [] []\n",
            "0 0 0\n",
            "495 223 252\n",
            "1957 ['beshear', 'basketball'] ['beshear']\n",
            "1 0 1\n",
            "496 223 253\n",
            "1958 ['beshear'] ['beshear']\n",
            "1 0 0\n",
            "497 223 253\n",
            "1959 [] []\n",
            "0 0 0\n",
            "497 223 253\n",
            "1960 [] []\n",
            "0 0 0\n",
            "497 223 253\n",
            "1961 [] []\n",
            "0 0 0\n",
            "497 223 253\n",
            "1962 [] []\n",
            "0 0 0\n",
            "497 223 253\n",
            "1963 ['basketball'] []\n",
            "0 0 1\n",
            "497 223 254\n",
            "1964 [] []\n",
            "0 0 0\n",
            "497 223 254\n",
            "1965 [] []\n",
            "0 0 0\n",
            "497 223 254\n",
            "1966 [] []\n",
            "0 0 0\n",
            "497 223 254\n",
            "1967 [] []\n",
            "0 0 0\n",
            "497 223 254\n",
            "1968 ['coronavirus'] ['coronavirus']\n",
            "1 0 0\n",
            "498 223 254\n",
            "1969 [] []\n",
            "0 0 0\n",
            "498 223 254\n",
            "1970 ['beshear'] ['beshear']\n",
            "1 0 0\n",
            "499 223 254\n",
            "1971 [] []\n",
            "0 0 0\n",
            "499 223 254\n",
            "1972 [] []\n",
            "0 0 0\n",
            "499 223 254\n",
            "1973 [] ['limerick']\n",
            "0 1 0\n",
            "499 224 254\n",
            "1974 ['richmond'] ['richmond']\n",
            "1 0 0\n",
            "500 224 254\n",
            "1975 ['shehu of borno', 'abubakar ibn umar garbai el amin', 'el-kanemi'] ['borno', 'abubakar', 'el-kanemi']\n",
            "1 2 2\n",
            "501 226 256\n",
            "1976 ['sweden', 'world health organisation'] ['sweden']\n",
            "1 0 1\n",
            "502 226 257\n",
            "1977 ['city of vidalia police'] ['city of vidalia']\n",
            "0 1 1\n",
            "502 227 258\n",
            "1978 [] []\n",
            "0 0 0\n",
            "502 227 258\n",
            "1979 [] []\n",
            "0 0 0\n",
            "502 227 258\n",
            "1980 ['rooney'] ['art rooney ii']\n",
            "0 1 1\n",
            "502 228 259\n",
            "1981 [] []\n",
            "0 0 0\n",
            "502 228 259\n",
            "1982 [] []\n",
            "0 0 0\n",
            "502 228 259\n",
            "1983 [] []\n",
            "0 0 0\n",
            "502 228 259\n",
            "1984 ['lila higgins'] ['lila higgins']\n",
            "1 0 0\n",
            "503 228 259\n",
            "1985 ['gov beshear'] ['gov beshear']\n",
            "1 0 0\n",
            "504 228 259\n",
            "1986 ['io connect services'] []\n",
            "0 0 1\n",
            "504 228 260\n",
            "1987 [] []\n",
            "0 0 0\n",
            "504 228 260\n",
            "1988 [] []\n",
            "0 0 0\n",
            "504 228 260\n",
            "1989 ['aa'] ['aa']\n",
            "1 0 0\n",
            "505 228 260\n",
            "1990 ['trump'] ['trump']\n",
            "1 0 0\n",
            "506 228 260\n",
            "1991 [] []\n",
            "0 0 0\n",
            "506 228 260\n",
            "1992 [] []\n",
            "0 0 0\n",
            "506 228 260\n",
            "1993 [] []\n",
            "0 0 0\n",
            "506 228 260\n",
            "1994 [] []\n",
            "0 0 0\n",
            "506 228 260\n",
            "1995 [] []\n",
            "0 0 0\n",
            "506 228 260\n",
            "1996 [] []\n",
            "0 0 0\n",
            "506 228 260\n",
            "1997 ['mbu athletic'] ['mbu athletic']\n",
            "1 0 0\n",
            "507 228 260\n",
            "TP||||FP||||FN\n",
            "507 228 260 735 767\n",
            "Precision: 0.689795918367347\n",
            "Recall: 0.6610169491525424\n",
            "F1: 0.6750998668442078\n",
            "time_taken 1.9367718696594238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyV04iUobxUU"
      },
      "source": [
        "## **Extracting Embeddings to train the Phrase Embedder on STS Dataset-- (NOT REQD TO RUN) unless training the phrase Embedder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLexEQkrlZzq"
      },
      "source": [
        "class PhraseEmbeddingDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, input, output):\n",
        "\n",
        "        # print(data[0])\n",
        "        # self.data = np.asarray(data)\n",
        "        # self.output = np.asarray(output)\n",
        "\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "\n",
        "        print(type(self.input),type(self.output))\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.input) == len(self.output)\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.input[idx]\n",
        "        y = self.output[idx]\n",
        "        return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKZgcTA_EPZd"
      },
      "source": [
        "def compile_token_embeddings(lst):\n",
        "    token_embeddings=[]\n",
        "    for tup in lst:\n",
        "        token = tup[0]\n",
        "        # tup[1] is feature dict, keys 0 to 99\n",
        "        embedding_dict = tup[1]\n",
        "        embedding_arr = np.array([embedding_dict['feat'+str(i)] for i in range(0,100)])\n",
        "        token_embeddings.append(torch.from_numpy(embedding_arr).float())\n",
        "    return token_embeddings\n",
        "\n",
        "def quick_extract_sentence_token_embeddings(gaguilar_sentence_outputs):\n",
        "    entity_embeddings=[]\n",
        "    for key, value in gaguilar_sentence_outputs.items():\n",
        "        token_embedding_list= list(value[4])\n",
        "        entity_aware_embeddings = compile_token_embeddings(token_embedding_list)\n",
        "        entity_embeddings.append(torch.stack(entity_aware_embeddings))\n",
        "    return entity_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Ke_L8gkvew",
        "outputId": "fa8e079a-d39f-4050-a82c-f84e54000bc8"
      },
      "source": [
        "def get_sts_data_scores(filename):\n",
        "    stsDataDictList=[]\n",
        "    stsData_columns =['sentence1', 'sentence2', 'score']\n",
        "    f=open(\"data/stsbenchmark/\"+filename+\".csv\",'r')\n",
        "    file_text= f.read()\n",
        "    lines=file_text.split('\\n')\n",
        "    for line in lines:\n",
        "        if(line):\n",
        "            fields=line.split('\\t')\n",
        "    #         print(len(fields))\n",
        "            dataDict={'sentence1':fields[5],'sentence2':fields[6],'score':float(fields[4])/5.0}\n",
        "    #         print(dataDict)\n",
        "            stsDataDictList.append(dataDict)\n",
        "    stsData=pd.DataFrame(stsDataDictList)\n",
        "    return stsData['score'].tolist()\n",
        "\n",
        "stsTrainDataScores = get_sts_data_scores('sts-train')\n",
        "print(len(stsTrainDataScores))\n",
        "# print(stsTrainData.columns)\n",
        "\n",
        "stsDevDataScores = get_sts_data_scores('sts-dev')\n",
        "print(len(stsDevDataScores))\n",
        "# print(stsDevData.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5749\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAByj-azNh0U",
        "outputId": "84f34c41-86d8-425d-9fcf-181651a905fb"
      },
      "source": [
        "from executor import *\n",
        "trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor = prepare_for_training()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4403 4403\n",
            "Loading twitter embeddings...\n",
            "Loading gazetteers embeddings...\n",
            "Building neural network...\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 00065: early stopping\n",
            "Saved model to disk\n",
            "got features...\n",
            "got xseq...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnry5EJWCuTq",
        "outputId": "df591cf2-8738-425d-8a42-1e7d61114674"
      },
      "source": [
        "# from executor import *\n",
        "\n",
        "#Training Data\n",
        "#For Source Sentences:\n",
        "# training_source_sentence_df_dict_gaguilar, training_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.train.conll')\n",
        "training_source_sentence_df_dict_gaguilar, training_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.train.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "source_sentence_embeddings_train = quick_extract_sentence_token_embeddings(training_source_sentence_df_dict_gaguilar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Argument List: sts.sentence1.train.conll\n",
            "data/stsbenchmark/sts.sentence1.train.conll.preproc.url\n",
            "data/stsbenchmark/sts.sentence1.train.conll.preproc.url.postag\n",
            "Loading tweets...\n",
            "Loading pos tags...\n",
            "5749 5749\n",
            "Generating encodings...\n",
            "reached prediction\n",
            "got probabilities\n",
            "end of prediction\n",
            "starting crf predictor\n",
            "got features...\n",
            "got xseq...\n",
            "crf prediction done\n",
            "63470 4 63470\n",
            "63470 63470 63470 63470\n",
            "100\n",
            "test time_taken: 21.119394302368164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pU7Z7aRNyPV",
        "outputId": "1b2456be-e7f1-40a9-aa16-1971c968f5a3"
      },
      "source": [
        "#Training Data\n",
        "#For Target Sentences:\n",
        "# training_target_sentence_df_dict_gaguilar, training_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.train.conll')\n",
        "training_target_sentence_df_dict_gaguilar, training_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.train.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "target_sentence_embeddings_train = quick_extract_sentence_token_embeddings(training_target_sentence_df_dict_gaguilar)\n",
        "assert len(source_sentence_embeddings_train)==len(target_sentence_embeddings_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Argument List: sts.sentence2.train.conll\n",
            "data/stsbenchmark/sts.sentence2.train.conll.preproc.url\n",
            "data/stsbenchmark/sts.sentence2.train.conll.preproc.url.postag\n",
            "Loading tweets...\n",
            "Loading pos tags...\n",
            "5749 5749\n",
            "Generating encodings...\n",
            "reached prediction\n",
            "got probabilities\n",
            "end of prediction\n",
            "starting crf predictor\n",
            "got features...\n",
            "got xseq...\n",
            "crf prediction done\n",
            "63559 4 63559\n",
            "63559 63559 63559 63559\n",
            "100\n",
            "test time_taken: 20.679669857025146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiSaTRFOm8Me",
        "outputId": "18b3dd6b-e02d-4140-da6f-24c3e82b1e0a"
      },
      "source": [
        "#Validation Data\n",
        "#For Source Sentences:\n",
        "# dev_source_sentence_df_dict_gaguilar, dev_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.dev.conll')\n",
        "dev_source_sentence_df_dict_gaguilar, dev_source_tweet_to_sentences_w_annotation = execute('sts.sentence1.dev.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "source_sentence_embeddings_dev = quick_extract_sentence_token_embeddings(dev_source_sentence_df_dict_gaguilar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Argument List: sts.sentence1.dev.conll\n",
            "data/stsbenchmark/sts.sentence1.dev.conll.preproc.url\n",
            "data/stsbenchmark/sts.sentence1.dev.conll.preproc.url.postag\n",
            "Loading tweets...\n",
            "Loading pos tags...\n",
            "1500 1500\n",
            "Generating encodings...\n",
            "reached prediction\n",
            "got probabilities\n",
            "end of prediction\n",
            "starting crf predictor\n",
            "got features...\n",
            "got xseq...\n",
            "crf prediction done\n",
            "19273 4 19273\n",
            "19273 19273 19273 19273\n",
            "100\n",
            "test time_taken: 6.149847030639648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0OQLHuK50-4",
        "outputId": "f9c1b9b9-79b0-4a8e-9e1c-46561ffd26eb"
      },
      "source": [
        "#Validation Data\n",
        "#For Target Sentences:\n",
        "# dev_target_sentence_df_dict_gaguilar, dev_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.dev.conll')\n",
        "dev_target_sentence_df_dict_gaguilar, dev_target_tweet_to_sentences_w_annotation = execute('sts.sentence2.dev.conll', trained_word2index, trained_index2postag, trained_gaze2index, trained_gaze_inputs, trained_char_inputs, trained_word_inputs, trained_mtl_network, trained_fextractor)\n",
        "target_sentence_embeddings_dev = quick_extract_sentence_token_embeddings(dev_target_sentence_df_dict_gaguilar)\n",
        "assert len(source_sentence_embeddings_dev)==len(target_sentence_embeddings_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Argument List: sts.sentence2.dev.conll\n",
            "data/stsbenchmark/sts.sentence2.dev.conll.preproc.url\n",
            "data/stsbenchmark/sts.sentence2.dev.conll.preproc.url.postag\n",
            "Loading tweets...\n",
            "Loading pos tags...\n",
            "1500 1500\n",
            "Generating encodings...\n",
            "reached prediction\n",
            "got probabilities\n",
            "end of prediction\n",
            "starting crf predictor\n",
            "got features...\n",
            "got xseq...\n",
            "crf prediction done\n",
            "19151 4 19151\n",
            "19151 19151 19151 19151\n",
            "100\n",
            "test time_taken: 6.857956171035767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QFGRmoWa010",
        "outputId": "e3068c4c-9c79-4776-c1c5-20385220b278"
      },
      "source": [
        "#Printing some shapes\n",
        "print(len(source_sentence_embeddings_train), len(target_sentence_embeddings_train))\n",
        "print(source_sentence_embeddings_train[0].shape)\n",
        "embeddingSize=list(source_sentence_embeddings_train[0][0].shape)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5749 5749\n",
            "torch.Size([6, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89OV0OTDbSrU",
        "outputId": "a39555df-9b7e-48f5-d942-ba0f424825ec"
      },
      "source": [
        "# Datasets and DataLoaders\n",
        "training_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_train, target_sentence_embeddings_train))),stsTrainDataScores)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, shuffle=True)\n",
        "\n",
        "validation_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_dev, target_sentence_embeddings_dev))),stsDevDataScores)\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> <class 'list'>\n",
            "<class 'list'> <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7iLY5dUga0h"
      },
      "source": [
        "def save_ckp(state, is_best, checkpoint_dir):\n",
        "    # f_path = checkpoint_dir + '/checkpoint.pt' \n",
        "    f_path = checkpoint_dir + '/checkpoint_model100.pt' #_model100\n",
        "    torch.save(state, f_path)\n",
        "    # if is_best:\n",
        "    #     best_fpath = best_model_dir +'/best_model.pt'\n",
        "    #     shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA95S7YQizvJ"
      },
      "source": [
        "# Initialize network\n",
        "phraseEmbeddingModel = PhraseEmbedding(embeddingSize, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuVuM-S9i_F4",
        "outputId": "3d9ec81f-97ac-4f3f-b561-fd237410869c"
      },
      "source": [
        "checkpoint_dir='entityEmbedding/model_checkpoints'\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 200\n",
        "patience = 5\n",
        "\n",
        "# Train Network\n",
        "history_training= []\n",
        "history_validation = []\n",
        "best_loss = np.float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_batch_loss=[]\n",
        "    for batch_idx, (data, target) in enumerate(training_generator):\n",
        "        target = torch.tensor(float(target)).to(device=device)\n",
        "        out = phraseEmbeddingModel(data)\n",
        "        # print(out.item())\n",
        "        # if(not math.isnan(out.item())):\n",
        "            # print(data)\n",
        "        loss = criterion(out, target)\n",
        "        # print(loss.item())\n",
        "        training_batch_loss.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(phraseEmbeddingModel.parameters(), 1.0)\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "    combined_training_loss = np.mean(training_batch_loss)\n",
        "    print('combined epoch training loss:', combined_training_loss)\n",
        "    history_training.append(combined_training_loss)\n",
        "\n",
        "    #Validation: DO NOT BACKPROPAGATE HERE\n",
        "    validation_batch_loss = []\n",
        "    print_only_one=True\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(validation_generator):\n",
        "            target = torch.tensor(float(target)).to(device=device)\n",
        "            if(print_only_one):\n",
        "                # print(len(data[0]),len(data[1]))\n",
        "                # print(type(data))\n",
        "                print_only_one=False\n",
        "            out = phraseEmbeddingModel(data)\n",
        "            loss = criterion(out, target)\n",
        "            validation_batch_loss.append(loss.item())\n",
        "            # print(validation_batch_loss)\n",
        "    combined_validation_loss= np.mean(validation_batch_loss)\n",
        "    history_validation.append(combined_validation_loss)\n",
        "    # if(((epoch+1)%10==0)|(epoch == (num_epochs-1))):\n",
        "    print('\\nEpoch',str(epoch+1),' Validation Loss:',combined_validation_loss)\n",
        "    \n",
        "    if(combined_validation_loss<best_loss):\n",
        "        best_loss = combined_validation_loss\n",
        "        print('saving this checkpoint')\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        save_ckp(checkpoint, True, checkpoint_dir)\n",
        "        no_improvement_counter=0\n",
        "        print('=====================================================================\\n')\n",
        "    else:\n",
        "        no_improvement_counter+=1\n",
        "        if(no_improvement_counter>patience):\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "combined epoch training loss: 0.10891616942352222\n",
            "\n",
            "Epoch 1  Validation Loss: 0.12554517802167742\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.10185760679177104\n",
            "\n",
            "Epoch 2  Validation Loss: 0.10434156257237658\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.10008667594474852\n",
            "\n",
            "Epoch 3  Validation Loss: 0.10055114981284212\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.09649177850050293\n",
            "\n",
            "Epoch 4  Validation Loss: 0.11075373659062514\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.09604405649547065\n",
            "\n",
            "Epoch 5  Validation Loss: 0.12556172273957916\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.09496434017787644\n",
            "\n",
            "Epoch 6  Validation Loss: 0.09451752110034481\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.09452879607698458\n",
            "\n",
            "Epoch 7  Validation Loss: 0.09344766367676573\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.09325055521371109\n",
            "\n",
            "Epoch 8  Validation Loss: 0.10551437864408295\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.09268569662687376\n",
            "\n",
            "Epoch 9  Validation Loss: 0.09588899192623292\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.09142340151560076\n",
            "\n",
            "Epoch 10  Validation Loss: 0.11093361574486824\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.09132581624944727\n",
            "\n",
            "Epoch 11  Validation Loss: 0.11075471364170539\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.09056864197786142\n",
            "\n",
            "Epoch 12  Validation Loss: 0.11030968930309495\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.0904545501498637\n",
            "\n",
            "Epoch 13  Validation Loss: 0.09186793746456562\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.09073285792161517\n",
            "\n",
            "Epoch 14  Validation Loss: 0.10625727870257967\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08884216146523119\n",
            "\n",
            "Epoch 15  Validation Loss: 0.09608570842529074\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08949825134774618\n",
            "\n",
            "Epoch 16  Validation Loss: 0.09520662359377612\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.0876103762844051\n",
            "\n",
            "Epoch 17  Validation Loss: 0.08947608150500526\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.08764630706589387\n",
            "\n",
            "Epoch 18  Validation Loss: 0.12085369315342967\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08841164103102717\n",
            "\n",
            "Epoch 19  Validation Loss: 0.09443884671512641\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08854414427035745\n",
            "\n",
            "Epoch 20  Validation Loss: 0.08591445285199817\n",
            "=====================================================================\n",
            "\n",
            "saving this checkpoint\n",
            "combined epoch training loss: 0.08743191640004275\n",
            "\n",
            "Epoch 21  Validation Loss: 0.09463686330579359\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08757446814471646\n",
            "\n",
            "Epoch 22  Validation Loss: 0.09481488223757374\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08741262244226493\n",
            "\n",
            "Epoch 23  Validation Loss: 0.08954499525641238\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08569902026165956\n",
            "\n",
            "Epoch 24  Validation Loss: 0.09207101688873774\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08619907719167302\n",
            "\n",
            "Epoch 25  Validation Loss: 0.0899756463330524\n",
            "=====================================================================\n",
            "\n",
            "combined epoch training loss: 0.08589564432579362\n",
            "\n",
            "Epoch 26  Validation Loss: 0.09697759616191791\n",
            "=====================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooCO54HSjd2q"
      },
      "source": [
        "checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint_model100.pt' #300\n",
        "# model_dir = 'best-model'\n",
        "# save_ckp(checkpoint, True, checkpoint_dir, model_dir)\n",
        "\n",
        "# load the last checkpoint with the best model\n",
        "entityPhraseEmbedder, optimizer, start_epoch = load_ckp(checkpoint_path, phraseEmbeddingModel, optimizer)\n",
        "print('Loading model from epoch:', start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgqGsSq3i1w4"
      },
      "source": [
        "## **Regular EMD Runs (NO NEED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l4-QtqguiV0"
      },
      "source": [
        "# !python3 main.py btc.emerging.test.conll\n",
        "# !python3 main.py emerging.test.conll"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}