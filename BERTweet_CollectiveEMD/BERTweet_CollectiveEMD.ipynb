{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERTweet-CollectiveEMD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wae23yf9PDRb",
        "PvEaaQhsX8aG",
        "GCq9EEmjU1li",
        "-a_6h3vG8UrG",
        "sm1P-HYgU6kb",
        "89dhqxmg5m25",
        "IN17vi2QJh9Q",
        "FOGT-F39-YBx",
        "tIpoGN3z2Wd0",
        "DhUJiCRR-iFG",
        "M4NIDPJU5NOQ",
        "p0eE9I3zuArg",
        "ncj3sC0FkBwA",
        "XY3zs23wC6_3",
        "uRvhsCdn-NxN",
        "mhVLIVUy57ZX",
        "LyuFLuUSby3M"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b58978082f3240e4b37e171069ca16a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d37e506ecf80405f92ebd50d619d40b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e89d6219dc6543c98d8d5e4dbc43b8d7",
              "IPY_MODEL_4ce57f43d9aa48b2bc7f57a373078133",
              "IPY_MODEL_1a9c81d1d0e94f6387c5600dc9163c07"
            ]
          }
        },
        "d37e506ecf80405f92ebd50d619d40b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e89d6219dc6543c98d8d5e4dbc43b8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8800f85f1d6f43c99ded82337e8edcb6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30c3ab9f76a24f3f80a2f38eebce120a"
          }
        },
        "4ce57f43d9aa48b2bc7f57a373078133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0a4cdede55494b1ebd1f53e2521088b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2543,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2543,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_296311dffe7b4dbca19a45afe9fc9279"
          }
        },
        "1a9c81d1d0e94f6387c5600dc9163c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0233b0dbf26245fdba0bfbfac2e64bc9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7.46k/? [00:00&lt;00:00, 277kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_720470eb591c4c3cb0f5d8c20b0093d4"
          }
        },
        "8800f85f1d6f43c99ded82337e8edcb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30c3ab9f76a24f3f80a2f38eebce120a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a4cdede55494b1ebd1f53e2521088b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "296311dffe7b4dbca19a45afe9fc9279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0233b0dbf26245fdba0bfbfac2e64bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "720470eb591c4c3cb0f5d8c20b0093d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ff87e8a208e45c499d0d93c5c7a83c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1cd5e91d89a14a5da207fbe088621f3a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e85ff5b5036d4bd78e087f902231f0a3",
              "IPY_MODEL_efe1b6d43c9b4d0e914c0eb0169ea56d",
              "IPY_MODEL_11c7ed478a714671a359e06ee0e96ed6"
            ]
          }
        },
        "1cd5e91d89a14a5da207fbe088621f3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e85ff5b5036d4bd78e087f902231f0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb92d806a1fd4203a74d201fa40c63a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7586b918ec9944caaea323aa06a7adb9"
          }
        },
        "efe1b6d43c9b4d0e914c0eb0169ea56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4c452a88fc2b4beea42fa8b63c4dcb7c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0a21d564fde46e49ee85c091e838280"
          }
        },
        "11c7ed478a714671a359e06ee0e96ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ef80a280a3342b5927f346377c8d8c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.28k/? [00:00&lt;00:00, 181kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d63bf8db2a54304b6a32cc6eac9d453"
          }
        },
        "fb92d806a1fd4203a74d201fa40c63a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7586b918ec9944caaea323aa06a7adb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c452a88fc2b4beea42fa8b63c4dcb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0a21d564fde46e49ee85c091e838280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ef80a280a3342b5927f346377c8d8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d63bf8db2a54304b6a32cc6eac9d453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5b0a83aed2b46ce82c22224142e9fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4464da1c5d644a208fb051c7e3ca04dc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b1a5790666e84a7784c1ef185397d920",
              "IPY_MODEL_0392174d3743419dbcffc11b89f7d0c4",
              "IPY_MODEL_2bd75ddc68a34449a5b6bda88ef64ac7"
            ]
          }
        },
        "4464da1c5d644a208fb051c7e3ca04dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1a5790666e84a7784c1ef185397d920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b76ebd709a64215ad0efce0b2e587cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6433580aec374bf1a98ba8cb8aa88218"
          }
        },
        "0392174d3743419dbcffc11b89f7d0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f59074ae6baf41b29a9ff0af6436dbd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_938b54b019f04b9e95cafd878d0b1e05"
          }
        },
        "2bd75ddc68a34449a5b6bda88ef64ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7087a331b3c34b2fa45a92a27cbbb6c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00,  3.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fecbd237361f4e458a56edbdaca0aed6"
          }
        },
        "5b76ebd709a64215ad0efce0b2e587cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6433580aec374bf1a98ba8cb8aa88218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f59074ae6baf41b29a9ff0af6436dbd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "938b54b019f04b9e95cafd878d0b1e05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7087a331b3c34b2fa45a92a27cbbb6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fecbd237361f4e458a56edbdaca0aed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "daa9ccca1cb944999802a1ddc262f57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_21c200891aff49c8a9dafb756a33e283",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8cf9e8fd9219430b8cb82ffc856ee495",
              "IPY_MODEL_5d758e0e598e4c4c94b1b341f75fbf69",
              "IPY_MODEL_4f751bd50b924aba992ccb0c1b6cb0b2"
            ]
          }
        },
        "21c200891aff49c8a9dafb756a33e283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8cf9e8fd9219430b8cb82ffc856ee495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b37d079e4e1a424b896dd46c1a1b9e22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ff00446f5754330b3370d8604d39304"
          }
        },
        "5d758e0e598e4c4c94b1b341f75fbf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aeb35acd796e4a00a72e554d870ae070",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 185319,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 185319,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23fafc05458a4e08bde49e5bec871652"
          }
        },
        "4f751bd50b924aba992ccb0c1b6cb0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e8fc1cd5b3424e0890705be08a038c50",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 494k/? [00:00&lt;00:00, 13.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5345ded755c74ceaaa24ab5cd0a3a6ed"
          }
        },
        "b37d079e4e1a424b896dd46c1a1b9e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ff00446f5754330b3370d8604d39304": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aeb35acd796e4a00a72e554d870ae070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23fafc05458a4e08bde49e5bec871652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8fc1cd5b3424e0890705be08a038c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5345ded755c74ceaaa24ab5cd0a3a6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40f05371ce9e4e54aae13db8138a772f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec27633defb44fe5b2204c3f6fc29457",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc62479acd514814850b4d54c535d221",
              "IPY_MODEL_55e9e1032d654d86ae2853d8496e9f45",
              "IPY_MODEL_dd517a47260344458dc14cf955c74d75"
            ]
          }
        },
        "ec27633defb44fe5b2204c3f6fc29457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc62479acd514814850b4d54c535d221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_23721ecae5434f83acc28d19a14157f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8597679268444a8e9ff4d3aaded9c056"
          }
        },
        "55e9e1032d654d86ae2853d8496e9f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9842c48ac08a4cc4945e64a947b72a3c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 39129,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 39129,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_427b8e0a331a4b20a7c8fe3ecfc790c2"
          }
        },
        "dd517a47260344458dc14cf955c74d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc188f774ef349e69e0e435da8570792",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 115k/? [00:00&lt;00:00, 3.99MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e98ddbf46e646279c178b1e1834000a"
          }
        },
        "23721ecae5434f83acc28d19a14157f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8597679268444a8e9ff4d3aaded9c056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9842c48ac08a4cc4945e64a947b72a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "427b8e0a331a4b20a7c8fe3ecfc790c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc188f774ef349e69e0e435da8570792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e98ddbf46e646279c178b1e1834000a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4891df1fda81478d88718d7342f9fa52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd4ebc1134c14726913d4daf28fef999",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b3a121ad8fca422fbb8060023a6b7958",
              "IPY_MODEL_e5b637ce8f27451ba9c484b905d68397",
              "IPY_MODEL_7ae6cc2e4cd243239337d393d2f9f81d"
            ]
          }
        },
        "bd4ebc1134c14726913d4daf28fef999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3a121ad8fca422fbb8060023a6b7958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_170977bc0f244ed8a6cfcceb245aaa0e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_235c6f43e7cf488ea0ff584b6e9dfe2a"
          }
        },
        "e5b637ce8f27451ba9c484b905d68397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_55b470cbef24434c82baae5d8cfc8214",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 66855,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 66855,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0558bd426e0c459f9fffff75dce7c2e3"
          }
        },
        "7ae6cc2e4cd243239337d393d2f9f81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_77ffcc54927842bbb7dc180ae8c5ed5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 192k/? [00:00&lt;00:00, 5.81MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2070595f6b464b849e6d3158656db65f"
          }
        },
        "170977bc0f244ed8a6cfcceb245aaa0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "235c6f43e7cf488ea0ff584b6e9dfe2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55b470cbef24434c82baae5d8cfc8214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0558bd426e0c459f9fffff75dce7c2e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77ffcc54927842bbb7dc180ae8c5ed5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2070595f6b464b849e6d3158656db65f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56afbabc75b54de1809dd5517f3be060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8920e243c40848d8ae9ab00addbdb5c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_deb2bdb228064c95baf21fd35cdc8170",
              "IPY_MODEL_cf8798aed733458bbb273f3336f87f9d",
              "IPY_MODEL_2e52b6f76b104c8ea326b6b73c589ab9"
            ]
          }
        },
        "8920e243c40848d8ae9ab00addbdb5c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "deb2bdb228064c95baf21fd35cdc8170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b91063c7d40144158a4fe9f0c3efe4e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e978a9e11def4eb2bd01e271b99cb53a"
          }
        },
        "cf8798aed733458bbb273f3336f87f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c0200f8c4d674506a22876bc7e274d43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c12ff3d88904c7d95e6cdddbd0efa7c"
          }
        },
        "2e52b6f76b104c8ea326b6b73c589ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_444957267a624f34859f2e6191b8c4d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 95.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5e2c948011b4e49a84d67051b1f7ccc"
          }
        },
        "b91063c7d40144158a4fe9f0c3efe4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e978a9e11def4eb2bd01e271b99cb53a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0200f8c4d674506a22876bc7e274d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c12ff3d88904c7d95e6cdddbd0efa7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "444957267a624f34859f2e6191b8c4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5e2c948011b4e49a84d67051b1f7ccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8bb6cc163e874b6789bbe30c92f1ad7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ff174c2cdb35432787787967fbece211",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8de4f1985b9b40609784eea7c398ffc0",
              "IPY_MODEL_3ddb63a6bf2c4b17b4f573e46394f4ce",
              "IPY_MODEL_73c8dd7ce9754492b48e0d32cafbf2a1"
            ]
          }
        },
        "ff174c2cdb35432787787967fbece211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8de4f1985b9b40609784eea7c398ffc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7b10c7001f840288dbbee9e08f364d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_14c72e78bd074d98a058f80cb11cd8a2"
          }
        },
        "3ddb63a6bf2c4b17b4f573e46394f4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_531a5f270bff4f4e9b002f3dd3b34a7c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ae088d3e94f4b5390d8e09899cfe4c7"
          }
        },
        "73c8dd7ce9754492b48e0d32cafbf2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32ef384732644d71958298a8712b0eaf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3087/0 [00:00&lt;00:00, 8244.00 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d821ad90189846d9bc96f68ed7332b45"
          }
        },
        "d7b10c7001f840288dbbee9e08f364d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "14c72e78bd074d98a058f80cb11cd8a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "531a5f270bff4f4e9b002f3dd3b34a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ae088d3e94f4b5390d8e09899cfe4c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32ef384732644d71958298a8712b0eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d821ad90189846d9bc96f68ed7332b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff92f61d5aae40d0a18c3a18df5da959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_35a98736a7a44128bbbf22de205f5758",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d82f3ca2a4584425a340531d33631029",
              "IPY_MODEL_0d693e0827944840a305eeb9f62bdf91",
              "IPY_MODEL_1d6abd6125e94fc28705610617158361"
            ]
          }
        },
        "35a98736a7a44128bbbf22de205f5758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d82f3ca2a4584425a340531d33631029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ffc0a35f0594a37a4b2f08ce53fc520",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88937c5d220c4a7cb8966bd0ea1f912c"
          }
        },
        "0d693e0827944840a305eeb9f62bdf91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ec94d706292439283ec14a1f1dfd255",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_517f74c01d614157a189f3ef8754e8e1"
          }
        },
        "1d6abd6125e94fc28705610617158361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bf4b0ca1104a4aff853754e0e351ddcd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 687/0 [00:00&lt;00:00, 6867.49 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd3685ecaf824583bcb836f8d59998cc"
          }
        },
        "9ffc0a35f0594a37a4b2f08ce53fc520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88937c5d220c4a7cb8966bd0ea1f912c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ec94d706292439283ec14a1f1dfd255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "517f74c01d614157a189f3ef8754e8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf4b0ca1104a4aff853754e0e351ddcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd3685ecaf824583bcb836f8d59998cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f430146b9694fe5ac8238725277ad29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eac8af198a6f4c28855a4a3e2c4b304e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6590fd96609c41e9892079643de2a596",
              "IPY_MODEL_3066e3a61b9448e4b25fbef0b9f4ab49",
              "IPY_MODEL_c89addbaa7554a0a96b52359d855e359"
            ]
          }
        },
        "eac8af198a6f4c28855a4a3e2c4b304e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6590fd96609c41e9892079643de2a596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7db5611d00e0452e8e0843ddd0071da9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a0f98716019434fa198362b4703ae97"
          }
        },
        "3066e3a61b9448e4b25fbef0b9f4ab49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3786f8393bfa494eaed13088d4fe189e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e15951575a8342cdbac0928001acc456"
          }
        },
        "c89addbaa7554a0a96b52359d855e359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bce3a9f59aef4e3e88f6542e6b535ed3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 615/0 [00:00&lt;00:00, 6145.30 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0bd4c7a4f674d038315a37f395482b6"
          }
        },
        "7db5611d00e0452e8e0843ddd0071da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a0f98716019434fa198362b4703ae97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3786f8393bfa494eaed13088d4fe189e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e15951575a8342cdbac0928001acc456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bce3a9f59aef4e3e88f6542e6b535ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0bd4c7a4f674d038315a37f395482b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9c6974e42894cd49edcaac074620468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_88db54b1f3394a66a7a9213907d1eebc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_69ffab3da94e4664b2b19690e88932da",
              "IPY_MODEL_a4aed5e717cd4b40a884685c312d4890",
              "IPY_MODEL_21802c2ac10d4e73b72b8407835de26e"
            ]
          }
        },
        "88db54b1f3394a66a7a9213907d1eebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69ffab3da94e4664b2b19690e88932da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3d5fdb8601c145dfb3e1460735ba59dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a1fa38d2aa64566b801f5bf076d2390"
          }
        },
        "a4aed5e717cd4b40a884685c312d4890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e6c8ac616b142e794ab5a3ab5166af8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f8450875c574907839dcb81a7e4caee"
          }
        },
        "21802c2ac10d4e73b72b8407835de26e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c931c293f29f4b14a373e6eb4aa032c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 82.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c36762ac548f432fbe57a7d627fa2082"
          }
        },
        "3d5fdb8601c145dfb3e1460735ba59dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a1fa38d2aa64566b801f5bf076d2390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e6c8ac616b142e794ab5a3ab5166af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f8450875c574907839dcb81a7e4caee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c931c293f29f4b14a373e6eb4aa032c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c36762ac548f432fbe57a7d627fa2082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96b969775a994a98b7785ab242c12578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d385f83df47b4d46b3463134666f9e10",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0095c1dccfe34082ac5c68aa4e26fd94",
              "IPY_MODEL_4fe15cc87f664a9390cdc554d9f8edc7",
              "IPY_MODEL_53492322777146e99fc92bf0644e5ba9"
            ]
          }
        },
        "d385f83df47b4d46b3463134666f9e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0095c1dccfe34082ac5c68aa4e26fd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bf8b344e9d564da384bf9ed3db42d85c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c312c92ddec849db8e996335fc1c3268"
          }
        },
        "4fe15cc87f664a9390cdc554d9f8edc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a706be44d88b43f59716759a18795fae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 558,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 558,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f34bfb41f7f4020a2898908bd7dd102"
          }
        },
        "53492322777146e99fc92bf0644e5ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6032d7bc68284db1b0f1236fe2dbc2dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 558/558 [00:00&lt;00:00, 21.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d09e9f401ef4ea7bad910be3f650008"
          }
        },
        "bf8b344e9d564da384bf9ed3db42d85c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c312c92ddec849db8e996335fc1c3268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a706be44d88b43f59716759a18795fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f34bfb41f7f4020a2898908bd7dd102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6032d7bc68284db1b0f1236fe2dbc2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d09e9f401ef4ea7bad910be3f650008": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f623a0a41e14ed8ac43074d41d5a397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cae1096a39674bd09188e271b933c7ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fff98770b1b344668751a9d30764752a",
              "IPY_MODEL_62bd37b23955464587bd2f8ccfbb8a04",
              "IPY_MODEL_a5a4b98834f443999d212cc44b0c3b79"
            ]
          }
        },
        "cae1096a39674bd09188e271b933c7ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fff98770b1b344668751a9d30764752a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_78ec5a48b3b54924918c8d1362b77b39",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a25b4fe0af54596905605ac4d5a6fc0"
          }
        },
        "62bd37b23955464587bd2f8ccfbb8a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dda2a39081c84298bcd116b2f910b0d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 843438,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 843438,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_718a318d654546d4a07522e47aaf6975"
          }
        },
        "a5a4b98834f443999d212cc44b0c3b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c7566c193cea431cbd633665e5ee9721",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 824k/824k [00:00&lt;00:00, 1.14MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad1648f6ea2545e297b6ef4d6b2878d0"
          }
        },
        "78ec5a48b3b54924918c8d1362b77b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a25b4fe0af54596905605ac4d5a6fc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dda2a39081c84298bcd116b2f910b0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "718a318d654546d4a07522e47aaf6975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7566c193cea431cbd633665e5ee9721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad1648f6ea2545e297b6ef4d6b2878d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bb91d4d3ac34e1b9490e6b680c5166f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0aa4a002302b4879809a8abc8e4331a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5977b385e038422bb8dfcae39af1792d",
              "IPY_MODEL_c16645e62d2f47f18b42a3ba2d25ff10",
              "IPY_MODEL_25400abde7524a7b9a2252a5c6f84605"
            ]
          }
        },
        "0aa4a002302b4879809a8abc8e4331a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5977b385e038422bb8dfcae39af1792d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a6e9a128eac487f91661fdeea2a67b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4698846bac14b4c902c9797ddb291f9"
          }
        },
        "c16645e62d2f47f18b42a3ba2d25ff10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_553a8742afce4dd7a43b1ba9b0d956d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1078931,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1078931,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d146a8044fe14e1b8607ec33e28fac07"
          }
        },
        "25400abde7524a7b9a2252a5c6f84605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7ad12689a5b841bbbd572ad644fdd91e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.03M/1.03M [00:00&lt;00:00, 2.03MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5801355a9a74f8aa78d2a417602f858"
          }
        },
        "8a6e9a128eac487f91661fdeea2a67b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4698846bac14b4c902c9797ddb291f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "553a8742afce4dd7a43b1ba9b0d956d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d146a8044fe14e1b8607ec33e28fac07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ad12689a5b841bbbd572ad644fdd91e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5801355a9a74f8aa78d2a417602f858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f38bb9e35bde4ffba605fe9d3b636c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f7765100db584589804f3dee6a0baff0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_62a47a6f62044f06afff56d113a3e43b",
              "IPY_MODEL_7f6f579e529c4e058a47e3fabb6d27dd",
              "IPY_MODEL_09cbb0b0901f4d66afa6cfc28612cd9b"
            ]
          }
        },
        "f7765100db584589804f3dee6a0baff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62a47a6f62044f06afff56d113a3e43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9cd56cce302d44f19ee1994e72058672",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3ffa76468494a21ab52207e62cf55aa"
          }
        },
        "7f6f579e529c4e058a47e3fabb6d27dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd333546f6a44a53ba9ffb3beb5c9a59",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3394,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3394,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_553e1c577ea644b090662eab7c5e39b1"
          }
        },
        "09cbb0b0901f4d66afa6cfc28612cd9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e80c94d1c8734757a14700cea4b31c4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3394/3394 [00:10&lt;00:00, 355.06ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61bbf9fa76c3456fa062532b6fe8ea7a"
          }
        },
        "9cd56cce302d44f19ee1994e72058672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3ffa76468494a21ab52207e62cf55aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd333546f6a44a53ba9ffb3beb5c9a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "553e1c577ea644b090662eab7c5e39b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e80c94d1c8734757a14700cea4b31c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61bbf9fa76c3456fa062532b6fe8ea7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85c74c61e91946ceb7dc62531e952699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed822e59b45d4a5c98dfce10b4e4af70",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5d6f44644384395817d53d75c77964c",
              "IPY_MODEL_14f57d4d7f034b548714852654b345b6",
              "IPY_MODEL_6105d45936064347b0a024d26e884a87"
            ]
          }
        },
        "ed822e59b45d4a5c98dfce10b4e4af70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5d6f44644384395817d53d75c77964c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4bd560eb9da64b1d978a00e73dabf5a4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da04f81baf6642e692f97db54e6f5626"
          }
        },
        "14f57d4d7f034b548714852654b345b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6de7505cb8494025960313b847974fa4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1009,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1009,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58cbceb0f2ac4f06a9d38a00eaa76430"
          }
        },
        "6105d45936064347b0a024d26e884a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7b2a2b4060b74cfd99337967528be074",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1009/1009 [00:02&lt;00:00, 389.00ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ae39bdaaaa849a3b15dc28b72e29feb"
          }
        },
        "4bd560eb9da64b1d978a00e73dabf5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da04f81baf6642e692f97db54e6f5626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6de7505cb8494025960313b847974fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58cbceb0f2ac4f06a9d38a00eaa76430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b2a2b4060b74cfd99337967528be074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ae39bdaaaa849a3b15dc28b72e29feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45e1ee5cd7ab499ebbdc3dffc45f8274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7432a8a16bfa4532ad23ec1eb659916f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3c5835d9159743a2b5cea4f30bc216c1",
              "IPY_MODEL_84ff79dd745f48ec93699c456325926e",
              "IPY_MODEL_ed1b39eee5ed4872986e5135398bd64c"
            ]
          }
        },
        "7432a8a16bfa4532ad23ec1eb659916f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c5835d9159743a2b5cea4f30bc216c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f21840f7a6864c4f9e3a683290db8deb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1d5a324dba84780b9ecc05010b2208e"
          }
        },
        "84ff79dd745f48ec93699c456325926e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e2c73ea81b544b6894a826488cf50a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1287,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1287,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6ac753ae6224b9b86dec91787e627a5"
          }
        },
        "ed1b39eee5ed4872986e5135398bd64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1467fbad2aa34957bdbd2f1b4ef4ddca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1287/1287 [00:04&lt;00:00, 361.38ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_397b3c99295d48d680cfcef492154659"
          }
        },
        "f21840f7a6864c4f9e3a683290db8deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1d5a324dba84780b9ecc05010b2208e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e2c73ea81b544b6894a826488cf50a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6ac753ae6224b9b86dec91787e627a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1467fbad2aa34957bdbd2f1b4ef4ddca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "397b3c99295d48d680cfcef492154659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d329dc879044cc590af8dd5ad722a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6b02c1c6231a43869c91ed9ee76c0a16",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_785802b23d954f5dba3b220f08a1ef0f",
              "IPY_MODEL_f42c36307a1c4e13af95b28c7b2a06d6",
              "IPY_MODEL_de1d0543e800472180f8e1126df0bf5c"
            ]
          }
        },
        "6b02c1c6231a43869c91ed9ee76c0a16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "785802b23d954f5dba3b220f08a1ef0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b19f47e6dc184e498177d312e05c8820",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_757f03c8803b4bd2bf90a5cc25f9844f"
          }
        },
        "f42c36307a1c4e13af95b28c7b2a06d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8f3134d01fba412d879b3614d4273e1c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2482,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2482,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1bc429bf18d41e4bd0a4b4392a6adc0"
          }
        },
        "de1d0543e800472180f8e1126df0bf5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_70c213468f254fc9bcc37e7fa5ff08e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6.34k/? [00:00&lt;00:00, 243kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_738aa81909584903809c835a91d5b98f"
          }
        },
        "b19f47e6dc184e498177d312e05c8820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "757f03c8803b4bd2bf90a5cc25f9844f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f3134d01fba412d879b3614d4273e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1bc429bf18d41e4bd0a4b4392a6adc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70c213468f254fc9bcc37e7fa5ff08e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "738aa81909584903809c835a91d5b98f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01a82f8c427e4598b2ae3242cc02bd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e2f783123cf4f15aeeb2aee771f036a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_46eaf8fc331f45eca9c48913217ef99c",
              "IPY_MODEL_84e936e3aae34390bf79d2b89aa3050d",
              "IPY_MODEL_0fb5fa84cb7d4a8d8d47f8cc86541bf8"
            ]
          }
        },
        "9e2f783123cf4f15aeeb2aee771f036a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46eaf8fc331f45eca9c48913217ef99c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dee7ba04afae41a48200a492682467a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_576c0fdd439f4cf5a798b28a83e4f989"
          }
        },
        "84e936e3aae34390bf79d2b89aa3050d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_24a5b8085d184e92b5f430115cf825d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 542529064,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 542529064,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37759e89fffd490c9f69ea387874e0cd"
          }
        },
        "0fb5fa84cb7d4a8d8d47f8cc86541bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_edd9f91bdaea494e8c6777ce2a1344cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 517M/517M [00:09&lt;00:00, 57.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b116730687d64e0ca3121b4445d9217c"
          }
        },
        "dee7ba04afae41a48200a492682467a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "576c0fdd439f4cf5a798b28a83e4f989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24a5b8085d184e92b5f430115cf825d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37759e89fffd490c9f69ea387874e0cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "edd9f91bdaea494e8c6777ce2a1344cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b116730687d64e0ca3121b4445d9217c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wae23yf9PDRb"
      },
      "source": [
        "## **Initial imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrLFVF0oiBlh",
        "outputId": "e373f8a5-9d35-4409-8fad-84fd493dc601"
      },
      "source": [
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO30EXrb_pcx"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/BERTweet-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3WDvVQhayIZ",
        "outputId": "0919ccd3-d21a-43dc-94ff-ee1d30b6cb3e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "%cd gdrive/My Drive/BERTweet-ner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/.shortcut-targets-by-id/1bqSzZYt1KrCwNCfxS95aGiZoAl213Vk7/BERTweet-ner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eS5AgF0BMz2"
      },
      "source": [
        "\n",
        "# sys.path.insert(0,'/content/gdrive/My Drive/BERTweet-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEvo5aXwYILN",
        "outputId": "fad987b2-e481-4309-8d80-ae8eec1cdcce"
      },
      "source": [
        "!pip3 install datasets\n",
        "!pip3 install transformers\n",
        "!pip3 install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 41.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 42.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 51 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 61 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 102 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 112 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 122 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 163 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 174 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 184 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 194 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 204 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 215 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 225 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 245 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 256 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 266 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 276 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 286 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 290 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 75.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 77.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 86.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 88.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 13.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 78.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.19)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=b710bbee44dea6b150b65bc1d98535373ee7cc8c83afb9f881d3bc31275104ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.1.0 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuR-p1-pMK2Q",
        "outputId": "bf860a2b-72b7-43f6-9a39-d9f1f600fa3f"
      },
      "source": [
        "!pip3 install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 61 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 92 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 122 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 153 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 14.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=b3ecd935f5b67ca09f5117e1c28979830a45760df14fb01dd4d87fabfa05031b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUz5eC7SfFqc",
        "outputId": "034e52d2-fe2f-4c56-afaa-e5a5ddb64382"
      },
      "source": [
        "!pip3 install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 37.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=077ef993bdc25d9f7219efbf3a9a6bdc148fd870d0ca850e42326cefa2a98383\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhAmsA61gP6q",
        "outputId": "40d9ad4b-9dbc-4c76-d015-9503c984c929"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f51c4950610>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvEaaQhsX8aG"
      },
      "source": [
        "## **Initial Trainer with Default Config (NOT REQUIRED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NDFUYBfCEwG"
      },
      "source": [
        "# labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
        "# metric.compute(predictions=[labels], references=[labels])\n",
        "\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=3) #Just BIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abGq0PXk040"
      },
      "source": [
        "# args = TrainingArguments(\n",
        "#     f\"test-{task}\",\n",
        "#     evaluation_strategy = \"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     per_device_eval_batch_size=batch_size,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p7ArnENIfTE"
      },
      "source": [
        "# trainer = Trainer(\n",
        "#     model,\n",
        "#     args,\n",
        "#     train_dataset=tokenized_datasets[\"train\"],\n",
        "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
        "#     data_collator=data_collator,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5YzgeKIlzj"
      },
      "source": [
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hscli_TYIq1c"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCq9EEmjU1li"
      },
      "source": [
        "## **Predict on validation/test (NOT REQUIRED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTyESmmTIzzM"
      },
      "source": [
        "# predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "# predictions = np.argmax(predictions, axis=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYKXZlTesZAI"
      },
      "source": [
        "model_returns, labels, _ = alt_trainer.predict(tokenized_datasets[\"test\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqXzfKOmvT8Y"
      },
      "source": [
        "predictions, _ = model_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th4654zaJCNA"
      },
      "source": [
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1emxCA5lDcg"
      },
      "source": [
        "predictions\n",
        "print(len(predictions[0]))\n",
        "print(len(predictions[5]))\n",
        "print(len(predictions[50]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5CPzI0skwbH"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzbUNafHt3kj"
      },
      "source": [
        "# print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8aiYWi6B5X6"
      },
      "source": [
        "# # print(predictions[:5])\n",
        "# # Remove ignored index (special tokens)\n",
        "# true_predictions = [\n",
        "#     [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#     for prediction, label in zip(predictions, labels)\n",
        "# ]\n",
        "\n",
        "# true_labels = [\n",
        "#     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#     for prediction, label in zip(predictions, labels)\n",
        "# ]\n",
        "\n",
        "# print(true_predictions[:5])\n",
        "# print(true_labels[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wktgUaW8zVl"
      },
      "source": [
        "# results = metric.compute(predictions=true_predictions, references=true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmCDlyQsoLJY"
      },
      "source": [
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a_6h3vG8UrG"
      },
      "source": [
        "## **Setting some global stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByJwnUdZ-YGg",
        "outputId": "36ceeb23-98ff-40bb-86b7-9a1f187e4d2c"
      },
      "source": [
        "import torch\n",
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qob4MoKoXgs"
      },
      "source": [
        "def collate_token_labels(token_dict, prediction_labels):\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    for key in token_dict.keys():\n",
        "        vals=token_dict[key]\n",
        "        labels=prediction_labels[counter:counter+len(vals)]\n",
        "        if('I' in labels):\n",
        "            collated_labels.append('I')\n",
        "        elif('B' in labels):\n",
        "            collated_labels.append('B')\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "        counter+=len(vals)\n",
        "    return collated_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg6Nz2W9lp5m"
      },
      "source": [
        "# def collate_token_label_embedding(token_dict, prediction_labels, entity_embeddings):\n",
        "#     counter=0\n",
        "#     collated_labels=[]\n",
        "#     collated_entity_embeddings=[]\n",
        "#     for key in token_dict.keys():\n",
        "#         vals=token_dict[key]\n",
        "#         labels=prediction_labels[counter:counter+len(vals)]\n",
        "#         token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "# #         print(token_entity_embeddings.shape)\n",
        "#         mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "#         mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "#         collated_entity_embeddings.append(mean_tensor)\n",
        "# #         print(collated_entity_embeddings)\n",
        "#         if('I' in labels):\n",
        "#             collated_labels.append('I')\n",
        "#         elif('B' in labels):\n",
        "#             collated_labels.append('B')\n",
        "#         else:\n",
        "#             collated_labels.append('O')\n",
        "#         counter+=len(vals)\n",
        "#     assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "#     return collated_labels,collated_entity_embeddings\n",
        "\n",
        "def collate_token_label_embedding(tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    collated_entity_embeddings=[]\n",
        "    for word in tweetWordList:\n",
        "        vals=token_dict[word]\n",
        "        # print(word,vals)\n",
        "        if(counter<len(prediction_labels)):\n",
        "            labels=prediction_labels[counter:counter+len(vals)]\n",
        "            token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "    #         print(token_entity_embeddings.shape)\n",
        "            mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "            mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "            collated_entity_embeddings.append(mean_tensor)\n",
        "    #         print(collated_entity_embeddings)\n",
        "            if('I' in labels):\n",
        "                collated_labels.append('I')\n",
        "            elif('B' in labels):\n",
        "                collated_labels.append('B')\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "            counter+=len(vals)\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "            collated_entity_embeddings.append(torch.zeros(768).to(device))\n",
        "    assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "    return collated_labels,collated_entity_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC69pRJD87ly"
      },
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "class PhraseEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,output_size,device):\n",
        "        super(PhraseEmbedding, self).__init__()\n",
        "        self.print_once=True\n",
        "        self.dense_layer = nn.Linear(input_size,output_size)\n",
        "        self.non_linear_layer = nn.Tanh()\n",
        "        self.cosine_layer = nn.CosineSimilarity(dim=0)\n",
        "        self.device = device\n",
        "        return\n",
        "\n",
        "    def encode(self, input_embedding):\n",
        "\n",
        "        # print(input_embedding.size())\n",
        "        input_sentence_embedding = input_embedding.squeeze(0)\n",
        "        # print(input_sentence_embedding.size())\n",
        "        # print('-----')\n",
        "\n",
        "        # Max Pool\n",
        "        # max_pooled_embedding = torch.max(input_sentence_embedding,dim=0)\n",
        "\n",
        "        #Average Pool\n",
        "        average_pooled_embedding = torch.mean(input_sentence_embedding,dim=0).to(device=self.device)\n",
        "        # print(average_pooled_embedding.size())\n",
        "\n",
        "        x = self.dense_layer(average_pooled_embedding)\n",
        "        # print(x.size())\n",
        "\n",
        "        out = self.non_linear_layer(x)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        # print(len(input_tuple))\n",
        "        input_source = input_tuple[0]\n",
        "        input_target = input_tuple[1]\n",
        "\n",
        "        output_source = self.encode(input_source)\n",
        "        output_target = self.encode(input_target)\n",
        "\n",
        "        similarity = self.cosine_layer(output_source, output_target)\n",
        "        # print(similarity)\n",
        "        return similarity\n",
        "\n",
        "    def getEmbedding(self, input_embeddings):\n",
        "        with torch.no_grad():\n",
        "            return self.encode(input_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm1P-HYgU6kb"
      },
      "source": [
        "## **Helper Functions: Pre-processing on custom test set (NOT REQD TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgRPt985oiDa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safU-gd3oaRx"
      },
      "source": [
        "import re\n",
        "import emoji\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "import string\n",
        "\n",
        "string.punctuation=string.punctuation+'…‘’'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgl7fxwtSnHO"
      },
      "source": [
        "def get_entities(word_tag_tuples):\n",
        "    \n",
        "    mentions=[]\n",
        "    candidateMention=''\n",
        "    #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "    for tup in word_tag_tuples:\n",
        "        candidate=tup[0]\n",
        "        tag=tup[1]\n",
        "        if(tag=='O'):\n",
        "            if(candidateMention):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append(mention_to_add)\n",
        "            candidateMention=''\n",
        "        else:\n",
        "            if (tag=='B'):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append(mention_to_add)\n",
        "                candidateMention=candidate\n",
        "            else:\n",
        "                candidateMention+=\" \"+candidate\n",
        "        # if (tag=='B'):\n",
        "        #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "        #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "        #         if(mention_to_add):\n",
        "        #             mentions.append(mention_to_add)\n",
        "        #     candidateMention=candidate\n",
        "        # else:\n",
        "        #     candidateMention+=\" \"+candidate\n",
        "    if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "        if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "            mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            if(mention_to_add!=''):\n",
        "                mentions.append(mention_to_add)\n",
        "        # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "    # print('extracted mentions:', mentions)\n",
        "    return mentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YqhxNBPM67V"
      },
      "source": [
        "def get_encoding_seq(tweet_word_list, mentions):\n",
        "    print(tweet_word_list)\n",
        "    print(mentions)\n",
        "    tweet_word_index=0\n",
        "    encoded_tag_sequence=[]\n",
        "    while(mentions):\n",
        "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
        "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
        "            encoded_tag_sequence.append('O')\n",
        "            tweet_word_index+=1\n",
        "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
        "            for token_index, token in enumerate(current_mention):\n",
        "                if(token_index==0):\n",
        "                    encoded_tag_sequence.append('B')\n",
        "                else:\n",
        "                    encoded_tag_sequence.append('I')\n",
        "                tweet_word_index+=1\n",
        "    while(tweet_word_index<len(tweet_word_list)):\n",
        "        encoded_tag_sequence.append('O')\n",
        "        tweet_word_index+=1\n",
        "        \n",
        "    print(encoded_tag_sequence)\n",
        "    return encoded_tag_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4dCD4GDmgnM"
      },
      "source": [
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "def normalize_to_sentences(text):\n",
        "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "    tweetSentenceList_inter=custom_flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "    return tweetSentenceList\n",
        "\n",
        "def custom_flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "    if (mylist !=[]):\n",
        "        for item in mylist:\n",
        "            #print not isinstance(item, ne.NE_candidate)\n",
        "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                custom_flatten(item, outlist)\n",
        "            else:\n",
        "                item=item.strip(' \\t\\n\\r')\n",
        "                outlist.append(item)\n",
        "    return outlist\n",
        "\n",
        "def preprocess(filename):\n",
        "    \"\"\"save a file with token, label and prediction in each row\"\"\"\n",
        "    tweet_to_sentences_w_annotation={}\n",
        "    sentenceID=0\n",
        "    test=pd.read_csv(\"data/\"+filename,sep =',',keep_default_na=False)\n",
        "    # outputfilename=\"data/covid/covid_2K.txt\"\n",
        "    \n",
        "    all_annotated_ne=[]\n",
        "    tweetsentences=[]\n",
        "    tokenizedsentences=[]\n",
        "    \n",
        "    for row in test.itertuples():\n",
        "        tweetID=str(row.Index)\n",
        "        text=str(row.TweetText)\n",
        "        row_sentences = normalize_to_sentences(text)\n",
        "        tweetsentences += row_sentences\n",
        "        tokenizedsentences += [tokenizer(sentence, is_split_into_words=True) for sentence in row_sentences]\n",
        "        # print(text)\n",
        "        \n",
        "        mentions=[]\n",
        "        for sentence_level in str(row.mentions_other).split(';'):\n",
        "            if(sentence_level):\n",
        "                for mention in sentence_level.split(','):\n",
        "                    if(mention):\n",
        "                        mentions.append(mention.lower().strip(string.punctuation).strip())\n",
        "        mentions=list(filter(lambda element: ((element !='')&(element !='nan')), mentions))\n",
        "        # all_annotated_ne.extend(mentions)\n",
        "        \n",
        "        # if(row_sentences):\n",
        "        tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+len(row_sentences)),mentions)\n",
        "        sentenceID+=len(row_sentences)\n",
        "        # else:\n",
        "        #     tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+1),mentions)\n",
        "        #     sentenceID+=1\n",
        "        # print(sentenceID,len(row_sentences))\n",
        "    return tweetsentences, tokenizedsentences, tweet_to_sentences_w_annotation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89dhqxmg5m25"
      },
      "source": [
        "## **Helper Functions: Evaluation on custom test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep9TRivf7R9-"
      },
      "source": [
        "def calculate_f1(tweet_to_sentences_w_annotation, ner_arrays):\n",
        "    \n",
        "    # dataset, i = [], 0\n",
        "    file_write_text=''\n",
        "    all_detected_ne=[]\n",
        "    all_annotated_ne=[]\n",
        "    \n",
        "    true_positive_count=0\n",
        "    false_positive_count=0\n",
        "    false_negative_count=0\n",
        "    total_mentions=0\n",
        "    total_annotation=0\n",
        "    entity_freq_dict={}\n",
        "    \n",
        "    for tweetID in tweet_to_sentences_w_annotation.keys():\n",
        "        unrecovered_annotated_mention_list=[]\n",
        "        tp_counter_inner=0\n",
        "        fp_counter_inner=0\n",
        "        fn_counter_inner=0\n",
        "        \n",
        "        annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
        "        all_annotated_ne.extend(annotated_mention_list)\n",
        "        output_mentions_list=[]\n",
        "        idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
        "        for sentID in range(idRange[0],idRange[1]):\n",
        "            output_mentions_list+=ner_arrays[sentID]\n",
        "        all_detected_ne+=output_mentions_list\n",
        "        print(tweetID,annotated_mention_list,output_mentions_list)\n",
        "        all_postitive_counter_inner=len(output_mentions_list)\n",
        "        for elem in annotated_mention_list:\n",
        "            try:\n",
        "                entity_freq_dict[elem]+=1\n",
        "            except KeyError:\n",
        "                entity_freq_dict[elem]=1\n",
        "\n",
        "        while(annotated_mention_list):\n",
        "            if(len(output_mentions_list)):\n",
        "                annotated_candidate= annotated_mention_list.pop()\n",
        "                if(annotated_candidate in output_mentions_list):\n",
        "                    output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                    tp_counter_inner+=1\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "            else:\n",
        "                unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                break\n",
        "        # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "        fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "        fp_counter_inner=all_postitive_counter_inner - tp_counter_inner\n",
        "        \n",
        "        print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "        \n",
        "        true_positive_count+=tp_counter_inner\n",
        "        false_positive_count+=fp_counter_inner\n",
        "        false_negative_count+=fn_counter_inner\n",
        "        \n",
        "    print('true_positive_count,false_positive_count,false_negative_count:')\n",
        "    print(true_positive_count,false_positive_count,false_negative_count)\n",
        "    \n",
        "    precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "    recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "    f_measure=2*(precision*recall)/(precision+recall)\n",
        "            \n",
        "    print('========Entity Mention Detection========')\n",
        "    print('precision: ',precision)\n",
        "    print('recall: ',recall)\n",
        "    print('f_measure: ',f_measure)\n",
        "\n",
        "    print('========Entity Detection========')\n",
        "    true_positive_entities =  len(list(set(all_detected_ne).intersection(set(all_annotated_ne))))\n",
        "    false_negatives = list(set(all_annotated_ne)-set(all_detected_ne))\n",
        "    false_negative_entities = len(list(set(all_annotated_ne)-set(all_detected_ne)))\n",
        "    false_positive_entities = len(list(set(all_detected_ne)-set(all_annotated_ne)))\n",
        "\n",
        "    precision= (true_positive_entities)/(true_positive_entities+false_positive_entities)\n",
        "    recall= (true_positive_entities)/(true_positive_entities+false_negative_entities)\n",
        "    f_measure = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "    print('totally missed entities:',len(false_negatives))\n",
        "    print('totally missed mentions:',sum([entity_freq_dict[elem] for elem in false_negatives]))\n",
        "    print('total annotated entities:',(true_positive_entities+false_negative_entities))\n",
        "\n",
        "    print('precision: ',precision)\n",
        "    print('recall: ',recall)\n",
        "    print('f_measure: ',f_measure)   \n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN17vi2QJh9Q"
      },
      "source": [
        "## **Initialization of some components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5rokKC-JgOk",
        "outputId": "4eff9f64-5bb9-4b3b-d738-ff170b80c0b2"
      },
      "source": [
        "from os import path\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "# Initialize network\n",
        "output_embedding_size = 300\n",
        "# output_embedding_size = 768\n",
        "phraseEmbeddingModel = PhraseEmbedding(768, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# define checkpoint saved path for entity phrase embedder\n",
        "# ckp_path = \"entityEmbedding/model_checkpoints/checkpoint.pt\" #768\n",
        "ckp_path = \"entityEmbedding/model_checkpoints/checkpoint_model300.pt\" #300\n",
        "\n",
        "if(path.exists(ckp_path)):\n",
        "    # load the saved checkpoint\n",
        "    entityPhraseEmbedder, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "\n",
        "    print(\"starting with model at epoch:\", start_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting with model at epoch: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUMEUhlS2N47",
        "outputId": "13ad2ac5-a617-49a3-dd65-8c86513292e2"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhlviRvO7DDE"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s.a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbajzPkDC_nO"
      },
      "source": [
        "# tweets_unpartitoned=pd.read_csv('data/wnut17test.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_6k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/covid_2K.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/billnye.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# #to train the Entity Classifiers\n",
        "# # tweets_unpartitoned=pd.read_csv('data/deduplicated_test.csv',sep =';',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/large_training_file.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('tweets_3k_annotated.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('venezuela.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billdeblasio.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('pikapika.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('ripcity.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billnye.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('roevwade.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('wnut17test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOGT-F39-YBx"
      },
      "source": [
        "## **Entity Classifier I**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-F1fd56KK7N"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,12)\n",
        "    self.linear2 = nn.Linear(12,input_size)\n",
        "    self.linear3 = nn.Linear(input_size,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifierI():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # separately using only syntactic features\n",
        "        self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']\n",
        "\n",
        "        self.relevant_columns = ['normalized_length','normalized_cap',\n",
        "            'normalized_substring-cap',\n",
        "            'normalized_s-o-sCap',\n",
        "            'normalized_all-cap',\n",
        "            'normalized_non-cap',\n",
        "            'normalized_non-discriminative']\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 200\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/checkpoint.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifierI_checkpoint_model300.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/checkpoint.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierI_checkpoint_model.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                # if(combined_validation_loss<best_loss):\n",
        "                if(f1>best_f1):\n",
        "                    # best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIpoGN3z2Wd0"
      },
      "source": [
        "## **Entity Classifier Alt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1LMGb6Lx7F5"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdox1Nclx3Pu"
      },
      "source": [
        "class CandidateDataset(Dataset):\n",
        "    def __init__(self, df, candidateEmbeddingDict, candidateBase_dict_alt):\n",
        "        self.samples= []\n",
        "        self.max_freq=-1\n",
        "        self.freq_arr = []\n",
        "        # self.embeddings_repo = []\n",
        "        self.output = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            candidate = row['candidate']\n",
        "            normalized_length = row['normalized_length']\n",
        "            cumulative= candidateBase_dict_alt[candidate][-1]\n",
        "            if(cumulative>self.max_freq):\n",
        "                self.max_freq=cumulative\n",
        "\n",
        "            output_val= row['class']\n",
        "\n",
        "            local_embedding_list= candidateEmbeddingDict[candidate]\n",
        "\n",
        "            tup=(candidate,normalized_length,cumulative)\n",
        "            # print(tup,output_val)\n",
        "\n",
        "            # self.len_arr.append(normalized_length)\n",
        "            self.freq_arr.append(cumulative)\n",
        "            # self.embeddings_repo.append(local_embedding_list)\n",
        "            self.samples.append(tup)\n",
        "            self.output.append(output_val)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tup2=(self.freq_arr[idx]/self.max_freq,)\n",
        "        tup=self.samples[idx]+tup2\n",
        "\n",
        "        return tup,self.output[idx]\n",
        "        # return self.len_arr[idx],self.cum_freq_arr[idx],self.output[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgbUgaLe2eXQ"
      },
      "source": [
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self,input_size,device):\n",
        "        super(NN, self).__init__()\n",
        "        self.device=device\n",
        "        self.input_size = input_size\n",
        "\n",
        "        #pooling functions\n",
        "        self.pooling_layer1 = nn.Linear(input_size,50)\n",
        "        self.pooling_layer2 = nn.Linear(50,1)\n",
        "\n",
        "        # self.aggregated_input = torch.zeros(input_size).to(self.device)\n",
        "        self.linear1 = nn.Linear(input_size+1,500)\n",
        "        self.linear2 = nn.Linear(500,200)\n",
        "        self.linear3 = nn.Linear(200,1)\n",
        "        self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "    def compute(self, x_tup):\n",
        "\n",
        "        x_len=x_tup[0]\n",
        "        x_List=x_tup[1]\n",
        "        x_cum_freq=x_tup[2]\n",
        "        x_normalized_freq=x_tup[3]\n",
        "\n",
        "        # print(type(x_tup))\n",
        "        # print(len(x_tup))\n",
        "        # print(x_len)\n",
        "        # print(len(x_List),len(x_List[0]))\n",
        "        # print(x_cum_freq)\n",
        "        \n",
        "        # x_tensor = torch.FloatTensor(np.array([x_len,x_normalized_freq],dtype=float)).to(self.device)\n",
        "        x_tensor = torch.FloatTensor(np.array([x_len],dtype=float)).to(self.device)\n",
        "\n",
        "        \n",
        "        local_embedding_list=[]\n",
        "        for x_elem in x_List:\n",
        "            x_elem_tensor = torch.FloatTensor(x_elem).to(self.device)\n",
        "            local_embedding_list.append(x_elem_tensor)\n",
        "        # print(len(local_embedding_list))\n",
        "        all_local_embedding = torch.stack(local_embedding_list).to(self.device)\n",
        "        pooling_output_1=F.relu(self.pooling_layer1(all_local_embedding))\n",
        "        pooling_output_2=self.pooling_layer2(pooling_output_1)\n",
        "        weights = pooling_output_2.reshape(-1).tolist()\n",
        "        # print(all_local_embedding.size(),len(weights))\n",
        "\n",
        "        aggregated_input = torch.zeros(self.input_size, requires_grad=True).to(self.device)\n",
        "        for ind, local_embedding in enumerate(all_local_embedding):\n",
        "            aggregated_input=aggregated_input.add(torch.mul(local_embedding, weights[ind]))\n",
        "        \n",
        "        aggregated_input=torch.mul(aggregated_input, float(1/x_cum_freq))\n",
        "        x = torch.cat((x_tensor,aggregated_input), 0).to(self.device)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        global_embeddings_arr=[]\n",
        "        for x_tup in batch_data:\n",
        "            global_embeddings_arr.append(self.compute(x_tup))\n",
        "        # global_embeddings_arr = np.array(global_embeddings_arr)\n",
        "        global_embeddings_batch = torch.stack(global_embeddings_arr).to(self.device)\n",
        "        # global_embeddings_batch.requires_grad = True\n",
        "        # print(global_embeddings_batch.size())\n",
        "        x = F.relu(self.linear1(global_embeddings_batch))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        out = self.sigmoid_layer(x)\n",
        "        return out\n",
        "\n",
        "class EntityClassifierAlt():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device, candidateEmbeddingDict, candidateBase_dict_alt):\n",
        "\n",
        "        # separately using only semantic features\n",
        "        # self.combined_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        # self.combined_feature_list=['length']+['cf_'+str(i) for i in range(300)]\n",
        "        self.max_candidate_length=7.0 #6+1, dont want this to be 1 in any point to go with other column normalizations\n",
        "        self.candidateEmbeddingDict=candidateEmbeddingDict\n",
        "        self.candidateBase_dict_alt_train=candidateBase_dict_alt\n",
        "        self.combined_feature_list=['length']\n",
        "\n",
        "        self.device=device\n",
        "\n",
        "        self.input_size = len(['normalized_cf_'+str(i) for i in range(300)])\n",
        "\n",
        "        # self.relevant_columns = ['candidate','normalized_length','cumulative']\n",
        "        self.relevant_columns = ['candidate','normalized_length','cumulative','class']\n",
        "        # self.relevant_columns = ['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(self.input_size,device).to(device)\n",
        "        #Loss and Optimizer\n",
        "        # self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_criterion = nn.BCELoss(reduction='sum' )\n",
        "        # self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.001, weight_decay=0.00001)\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.0015, weight_decay=1e-8)\n",
        "        self.ec_batch_size = 128\n",
        "        self.ec_num_epochs = 1000\n",
        "        # self.patience = 20\n",
        "        self.patience = 5\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            train_df = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "\n",
        "            self.train = train_df[train_df['candidate'].isin(list(self.candidateEmbeddingDict.keys()))]\n",
        "\n",
        "            #pre-processing : this completes the average pooling\n",
        "            \n",
        "            # max_length=self.train['length'].max()\n",
        "            # print(self.train['length'].tolist())\n",
        "            self.train['normalized_length']= self.train['length']/self.max_candidate_length\n",
        "\n",
        "            print(self.train['normalized_length'].max())\n",
        "\n",
        "            # for column in self.combined_feature_list[1:]:\n",
        "            #     self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            # candidate_array = self.train['candidate'].tolist()\n",
        "            dataset_df = self.train[self.relevant_columns]\n",
        "\n",
        "            dataset = CandidateDataset(dataset_df, self.candidateEmbeddingDict,candidateBase_dict_alt)\n",
        "\n",
        "            print(dataset.__getitem__(0))\n",
        "\n",
        "            train_len=int(math.ceil(len(dataset_df)*0.8))\n",
        "            val_len=len(dataset_df)-train_len\n",
        "\n",
        "            print('train_len',train_len)\n",
        "            print('val_len',val_len)\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, shuffle=True) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifierAlt/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint768.pt\" #768\n",
        "            ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300.pt\" #300\n",
        "            # ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300_emd_loss.pt\" #300\n",
        "            # ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300_ed.pt\" #300\n",
        "            # ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300_emd_f1.pt\" #300\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300_3K.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/classifierII_checkpoint768.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierAlt_checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def custom_loss(self, out, targets, freq):\n",
        "        vec=(torch.abs((out - targets))*freq)\n",
        "        # print('vec shape',vec.size())\n",
        "        loss = torch.sum(vec)/torch.sum(freq)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data_inter, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                # data = data.to(device=device)\n",
        "\n",
        "                # print(len(data_inter),len(targets),type(data_inter))\n",
        "                # print(len(data_inter[0]),len(data_inter[1]),len(data_inter[2]))\n",
        "\n",
        "                # print(data_inter[0][:2])\n",
        "                # print(data_inter[1][:2])\n",
        "                # print(data_inter[2][:2])\n",
        "                # print(targets[:2])\n",
        "\n",
        "                candidates = data_inter[0]\n",
        "                length_arr = data_inter[1]\n",
        "                cumulative_arr = data_inter[2]\n",
        "                normalized_freq_arr = data_inter[3]\n",
        "\n",
        "                data = [(length_arr[index], self.candidateEmbeddingDict[candidate], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                targets = targets.unsqueeze(1).to(device=self.device)\n",
        "\n",
        "                self.ec_optimizer.zero_grad()\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                out = out.to(torch.float32)\n",
        "                targets = targets.to(torch.float32)\n",
        "\n",
        "                # loss = self.ec_criterion(out, targets) \n",
        "                loss = self.custom_loss(out, targets, torch.FloatTensor(np.array(cumulative_arr)).unsqueeze(1).to(device=self.device)) #custom l1 loss factoring in frequency\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(out.requires_grad)\n",
        "                # print(targets.shape)\n",
        "                # print(loss.requires_grad)\n",
        "\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            print('combined_training_loss:',combined_training_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            all_candidates = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data_inter, val_targets) in enumerate(self.val_loader):\n",
        "                    # val_data = val_data.to(device=device)\n",
        "                    candidates = val_data_inter[0]\n",
        "                    length_arr = val_data_inter[1]\n",
        "                    cumulative_arr = val_data_inter[2]\n",
        "                    normalized_freq_arr = val_data_inter[3]\n",
        "                    # print(candidates[:2])\n",
        "                    # print(length_arr[:2])\n",
        "                    # print(cumulative_arr[:2])\n",
        "\n",
        "                    val_data = [(length_arr[index], self.candidateEmbeddingDict[candidate], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    out = out.to(torch.float32)\n",
        "                    val_targets = val_targets.to(torch.float32)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "                    all_candidates+=candidates\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    # loss = self.ec_criterion(out, val_targets)\n",
        "                    loss = self.custom_loss(out, val_targets, torch.FloatTensor(np.array(cumulative_arr)).unsqueeze(1).to(device=self.device)) #custom l1 loss factoring in frequency\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "\n",
        "                # #ED training objective\n",
        "                # tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                # fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                # fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                #EMD training objective\n",
        "\n",
        "                tp = sum([self.candidateBase_dict_alt_train[all_candidates[idx]][-1] for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = sum([self.candidateBase_dict_alt_train[all_candidates[idx]][-1] for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = sum([self.candidateBase_dict_alt_train[all_candidates[idx]][-1] for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                if((tp+fp)==0):\n",
        "                    precision = 0\n",
        "                else:\n",
        "                    precision = tp/(tp+fp)\n",
        "\n",
        "                if((tp+fn)==0):\n",
        "                    recall = 0\n",
        "                else:\n",
        "                    recall = tp/(tp+fn)\n",
        "\n",
        "                if((precision + recall)==0):\n",
        "                    f1 = 0\n",
        "                else:\n",
        "                    f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                if(combined_validation_loss<best_loss):\n",
        "                # if(f1>best_f1):\n",
        "                    best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase,candidateBase_dict_alt):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        candidateBase['class']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/self.max_candidate_length\n",
        "\n",
        "        # for column in self.combined_feature_list[1:]:\n",
        "        #     candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        # test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        # test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        # # test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        # # test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        # test_inputs = torch.from_numpy(test_inputs_array)\n",
        "        # test_targets = torch.from_numpy(test_targets_array)\n",
        "\n",
        "        # test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        # test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        test_dataset = CandidateDataset(candidateBase, self.candidateEmbeddingDict, candidateBase_dict_alt)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data_inter, targets) in enumerate(test_loader):\n",
        "                # data = data.to(device=device)\n",
        "                candidates = data_inter[0]\n",
        "                length_arr = data_inter[1]\n",
        "                cumulative_arr = data_inter[2]\n",
        "                normalized_freq_arr = data_inter[3]\n",
        "\n",
        "                data = [(length_arr[index], self.candidateEmbeddingDict[candidate], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                # print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhUJiCRR-iFG"
      },
      "source": [
        "## **Entity Classifier II**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX66sYfCKMHT"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,500)\n",
        "    self.linear2 = nn.Linear(500,200)\n",
        "    self.linear3 = nn.Linear(200,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifierII():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # separately using only semantic features\n",
        "        # self.combined_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        self.combined_feature_list=['length']+['cf_'+str(i) for i in range(300)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length']+['normalized_cf_'+str(i) for i in range(300)]\n",
        "        # self.relevant_columns = ['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.00001, weight_decay=0.00001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 1000\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint768.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300.pt\" #300\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300_3K.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/classifierII_checkpoint768.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierII_checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                if((tp+fp)==0):\n",
        "                    precision = 0\n",
        "                else:\n",
        "                    precision = tp/(tp+fp)\n",
        "\n",
        "                if((tp+fn)==0):\n",
        "                    recall = 0\n",
        "                else:\n",
        "                    recall = tp/(tp+fn)\n",
        "\n",
        "                if((precision + recall)==0):\n",
        "                    f1 = 0\n",
        "                else:\n",
        "                    f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                if(combined_validation_loss<best_loss):\n",
        "                # if(f1>best_f1):\n",
        "                    best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4NIDPJU5NOQ"
      },
      "source": [
        "## **Entity Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3oTVBI5PWL"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,500)\n",
        "    self.linear2 = nn.Linear(500,200)\n",
        "    self.linear3 = nn.Linear(200,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifier():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(768)]\n",
        "        self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(300)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length',\n",
        "            'normalized_cap',\n",
        "            'normalized_substring-cap',\n",
        "            'normalized_s-o-sCap',\n",
        "            'normalized_all-cap',\n",
        "            'normalized_non-cap',\n",
        "            'normalized_non-discriminative'\n",
        "            ]\n",
        "            +['normalized_cf_'+str(i) for i in range(300)]\n",
        "            # +['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.00001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 200\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/checkpoint.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/checkpoint_model300.pt\" #300\n",
        "            \n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/checkpoint.pt' #768\n",
        "        f_path = checkpoint_dir + '/checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                # if(combined_validation_loss<best_loss):\n",
        "                if(f1>best_f1):\n",
        "                    # best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0eE9I3zuArg"
      },
      "source": [
        "## **Phase I: Local NER to collect entity candidates and Token Contextual Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf9KEE5H5xAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e0505e-b985-4174-ea6d-37ea222a784e"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import emoji\n",
        "from emoji import demojize\n",
        "import string\n",
        "\n",
        "# import numpy as np\n",
        "import pandas  as pd\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "#from datasketch import MinHash, MinHashLSH\n",
        "# import NE_candidate_module as ne\n",
        "# import Mention\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "import trie as trie\n",
        "import ast\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "# import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "# import matplotlib.pyplot as plt\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"other\",\"since\",\"hence\",\"onto\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"up\")\n",
        "\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"v.\"]  #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"httpurl\",\"n't\",\"pls\",\"bye\",\"€\",\"vs\",\"ouch\",\"am\",\"pm\",\"omw\",\"http\",\"https\",\"tv\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"ip\",\"qft\",\"ima\",\"icymi\",\"bdsm\",\"ah\",\"ive\",\"qt\",\"dj\",\"dm\",\"pts\",\"pt\",\"yrs\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "class LocalNERModule():\n",
        "    def __init__(self, sentenceTokenizer, nerTokenizer, nerEngine, device):\n",
        "        self.counter=0\n",
        "\n",
        "        if(sentenceTokenizer):\n",
        "            self.my_sentence_tokenizer = sentenceTokenizer\n",
        "        else:\n",
        "            nltk.download('gutenberg')\n",
        "            gutenberg_text = \"\"\n",
        "            for file_id in gutenberg.fileids():\n",
        "                gutenberg_text += gutenberg.raw(file_id)\n",
        "            tokenizer_trainer = PunktTrainer()\n",
        "            tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "            tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "            self.my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "        self.quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        self.tweet_to_sentences_w_annotation = {}\n",
        "        self.device = device\n",
        "        self.apostrophe_list =[\"'s\",'’s','s']\n",
        "        self.label_list = ['O','B','I']\n",
        "\n",
        "        self.tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "        self.contextual_embeddings = {}\n",
        "\n",
        "        print('Starting Local NER Engine!')\n",
        "        self.expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location', 8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "        self.BIO_dict={'O':0,'B':1,'I':2}\n",
        "        if((nerTokenizer is not None)&(nerEngine is not None)):\n",
        "            self.nerTokenizer = nerTokenizer\n",
        "            self.localNEREngine = nerEngine\n",
        "        else:\n",
        "            self.train_engine()\n",
        "\n",
        "    def train_engine(self):\n",
        "        task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "        model_checkpoint = \"vinai/bertweet-base\"\n",
        "        batch_size = 16\n",
        "        # set_seed(42)\n",
        "        datasets = load_dataset(\"wnut_17\")\n",
        "        self.nerTokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "        label_all_tokens = True\n",
        "        tokenized_datasets = datasets.map(self.tokenize_and_align_labels)\n",
        "        data_collator = DataCollatorForTokenClassification(self.nerTokenizer)\n",
        "        self.metric = load_metric(\"seqeval\")\n",
        "        self.localNEREngine = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(self.label_list))\n",
        "        alt_training_args = TrainingArguments(\n",
        "            f\"test-{task}\",\n",
        "            evaluation_strategy = \"epoch\",\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "        alt_trainer = Trainer(\n",
        "        self.localNEREngine,\n",
        "        alt_training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=self.nerTokenizer,\n",
        "        compute_metrics=self.compute_metrics\n",
        "        )\n",
        "        alt_trainer.train()\n",
        "\n",
        "        # tokenizer.save_pretrained('test-ner/')\n",
        "        # alt_model.save_pretrained('test-ner/')\n",
        "\n",
        "    def tokenize_and_align_labels(self,example):\n",
        "        \n",
        "        tokenized_ds_input = self.nerTokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "        inputId_to_token_dict={}\n",
        "        for index, token in enumerate(example[\"tokens\"]):\n",
        "            values=self.nerTokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "            for value in values:\n",
        "                try:\n",
        "                    inputId_to_token_dict[value].append(index)\n",
        "                except KeyError:\n",
        "                    inputId_to_token_dict[value]=[index]\n",
        "        labels=[]\n",
        "        for inputID in tokenized_ds_input['input_ids']:\n",
        "            try:\n",
        "                index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "                index_to_address=index_list.pop(0)\n",
        "\n",
        "                label=self.BIO_dict[self.expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "                # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "                labels.append(label)\n",
        "                inputId_to_token_dict[inputID]=index_list\n",
        "            except KeyError:\n",
        "                labels.append(-100)\n",
        "\n",
        "        assert (len(tokenized_ds_input['input_ids']) == len(labels))\n",
        "        tokenized_ds_input['labels']=labels\n",
        "        \n",
        "        return tokenized_ds_input\n",
        "\n",
        "    def compute_metrics(self, p):\n",
        "        # print(p.shape)\n",
        "        output, labels = p\n",
        "\n",
        "        # print(len(predictions))\n",
        "        # print(predictions[0].shape)\n",
        "        # for elem in predictions[1]:\n",
        "        #   print(elem.shape)\n",
        "\n",
        "        predictions, _ = output\n",
        "        \n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # Remove ignored index (special tokens)\n",
        "        true_predictions = [\n",
        "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }\n",
        "\n",
        "    def is_float(self,string):\n",
        "        try:\n",
        "            f=float(string)\n",
        "            if(f==0.0):\n",
        "              return True\n",
        "            else:\n",
        "              return ((f) and (string.count(\".\")==1))\n",
        "      #return True# True if string is a number with a dot\n",
        "        except ValueError:  # if string is not a number\n",
        "          return False\n",
        "\n",
        "    def collate_token_label_embedding(self, tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "        counter=0\n",
        "        collated_labels=[]\n",
        "        collated_entity_embeddings=[]\n",
        "        for word in tweetWordList:\n",
        "            vals=token_dict[word]\n",
        "            # print(word,vals)\n",
        "            if(counter<len(prediction_labels)):\n",
        "                labels=prediction_labels[counter:counter+len(vals)]\n",
        "                token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "        #         print(token_entity_embeddings.shape)\n",
        "                mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "                mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "                collated_entity_embeddings.append(mean_tensor)\n",
        "        #         print(collated_entity_embeddings)\n",
        "                if('I' in labels):\n",
        "                    collated_labels.append('I')\n",
        "                elif('B' in labels):\n",
        "                    collated_labels.append('B')\n",
        "                else:\n",
        "                    collated_labels.append('O')\n",
        "                counter+=len(vals)\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "                collated_entity_embeddings.append(torch.zeros(768).to(self.device))\n",
        "        assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "        return collated_labels,collated_entity_embeddings\n",
        "\n",
        "\n",
        "    def get_entities(self, word_tag_tuples):\n",
        "        mentions=[]\n",
        "        candidateMention=''\n",
        "        positions=[]\n",
        "        \n",
        "        #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "        for index, tup in enumerate(word_tag_tuples):\n",
        "            candidate=tup[0]\n",
        "            tag=tup[1]\n",
        "            if(tag=='O'):\n",
        "                if(candidateMention):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,positions)\n",
        "                                return\n",
        "                candidateMention=''\n",
        "                positions=[]\n",
        "            else:\n",
        "                if (tag=='B'):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,positions)\n",
        "                                return\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention=candidate\n",
        "                        positions=[index]\n",
        "                else:\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention+=\" \"+candidate\n",
        "                        positions.append(index)\n",
        "            # if (tag=='B'):\n",
        "            #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "            #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            #         if(mention_to_add):\n",
        "            #             mentions.append(mention_to_add)\n",
        "            #     candidateMention=candidate\n",
        "            # else:\n",
        "            #     candidateMention+=\" \"+candidate\n",
        "        if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "            if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                if(mention_to_add!=''):\n",
        "                    try:\n",
        "                        assert len(mention_to_add.split()) == len(positions)\n",
        "                        mentions.append((mention_to_add,positions))\n",
        "                    except AssertionError:\n",
        "                        print(word_tag_tuples)\n",
        "                        print(mention_to_add,positions)\n",
        "                        return\n",
        "            # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "        # print('extracted mentions:', mentions)\n",
        "        return mentions\n",
        "\n",
        "    def normalize_to_sentences(self, text):\n",
        "        tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "        tweetSentenceList_inter=self.custom_flatten(list(map(lambda sentText: self.my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "        tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "        return tweetSentenceList\n",
        "\n",
        "    def custom_flatten(self, mylist, outlist, ignore_types=(str, bytes, int)):\n",
        "        \n",
        "        if (mylist !=[]):\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.custom_flatten(item, outlist)\n",
        "                else:\n",
        "                    item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "\n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "    #         elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "    #             temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "    #             if(temp1):\n",
        "    #                 temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "    #             temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.custom_flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "\n",
        "    def remAmpersand(self,candidateStr):\n",
        "        candidateStr=candidateStr.replace('&amp;','')\n",
        "        return candidateStr\n",
        "\n",
        "    def normalizeToken(self,token):\n",
        "        lowercased_token = token.lower()\n",
        "        if token.startswith(\"@\"):\n",
        "            return \"@USER\"\n",
        "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "            return \"HTTPURL\"\n",
        "        elif len(token) == 1:\n",
        "            return demojize(token)\n",
        "        else:\n",
        "            if token == \"’\":\n",
        "                return \"'\"\n",
        "            elif token == \"…\":\n",
        "                return \"...\"\n",
        "            else:\n",
        "                return token\n",
        "\n",
        "    def normalizeTweet(self, tweet):\n",
        "        tokens = self.tweetTokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "        normTweet = \" \".join([self.normalizeToken(token) for token in tokens])\n",
        "\n",
        "        normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "        normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "        normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
        "\n",
        "        normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "        \n",
        "        return normTweet\n",
        "\n",
        "    #removing commonly used expletives, enunciated chat words and other common words (like days of the week, common expressions)\n",
        "    def slang_remove(self,ne_phrase):\n",
        "        phrase=ne_phrase.strip().strip(string.punctuation).lower()\n",
        "        p1= re.compile(r'([A-Za-z]+)\\1\\1{1,}')\n",
        "        match_lst = p1.findall(phrase)\n",
        "        if phrase in article_list:\n",
        "            return True\n",
        "        elif phrase in day_list:\n",
        "            return True\n",
        "        elif phrase in month_list:\n",
        "            return True\n",
        "        elif match_lst:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def all_slang(self, ne_phrase):\n",
        "        ne_words=ne_phrase.split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list\n",
        "        is_invalid=0\n",
        "\n",
        "        for word in ne_words:\n",
        "            if(word.strip().strip(string.punctuation).lower() in combined):\n",
        "                is_invalid+=1\n",
        "        if(is_invalid==len(ne_words)):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def start_end_token_check(self, ne_candidate):\n",
        "        positions = ne_candidate[1]\n",
        "        ne_words=ne_candidate[0].split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list+prep_list+article_list\n",
        "        # print(combined)\n",
        "        # print(ne_words)\n",
        "        start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "        end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        while((len(ne_words)>1)&((start_word in combined)|(end_word in combined))):\n",
        "            if(start_word in combined):\n",
        "                ne_words.pop(0)\n",
        "                positions.pop(0)\n",
        "            if(end_word in combined):\n",
        "                ne_words.pop()\n",
        "                positions.pop()\n",
        "            if(len(ne_words)>1):\n",
        "                start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "                end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        ne_phrase = (' '.join(ne_words)).strip()\n",
        "        # print(ne_phrase)\n",
        "        return (ne_phrase,positions)\n",
        "\n",
        "    def set_stopword_exceptions(self,words):\n",
        "        combined=cachedStopWords+prep_list+article_list+day_list\n",
        "        for word in words:\n",
        "            if word in combined:\n",
        "                self.swSet.add(word)\n",
        "\n",
        "    def extract(self, batch, batch_number):\n",
        "        print(\"Running Local NER now\")\n",
        "        time_in=time.time()\n",
        "        self.batch=batch\n",
        "        self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'TweetSentence','tweetwordList', 'phase1Candidates','start_time','entry_batch'))\n",
        "        df_holder=[]\n",
        "        self.swSet= set()\n",
        "        annotations_available=False\n",
        "\n",
        "        if('mentions_other' in self.batch.columns.tolist()):\n",
        "            annotations_available=True\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates','correct_candidates_tweet'))\n",
        "            #dict1 = {'tweetID':0, 'sentID':0, 'hashtags':'first', 'user':'user', 'TweetSentence':'sentence', 'phase1Candidates':'phase1Out','start_time':'now','entry_batch':'batch_number'}\n",
        "            self.CTrie=trie.Trie(\"ROOT\")\n",
        "            self.phase2stopWordList=[]\n",
        "            self.sentenceID = 0\n",
        "            self.f=0\n",
        "\n",
        "        for row in self.batch.itertuples():\n",
        "\n",
        "            now = datetime.datetime.now()\n",
        "            tweetID=str(row.Index)\n",
        "            text=str(row.TweetText)\n",
        "            row_sentences = self.normalize_to_sentences(text)\n",
        "\n",
        "            annnotated_mentions=[]\n",
        "\n",
        "            if(annotations_available):\n",
        "                for sentence_level in str(row.mentions_other).split(';'):\n",
        "                    if(sentence_level):\n",
        "                        for mention in sentence_level.split(','):\n",
        "                            if(mention):\n",
        "                                annnotated_mentions.append(mention.lower().strip(string.punctuation).strip())\n",
        "                annnotated_mentions=list(filter(lambda element: ((element !='')&(element !='nan')), annnotated_mentions))\n",
        "\n",
        "            self.tweet_to_sentences_w_annotation[tweetID]=((self.sentenceID,self.sentenceID+len(row_sentences)),annnotated_mentions)\n",
        "            self.sentenceID+=len(row_sentences)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for sen_index, sentence in enumerate(row_sentences):\n",
        "\n",
        "                    # print('tuple index:',tweetID,sen_index)\n",
        "                    phase1Out=\"\"\n",
        "                    sentence = self.normalizeTweet(sentence)\n",
        "                    # tweetWordList=self.getWords(sentence)\n",
        "                    tweetWordList = sentence.split()\n",
        "                    enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "                    entities_from_sentence=[]\n",
        "                    entity_aware_embeddings=[]\n",
        "\n",
        "                    if(len(tweetWordList)>0):\n",
        "\n",
        "                        # print(test_record)\n",
        "                        # tokenized_input=tokenizer(test_record)\n",
        "                        # initial_input_ids = torch.tensor([tokenizer.encode(test_record)])\n",
        "                        # token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in test_record.split()}\n",
        "                        # input_ids = initial_input_ids.to(device)\n",
        "                        # tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        tokenized_input= self.nerTokenizer(sentence)\n",
        "                        initial_input_ids = torch.tensor([self.nerTokenizer.encode(sentence)])\n",
        "                        # num_tokens = initial_input_ids.shape[1]\n",
        "\n",
        "                        initial_input_ids = initial_input_ids[:,:128]\n",
        "                        token_dict = {x : self.nerTokenizer.encode(x, add_special_tokens=False) for x in sentence.split()} #token, add_special_tokens=False, truncation=True\n",
        "                        input_ids = initial_input_ids.to(self.device)\n",
        "                        tokens = self.nerTokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        output = self.localNEREngine(input_ids)\n",
        "                        token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "                        prediction = (torch.argmax(output.logits, axis=2))\n",
        "                        prediction = prediction.cpu().numpy().reshape(-1)\n",
        "\n",
        "                        # prediction_labels=[self.label_list[l].split('-')[0] for l in prediction]\n",
        "                        prediction_labels=[self.label_list[l] for l in prediction] #Just BIO\n",
        "\n",
        "                        prediction_labels, entity_aware_embeddings=self.collate_token_label_embedding(tweetWordList, token_dict, prediction_labels[1:-1],token_embeddings)\n",
        "\n",
        "                        assert len(enumerated_tweetWordList)==len(entity_aware_embeddings)\n",
        "                        assert len(prediction_labels)==len(enumerated_tweetWordList)\n",
        "\n",
        "                        word_tag_tuples=list(zip(token_dict.keys(),prediction_labels))\n",
        "                        # print(list(word_tag_tuples))\n",
        "                        entities_from_sentence=self.get_entities(word_tag_tuples)\n",
        "                        # print('entities_from_sentence:',entities_from_sentence)\n",
        "\n",
        "                        if(self.f<5):\n",
        "                            print(len(tweetWordList),initial_input_ids.shape,len(prediction[1:-1]))\n",
        "                            print(tweetWordList)\n",
        "                            print(token_dict)\n",
        "                            print(initial_input_ids)\n",
        "                            print(input_ids)\n",
        "                            print(prediction)\n",
        "                            print('entities_from_sentence:',entities_from_sentence)\n",
        "                            print('======')\n",
        "                            self.f+=1\n",
        "\n",
        "                    just_candidates=[]\n",
        "\n",
        "                    # place some necessary filters\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.slang_remove(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(map(lambda element: self.start_end_token_check(element), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.all_slang(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: len(element[0])>1, entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: len(element[0].split())<=6, entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: element[0]!='', entities_from_sentence))\n",
        "\n",
        "                    \n",
        "\n",
        "                    for candidateTuple in entities_from_sentence:\n",
        "                        #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
        "                        candidateText, positions = candidateTuple\n",
        "                        candidateText=(((candidateText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
        "                        candidateText=(self.remAmpersand(candidateText).lstrip('“‘’”')).rstrip('“‘’”')\n",
        "                        candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "                        candidateText= candidateText.strip()\n",
        "                        self.set_stopword_exceptions(candidateText.split())\n",
        "                        just_candidates.append(candidateText)\n",
        "                        # if(index==9423):\n",
        "                        #     print(candidateText)\n",
        "                        position = '*'+'*'.join(str(v) for v in positions)\n",
        "                        position=position+'*'\n",
        "\n",
        "                        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
        "\n",
        "                        combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
        "                        if not ((candidateText in combined)|(len(candidateText)<=1)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
        "                            if(self.quickRegex.match(candidateText)):\n",
        "                                self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
        "                    \n",
        "                    print('entities_from_sentence:',just_candidates)\n",
        "                    #storing the outputs and token embeddings in dataframe tweetID,sen_index\n",
        "                    dict1 = {'tweetID':str(tweetID), 'sentID':str(sen_index), 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList,'phase1Candidates': just_candidates ,'phase1CandidatesWPositions':phase1Out,\n",
        "                             'contextual_embeddings':entity_aware_embeddings,\n",
        "                             'start_time':now,'entry_batch':batch_number}\n",
        "                    df_holder.append(dict1)\n",
        "                    # self.contextual_embeddings[(tweetID,sen_index)] = entity_aware_embeddings\n",
        "\n",
        "        time_out=time.time()\n",
        "        self.append_rows(df_holder)\n",
        "        self.phase2stopWordList=list(set(self.phase2stopWordList)|self.swSet)\n",
        "        self.counter=self.counter+1\n",
        "        \n",
        "        #return (copy.deepcopy(self.df_out),copy.deepcopy(freqs),time_in,time_out)\n",
        "        return (self.df_out,self.contextual_embeddings,self.CTrie,time_in,time_out,self.phase2stopWordList,self.tweet_to_sentences_w_annotation)\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        self.df_out=self.df_out.append(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncj3sC0FkBwA"
      },
      "source": [
        "## **Phase II: Global NER with Phrase Embedder to collect the Entity Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noxa5i5JPJi4"
      },
      "source": [
        "# coding: utf-8\n",
        "from nltk.corpus import stopwords\n",
        "import pandas  as pd\n",
        "# import NE_candidate_module as ne\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "import copy\n",
        "# import numpy\n",
        "import math\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from scipy import stats\n",
        "import emoji\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import trie as trie\n",
        "import re\n",
        "import ast\n",
        "import pickle\n",
        "import itertools\n",
        "from scipy import spatial\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.cluster import KMeans, MeanShift\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# import torch\n",
        "\n",
        "#CLASSIFIER CLASS\n",
        "# import SVM as svm\n",
        "# import entityClassifier as entityClassifier\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"since\",\"hence\",\"onto\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"us\")\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"&;\",\"v.\"] #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"httpurl\",\"n't\",\"&amp;\",\"gee\",\"€\",\"hmm\",\"bye\",\"pls\",\"please\",\"yrs\",\"4get\",\"ooh\",\"ouch\",\"am\",\"tv\",\"ima\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"qft\",\"ip\",\"icymi\",\"bdsm\",\"ah\",\"http\",\"https\",\"pm\",\"omw\",\"pts\",\"pt\",\"ive\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "\n",
        "class GlobalNERModule():\n",
        "\n",
        "\n",
        "    def executor(self,max_batch_value,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others,train_classifier):\n",
        "    # def executor(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others)\n",
        "\n",
        "\n",
        "        # SET CB\n",
        "        # print(phase2stopwordList)\n",
        "        if(train_classifier):\n",
        "            return self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,train_classifier)\n",
        "        candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag = self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,train_classifier)\n",
        "        candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        # print(self.good_candidates)\n",
        "\n",
        "        # SET TF \n",
        "        untrashed_tweets=self.set_tf(data_frame_holder, candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "        #mark incomplete tweets\n",
        "        self.set_column_for_candidates_in_incomplete_tweets(candidate_featureBase_DF,untrashed_tweets)\n",
        "\n",
        "        # SAVE INCOMING TWEETS FOR ANNOTATION FOR OTHERS\n",
        "        # self.raw_tweets_for_others=pd.concat([self.raw_tweets_for_others,raw_tweets_for_others ])\n",
        "\n",
        "        # DROP TF\n",
        "        just_converted_tweets=self.get_complete_tf(untrashed_tweets)\n",
        "\n",
        "        #incomplete tweets at the end of current batch\n",
        "        incomplete_tweets=self.get_incomplete_tf(untrashed_tweets)\n",
        "\n",
        "        #all incomplete_tweets---> incomplete_tweets at the end of current batch + incomplete_tweets not reintroduced\n",
        "        # self.incomplete_tweets=incomplete_tweets #without reintroduction--- when everything is reintroduced, just incomplete_tweets\n",
        "        # self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized','annotation','stanford_candidates'])\n",
        "        # self.incomplete_tweets=pd.concat([incomplete_tweets,self.not_reintroduced],ignore_index=True)\n",
        "        self.incomplete_tweets=pd.concat([incomplete_tweets],ignore_index=True)\n",
        "\n",
        "        print('completed tweets:',len(just_converted_tweets))\n",
        "        print('incomplete tweets:',len(incomplete_tweets))\n",
        "\n",
        "\n",
        "        # #recording tp, fp , f1\n",
        "        # #self.accuracy_tuples_prev_batch.append((just_converted_tweets.tp.sum(), just_converted_tweets.total_mention.sum(),just_converted_tweets.fp.sum(),just_converted_tweets.fn.sum()))\n",
        "\n",
        "\n",
        "        # #operations for getting ready for next batch.\n",
        "        # # self.incomplete_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "        # self.incomplete_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "        # self.counter=self.counter+1\n",
        "\n",
        "        self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets)\n",
        "\n",
        "        time_out=time.time()\n",
        "\n",
        "        self.calculate_tp_fp_f1(z_score_threshold,candidate_featureBase_DF,untrashed_tweets,raw_tweets_for_others)\n",
        "\n",
        "        if(self.counter==(max_batch_value+1)):\n",
        "            # self.just_converted_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "            self.just_converted_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "\n",
        "            print('completed tweets: ', len(self.just_converted_tweets),'incomplete tweets: ', len(self.incomplete_tweets))\n",
        "            \n",
        "            print(len(list(self.just_converted_tweets.columns.values)))\n",
        "            print(len(list(self.incomplete_tweets.columns.values)))\n",
        "\n",
        "            combined_frame_list=[self.just_converted_tweets, self.incomplete_tweets]\n",
        "            complete_tweet_dataframe = pd.concat(combined_frame_list)\n",
        "\n",
        "            print('final tally: ', (len(self.just_converted_tweets)+len(self.incomplete_tweets)), len(complete_tweet_dataframe))\n",
        "\n",
        "            #to groupby tweetID and get one tuple per tweetID\n",
        "            complete_tweet_dataframe_grouped_df= (complete_tweet_dataframe.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "            complete_tweet_dataframe_grouped_df['tweetID']=complete_tweet_dataframe_grouped_df['tweetID'].astype(int)\n",
        "            self.complete_tweet_dataframe_grouped_df_sorted=(complete_tweet_dataframe_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "            print(list(self.complete_tweet_dataframe_grouped_df_sorted.columns.values))\n",
        "\n",
        "\n",
        "        #self.aggregator_incomplete_tweets.to_csv(\"all_incompletes.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "\n",
        "        #self.just_converted_tweets.to_csv(\"all_converteds.csv\", sep=',', encoding='utf-8')\n",
        "        #self.incomplete_tweets.to_csv(\"incomplete_for_last_batch.csv\", sep=',', encoding='utf-8')\n",
        "        return candidate_featureBase_DF, self.complete_tweet_dataframe_grouped_df_sorted,time_out \n",
        "        # return candidate_featureBase_DF, untrashed_tweets,time_out\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,entity_phrase_embedder,device,filename):\n",
        "        self.counter=0\n",
        "        self.decay_factor=2**(-1/2)\n",
        "        self.decay_base_staggering=2\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "        self.device=device\n",
        "        self.save_file= filename\n",
        "\n",
        "        self.entity_phrase_embedder = entity_phrase_embedder\n",
        "\n",
        "        # context_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        context_feature_list=['cf_'+str(i) for i in range(300)]\n",
        "        self.candidateBaseHeaders=['candidate', 'batch', 'length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+context_feature_list+['cumulative']\n",
        "        self.candidateBaseHeaders_alt=['candidate', 'batch', 'length','cumulative']\n",
        "        \n",
        "        ## When not running on a notebook\n",
        "        # self.entity_classifier = entityClassifier.EntityClassifier('data/candidate_train_records.csv',True,self.device)\n",
        "\n",
        "        ################################### To do a fresh training\n",
        "\n",
        "        ## With one unified classifier\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large.csv',True,self.device)\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_300d.csv',True,self.device)\n",
        "\n",
        "        # # With two separate classifiers\n",
        "        # self.entity_classifierI= EntityClassifierI('data/training.csv',True,self.device)\n",
        "        # # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large.csv',True,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large_300d.csv',True,self.device)\n",
        "\n",
        "        ################################### To Load a pre-trained model\n",
        "\n",
        "        ## With one unified classifier\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large.csv',False,self.device)\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_300d.csv',False,self.device)\n",
        "\n",
        "        ## With two separate classifiers\n",
        "        # self.entity_classifierI= EntityClassifierI('data/training.csv',False,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large.csv',False,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large_300d.csv',False,self.device)\n",
        "\n",
        "\n",
        "        ################################### Older SVM classifier\n",
        "        # self.my_classifier= svm.SVM1('/home/satadisha/Desktop/GitProjects/TwiCSv2/production_code/training.csv')\n",
        "        \n",
        "        # self.my_classifier= svm.SVM1('/Users/satadisha/Documents/GitHub/TwiCSv2/production_code/training.csv')\n",
        "        # self.my_classifier= svm.SVM1('/Users/satadisha/Documents/GitHub/tweebo-parser/training.csv')\n",
        "        # self.my_classifier= svm.SVM1('training.csv')\n",
        "        self.complete_tweet_dataframe_grouped_df_sorted=pd.DataFrame([], columns=['tweetID', 'TweetSentence', 'ambiguous_candidates', 'annotation', 'candidates_with_label', 'completeness', 'current_minus_entry', 'entry_batch', 'hashtags', 'index', 'only_good_candidates', 'output_mentions', 'phase1Candidates', 'sentID', 'stanford_candidates', 'user'])\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1_generic(self,raw_tweets_for_others,state_of_art):\n",
        "\n",
        "        column_candidates_holder = raw_tweets_for_others[state_of_art].tolist()\n",
        "        \n",
        "\n",
        "        column_annot_holder= raw_tweets_for_others['mentions_other'].tolist()\n",
        "        # column_annot_holder= raw_tweets_for_others['annotation_limited types'].tolist()\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=column_annot_holder[idx].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=ast.literal_eval(column_candidates_holder[idx])\n",
        "            else:\n",
        "                output_mentions_list=column_candidates_holder[idx].split(',')\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            # print(annotated_mention_list,output_mentions_list)\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall))    \n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "        # tweet_ids_df[\"tp\"+state_of_art]=true_positive_holder\n",
        "        # tweet_ids_df[\"fn\"+state_of_art]=false_negative_holder\n",
        "        # tweet_ids_df['fp'+state_of_art]= false_positive_holder\n",
        "        \n",
        "        # if(state_of_art==\"ritter_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"ritter_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # if(state_of_art==\"stanford_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"stanford_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "    def calculate_tp_fp_f1_alternate(self,raw_tweets_for_others, state_of_art):\n",
        "        if(state_of_art=='neuroner'):\n",
        "            # fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/venezuela_input_2019-05-10_12-33-16-15380/mentions_output.txt\",\"r\")\n",
        "            fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/tweets_3K_input_2019-04-26_16-49-32-20455/mentions_output.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='stanford_candidates'):\n",
        "            fp= open(\"/home/satadisha/Desktop/stanford-ner-2016-10-31/stanford_venezuela_mentions.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='ritter_candidates'):\n",
        "            tweets_ritter=pd.read_csv(\"/home/satadisha/Desktop/GitProjects/twitter_nlp-master/ritter-venezuela-output.csv\",sep =',', keep_default_na=False)\n",
        "        if(state_of_art=='calai_candidates'):\n",
        "            tweets_calai=pd.read_csv(\"/home/satadisha/Desktop/opencalai_versions/venezuela_output.csv\",sep =',', keep_default_na=False)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        for index, row in raw_tweets_for_others.iterrows():\n",
        "            \n",
        "\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=row['mentions_other'].split(';')\n",
        "            # tweet_level_candidate_list=row['annotation_limited types'].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='ritter_candidates'):\n",
        "                # output_mentions_list=ast.literal_eval(mentions_list[idx])\n",
        "                output_mentions_list=tweets_ritter[tweets_ritter['ID']==row['ID']]['Output'].iloc[0].split(',')\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=tweets_calai[tweets_calai['ID']==row['ID']]['calai_candidates'].iloc[0].split(',')\n",
        "            if((state_of_art=='neuroner')|(state_of_art=='stanford_candidates')):\n",
        "                #for 3k Tweets:\n",
        "                idx=int(row['ID'])\n",
        "\n",
        "                # #for others:\n",
        "                # idx=int(row['ID']-1)\n",
        "\n",
        "                output_mentions_list=mentions_list[idx].split(',')\n",
        "\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            print(annotated_mention_list,output_mentions_list)\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall)) \n",
        "        if(state_of_art==\"neuroner\"):\n",
        "            self.accuracy_vals_neuroner.append((f_measure,precision,recall))\n",
        "\n",
        "\n",
        "        # output_mentions_list= mentions_list[output_index].split(',')\n",
        "#     # output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "#     # output_mentions_list=list(filter(lambda element: element !='', output_mentions_list))\n",
        "\n",
        "    def calculate_tp_fp_f1_for_others(self,raw_tweets_for_others):\n",
        "\n",
        "        opencalai=\"calai_candidates\"\n",
        "        stanford=\"stanford_candidates\"\n",
        "        ritter=\"ritter_candidates\"\n",
        "        neuroner=\"neuroner\"\n",
        "\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,opencalai)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,stanford)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,ritter)\n",
        "\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,opencalai)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,stanford)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,ritter)\n",
        "\n",
        "        self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,neuroner)\n",
        "\n",
        "    #################################\n",
        "    #input candidate_feature_Base\n",
        "    #output candidate_feature_Base with [\"Z_score\"], [\"probability\"],[\"class\"]\n",
        "    #################################\n",
        "    def classify_candidate_base(self,z_score_threshold,candidate_featureBase_DF):\n",
        "\n",
        "        # #filtering test set based on z_score\n",
        "        # mert1=candidate_featureBase_DF['cumulative'].as_matrix()\n",
        "        #frequency_array = np.array(list(map(lambda val: val[0], sortedCandidateDB.values())))\n",
        "        # zscore_array1=stats.zscore(mert1)\n",
        "\n",
        "        zscore_array1=stats.zscore(candidate_featureBase_DF['cumulative'])\n",
        "\n",
        "        candidate_featureBase_DF['Z_ScoreUnweighted']=zscore_array1\n",
        "        cumulative_threshold=1.0 #set threshold here\n",
        "        z_score_threshold=candidate_featureBase_DF[candidate_featureBase_DF['cumulative']==cumulative_threshold].Z_ScoreUnweighted.tolist()[0]\n",
        "        print(cumulative_threshold,z_score_threshold)\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_new_with_z_score.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        #multi-word infrequent candidates ---> to be used for recall correction\n",
        "        infrequent_candidates=candidate_featureBase_DF[(candidate_featureBase_DF['Z_ScoreUnweighted'] < z_score_threshold) & (candidate_featureBase_DF.length>1)].candidate.tolist()\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF[candidate_featureBase_DF['Z_ScoreUnweighted'] >= z_score_threshold]\n",
        "\n",
        "        # # #######################with one unified classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # return (self.entity_classifier.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # # #######################with only semantic classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # return (self.entity_classifierII.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with alt semantic classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        return (self.entity_classifier_alt.run(candidate_featureBase_DF,self.CandidateBase_dict_alt),infrequent_candidates)\n",
        "\n",
        "        # #######################with two separate classifiers--- requires some additional lines of code\n",
        "        # candidateList = candidate_featureBase_DF.candidate.tolist()\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # candidate_featureBase_DF.set_index(\"candidate\", inplace=True)\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF.index.name)\n",
        "\n",
        "        # # returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # candidate_featureBase_DF_classifierI = self.entity_classifierI.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "        # candidate_featureBase_DF_classifierII = self.entity_classifierII.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "\n",
        "        # # candidate_featureBase_DF_classifierI.to_csv('classifierI.csv', sep=',', encoding='utf-8')\n",
        "        # # candidate_featureBase_DF_classifierII.to_csv('classifierII.csv', sep=',', encoding='utf-8') #candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # final_probability_dict = {}\n",
        "        # print5=0\n",
        "        # for candidate in candidateList:\n",
        "        #     prob1 = candidate_featureBase_DF_classifierI.loc[candidate]['probability']\n",
        "        #     prob2 = candidate_featureBase_DF_classifierII.loc[candidate]['probability']\n",
        "        #     final_probability = max(prob1,prob2)\n",
        "        #     if(print5<5):\n",
        "        #         print(candidate,prob1,prob2,final_probability)\n",
        "        #         print5+=1\n",
        "        #     final_probability_dict[candidate] = final_probability\n",
        "\n",
        "        # candidate_featureBase_DF[\"probability\"] = pd.Series(final_probability_dict)\n",
        "\n",
        "        # candidate_featureBase_DF.reset_index(drop=False,inplace=True)\n",
        "        # print('after columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF[['candidate','probability']].head(5))\n",
        "\n",
        "        # return (candidate_featureBase_DF,infrequent_candidates)\n",
        "\n",
        "    # recall_correction\n",
        "    def set_partition_dict(self,candidate_featureBase_DF,infrequent_candidates):\n",
        "\n",
        "        #print(list(self.partition_dict.keys()))\n",
        "        ambiguous_bad_candidates=candidate_featureBase_DF[(((candidate_featureBase_DF.status==\"a\")|(candidate_featureBase_DF.status==\"b\"))&(candidate_featureBase_DF.length.astype(int)>1))]\n",
        "        good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "        flag1=False\n",
        "        flag2=False\n",
        "        if(len(ambiguous_bad_candidates)>0):\n",
        "            # ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1) \n",
        "            # ambiguous_bad_candidates_wFilter= ambiguous_bad_candidates[ambiguous_bad_candidates.max_column=='substring-cap']\n",
        "\n",
        "            #good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "            #print(ambiguous_bad_candidates_wFilter.candidate.tolist())\n",
        "\n",
        "            # for candidate in ambiguous_bad_candidates_wFilter.candidate.tolist():\n",
        "            for candidate in ambiguous_bad_candidates.candidate.tolist():\n",
        "                \n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "\n",
        "            flag1= True\n",
        "        if(len(infrequent_candidates)>0):\n",
        "            #print(len(ambiguous_bad_candidates_wFilter.candidate.tolist()))\n",
        "\n",
        "            for candidate in infrequent_candidates:\n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "            flag2= True\n",
        "        print(list(self.partition_dict.keys()))\n",
        "        return (flag1|flag2)\n",
        "\n",
        "    def get_reintroduced_tweets(self, reintroduction_threshold):\n",
        "        #no reintroduction\n",
        "        #no preferential selection\n",
        "        print(\"incomplete tweets in batch: \",len(self.incomplete_tweets))\n",
        "        # print(list(self.incomplete_tweets.columns.values))\n",
        "\n",
        "        reintroduced_tweets=self.incomplete_tweets[(self.counter-self.incomplete_tweets['entry_batch'])<=reintroduction_threshold]\n",
        "        self.not_reintroduced=self.incomplete_tweets[~self.incomplete_tweets.index.isin(reintroduced_tweets.index)]\n",
        "\n",
        "        print(\"reintroduced tweets: \",len(reintroduced_tweets))\n",
        "        # for i in range(self.counter):\n",
        "        #     print('i:',len(self.incomplete_tweets[self.incomplete_tweets['entry_batch']==i]))\n",
        "        return reintroduced_tweets\n",
        "\n",
        "    def set_cb(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,train_classifier):\n",
        "\n",
        "        #input new_tweets, z_score, Updated candidatebase of phase1\n",
        "        #output candidate_featureBase_DF, Incomplete_tweets\n",
        "        data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "\n",
        "        if(train_classifier):\n",
        "            return self.extract(TweetBase,CTrie,phase2stopwordList,0,train_classifier)\n",
        "\n",
        "        candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted= self.extract(TweetBase,CTrie,phase2stopwordList,0,train_classifier)\n",
        "        phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "        phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "        df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        if((self.counter>0)&(len(self.incomplete_tweets)>0)):\n",
        "\n",
        "            #tweet candidates for Reintroduction\n",
        "            reintroduced_tweets=self.get_reintroduced_tweets(reintroduction_threshold)\n",
        "            candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted = self.extract(reintroduced_tweets,CTrie,phase2stopwordList,1,train_classifier)\n",
        "            phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "            phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "            df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        #print(len(df_holder))\n",
        "        data_frame_holder = pd.DataFrame(df_holder)\n",
        "        #print(len(self.incomplete_tweets),len(data_frame_holder),len(candidate_featureBase_DF))\n",
        "        \n",
        "\n",
        "        #set ['probabilities'] for candidate_featureBase_DF\n",
        "        candidate_featureBase_DF,self.infrequent_candidates= self.classify_candidate_base(z_score_threshold,candidate_featureBase_DF)\n",
        "\n",
        "        # set readable labels (a,g,b) for candidate_featureBase_DF based on ['probabilities.']\n",
        "        candidate_featureBase_DF=self.set_readable_labels(candidate_featureBase_DF)\n",
        "\n",
        "        self.good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        self.ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "        self.bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        print('good_candidates:',self.good_candidates)\n",
        "        print('bad_candidates:',self.bad_candidates)\n",
        "        print('ambiguous_candidates:',self.ambiguous_candidates)\n",
        "\n",
        "        # entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.good_candidates)]\n",
        "        # non_entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.bad_candidates)]\n",
        "        # ambiguous_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.ambiguous_candidates)]\n",
        "\n",
        "        correction_flag=self.set_partition_dict(candidate_featureBase_DF,self.infrequent_candidates)\n",
        "\n",
        "        ambiguous_turned_good=[]\n",
        "        ambiguous_turned_bad=[]\n",
        "        ambiguous_remaining_ambiguous=[]\n",
        "        converted_candidates=[]\n",
        "\n",
        "        #['probability'],['a,g,b']\n",
        "        return candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag\n",
        "\n",
        "\n",
        "        #flush out completed tweets\n",
        "        # input candidate base, looped over tweets (incomplete tweets+ new tweets)\n",
        "        # output: incomplete tweets (a tags in it.), incomplete_tweets[\"Complete\"]\n",
        "    def set_tf(self,data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        return self.set_completeness_in_tweet_frame(data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "    def get_incomplete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==False]\n",
        "\n",
        "    def get_complete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==True]\n",
        "\n",
        "    def compute_seen_tweets_so_far(self,start_batch,end_batch):\n",
        "        if(start_batch==end_batch):\n",
        "            sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch]\n",
        "\n",
        "\n",
        "        sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch:]\n",
        "\n",
        "\n",
        "        counter=0\n",
        "        for elem in sliced_seen_tweets:\n",
        "            counter=counter+elem\n",
        "\n",
        "        return counter\n",
        "\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "    #ME_EXTR=Mention.Mention_Extraction()\n",
        "\n",
        "\n",
        "    # experiment function\n",
        "    def set_x_axis(self,just_converted_tweets_for_current_batch):\n",
        "\n",
        "        self.incomplete_tweets.to_csv(\"set_x_axis_debug.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        self.incomplete_tweets['number_of_seen_tweets'] = self.incomplete_tweets['entry_batch'].apply(lambda x: self.compute_seen_tweets_so_far(x,self.counter))\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"entry_vs_tweet_seen_ratio\"]=self.incomplete_tweets['entry_batch']/self.incomplete_tweets['number_of_seen_tweets']\n",
        "\n",
        "\n",
        "        #counter_list= \n",
        "        self.incomplete_tweets[\"ratio_entry_vs_current\"]=self.incomplete_tweets['entry_batch']/self.counter\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"current_minus_entry\"]=self.counter-self.incomplete_tweets['entry_batch']\n",
        "\n",
        "        just_converted_tweets_for_current_batch[\"current_minus_entry\"]=self.counter-just_converted_tweets_for_current_batch['entry_batch']\n",
        "\n",
        "        return just_converted_tweets_for_current_batch\n",
        "\n",
        "\n",
        "\n",
        "    def set_column_for_candidates_in_incomplete_tweets(self,candidate_featureBase_DF,input_to_eval):\n",
        "\n",
        "        incomplete_candidates= input_to_eval['2nd Iteration Candidates'].tolist()\n",
        "\n",
        "        candidate_featureBase_DF= candidate_featureBase_DF.set_index('candidate')\n",
        "\n",
        "        candidate_with_label_holder=[]\n",
        "        one_level=[]\n",
        "        \n",
        "\n",
        "        for sentence_level_candidates in incomplete_candidates:\n",
        "\n",
        "            one_level.clear()\n",
        "\n",
        "            for candidate in sentence_level_candidates:\n",
        "                if candidate.lower() in candidate_featureBase_DF.index:\n",
        "                    # label=candidate_featureBase_DF.get_value(candidate.lower(),'status')\n",
        "                    label=candidate_featureBase_DF.at[candidate.lower(),'status']\n",
        "                    one_level.append((candidate,label))\n",
        "                else:\n",
        "                    one_level.append((candidate,\"na\"))\n",
        "\n",
        "            candidate_with_label_holder.append(copy.deepcopy(one_level))\n",
        "\n",
        "\n",
        "        input_to_eval[\"candidates_with_label\"]=candidate_with_label_holder\n",
        "        debug_candidates_label_list= input_to_eval['candidates_with_label'].tolist()\n",
        "        candidates_filtered_g_labeled=[]\n",
        "        row_level_candidates=[]\n",
        "        index_outer=0\n",
        "\n",
        "        candidates_filtered_a_labeled=[]\n",
        "        row_level_a_candidates=[]\n",
        "\n",
        "        for sentence_level in debug_candidates_label_list:\n",
        "\n",
        "            # sentence_level_candidates_unnormalized= incomplete_candidates_unnormalized[index_outer]\n",
        "            row_level_candidates.clear()\n",
        "            row_level_a_candidates.clear()\n",
        "            for candidate in sentence_level:\n",
        "                if(candidate[1]==\"g\"):\n",
        "                    candidate_str = self.erode_article(candidate[0])\n",
        "                    row_level_candidates.append(candidate_str)\n",
        "                if(((candidate[1]==\"b\")|(candidate[1]==\"a\"))&(candidate[0]==\"US\")):\n",
        "                    # print('here')\n",
        "                    row_level_candidates.append(candidate[0])\n",
        "                if(candidate[1]==\"a\"):\n",
        "                    row_level_a_candidates.append(candidate[0])\n",
        "\n",
        "            candidates_filtered_g_labeled.append(copy.deepcopy(row_level_candidates))\n",
        "            candidates_filtered_a_labeled.append(copy.deepcopy(row_level_a_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "\n",
        "        input_to_eval[\"only_good_candidates\"]=candidates_filtered_g_labeled\n",
        "        input_to_eval[\"ambiguous_candidates\"]=candidates_filtered_a_labeled\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1(self,z_score_threshold,candidate_featureBase_DF,input_to_eval,raw_tweets_for_others):\n",
        "\n",
        "        input_to_eval_grouped_df= (input_to_eval.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "        input_to_eval_grouped_df['tweetID']=input_to_eval_grouped_df['tweetID'].astype(int)\n",
        "        input_to_eval_df_sorted=(input_to_eval_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "        print(len(input_to_eval_df_sorted),len(raw_tweets_for_others))\n",
        "        \n",
        "        print(set(input_to_eval_df_sorted['tweetID'].values.tolist())-set(raw_tweets_for_others['ID'].values.tolist()))\n",
        "\n",
        "        input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_other'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_limited_types'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['annotation_limited types'].iloc[0])\n",
        "\n",
        "        column_candidates_holder = input_to_eval_df_sorted['only_good_candidates'].tolist()\n",
        "        column_phase1Candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "        # column_candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "\n",
        "        file1 = open(self.save_file+\".txt\", \"w\")\n",
        "        \n",
        "        phase1Candidates=candidate_featureBase_DF.candidate.tolist()\n",
        "\n",
        "        column_annot_holder= input_to_eval_df_sorted['annotation'].tolist()\n",
        "        \n",
        "        output_str=''\n",
        "        # print(column_candidates_holder)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "        total_annotation=0\n",
        "        phase1NotInphase2_count=0\n",
        "        mislabelledCandidateMentions=[]\n",
        "\n",
        "        all_annotations=[]\n",
        "        annotation_dict={}\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        print('=========================BERTNER_candidates')\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "            phase1NotInphase2_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            output_mentions_list=[]\n",
        "            phase1_mentions_list=[]\n",
        "\n",
        "            tweet_level_candidate_list=str(column_annot_holder[idx]).split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            \n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: quickRegex.match(element), annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: ((element !='')&(element !='nan')), annotated_mention_list))\n",
        "\n",
        "            for lst in column_candidates_holder[idx]:\n",
        "                output_mentions_list.extend(lst)\n",
        "\n",
        "            for lst in column_phase1Candidates_holder[idx]:\n",
        "                phase1_mentions_list.extend(lst)\n",
        "\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "            output_mentions_list=list(map(lambda element: element.lower(), output_mentions_list))\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            for annotated_entity in annotated_mention_list:\n",
        "                try:\n",
        "                    annotation_dict[annotated_entity]+=1\n",
        "                except KeyError:\n",
        "                    annotation_dict[annotated_entity]=1\n",
        "\n",
        "            print(idx, annotated_mention_list,output_mentions_list)\n",
        "            print(phase1_mentions_list)\n",
        "\n",
        "            output_str+=','.join(output_mentions_list)+'\\n'\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            # candidate in Candidate but missed here\n",
        "            # for elem in annotated_mention_list:\n",
        "            #     elem = self.normalize(elem)\n",
        "            #     if((elem in phase1Candidates)&(elem not in output_mentions_list)):\n",
        "            #         mislabelledCandidates.append(elem)\n",
        "            #         mislabelledCandidateMentionCount+=1\n",
        "\n",
        "            # add phase1NotInphase2 code!!!!!!!!            \n",
        "            for elem in annotated_mention_list:\n",
        "                elem = self.normalize(elem)\n",
        "                if(phase1_mentions_list):\n",
        "                    if ((elem in phase1_mentions_list)&(elem not in output_mentions_list)):\n",
        "                        phase1_mentions_list.pop(phase1_mentions_list.index(elem))\n",
        "                        phase1NotInphase2_inner+=1\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= self.normalize(annotated_mention_list.pop())\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        if(annotated_candidate in phase1Candidates):\n",
        "                            mislabelledCandidateMentions.append(annotated_candidate)\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    for elem in annotated_mention_list:\n",
        "                        elem = self.normalize(elem)\n",
        "                        if(elem in phase1Candidates):\n",
        "                            mislabelledCandidateMentions.append(elem)\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            # print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "            self.true_positive_count+=tp_counter_inner\n",
        "            self.false_positive_count+=fp_counter_inner\n",
        "            self.false_negative_count+=fn_counter_inner\n",
        "            phase1NotInphase2_count+=phase1NotInphase2_inner\n",
        "\n",
        "        print('TP||||FP||||FN')\n",
        "        print(self.true_positive_count,self.false_positive_count,self.false_negative_count,total_mentions,total_annotation)\n",
        "        print('Phase1 but not in Phase2:',phase1NotInphase2_count)\n",
        "\n",
        "        print('mislabelled candidates:',len(list(set(mislabelledCandidateMentions))))\n",
        "        print('mentions missed by mislabeling:',len(mislabelledCandidateMentions))\n",
        "\n",
        "        precision=(self.true_positive_count)/(self.true_positive_count+self.false_positive_count)\n",
        "        recall=(self.true_positive_count)/(self.true_positive_count+self.false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        all_annotations=set(all_annotations)\n",
        "        all_mentions=set(all_mentions)\n",
        "        \n",
        "        true_positives = all_annotations.intersection(all_mentions)\n",
        "        true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        false_positive_count=len(all_mentions-all_annotations)\n",
        "        false_negative_count=len(all_annotations-all_mentions)\n",
        "        total_mentions=len(all_mentions)\n",
        "        total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        freq_bucket = {} \n",
        "        for candidate in all_annotations:\n",
        "            candidate_freq = annotation_dict[candidate]\n",
        "            flag = False\n",
        "            if(candidate in true_positives):\n",
        "                flag = True\n",
        "            try:\n",
        "                old_tup = freq_bucket[candidate_freq]\n",
        "                if flag:\n",
        "                    freq_bucket[candidate_freq] = (old_tup[0]+1,old_tup[1])\n",
        "                else:\n",
        "                    freq_bucket[candidate_freq] = (old_tup[0],old_tup[1]+1)\n",
        "            except KeyError:\n",
        "                if flag:\n",
        "                    freq_bucket[candidate_freq] = (1,0)\n",
        "                else:\n",
        "                    freq_bucket[candidate_freq] = (0,1)\n",
        "        \n",
        "        x_axis=[]\n",
        "        y_axis =[]\n",
        "        cumulative_tp=0\n",
        "        cumulative_annotated=0\n",
        "        freq_bucket_sorted = {k : freq_bucket[k] for k in sorted(freq_bucket)}\n",
        "\n",
        "        maxKey = list(freq_bucket_sorted.keys())[-1]\n",
        "        print(freq_bucket_sorted.keys(),maxKey)\n",
        "\n",
        "        step=0\n",
        "        for key in range(maxKey):\n",
        "            try:\n",
        "                tup = freq_bucket_sorted[key]\n",
        "            except KeyError:\n",
        "                tup = (0,0)\n",
        "\n",
        "            # freq_recall= tup[0]/(tup[0]+tup[1])\n",
        "\n",
        "            if(step==5):\n",
        "                if((cumulative_tp==0)&(cumulative_annotated==0)):\n",
        "                    freq_recall= y_axis[-1]\n",
        "                else:\n",
        "                    freq_recall= cumulative_tp/cumulative_annotated\n",
        "                # print(key, freq_recall)\n",
        "                x_axis.append(key)\n",
        "                y_axis.append(freq_recall)\n",
        "                step=0\n",
        "                cumulative_tp=0\n",
        "                cumulative_annotated=0\n",
        "\n",
        "\n",
        "            cumulative_tp+=tup[0]\n",
        "            cumulative_annotated+=(tup[0]+tup[1])\n",
        "            # freq_recall= cumulative_tp/cumulative_annotated\n",
        "            step+=1\n",
        "\n",
        "        print(x_axis)\n",
        "        print(y_axis)\n",
        "\n",
        "        # print20=20\n",
        "        for ind, elem in enumerate(x_axis):\n",
        "            # if(print20>0):\n",
        "            print(x_axis[ind],':',y_axis[ind])\n",
        "            #     print20=-1\n",
        "            # else:\n",
        "            #     print(x_axis[ind],':',y_axis[ind])\n",
        "            #     print20=20\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "\n",
        "        # true_positive_count_IPQ=true_positive_count\n",
        "        # false_positive_count_IPQ = false_positive_count\n",
        "        # false_negative_count_IPQ= false_negative_count\n",
        "        # total_mention_count_IPQ=total_mentions\n",
        "\n",
        "\n",
        "        # tp_count=0\n",
        "        # tm_count=0\n",
        "        # fp_count=0\n",
        "        # fn_count=0\n",
        "\n",
        "        # for idx,tup in enumerate(self.accuracy_tuples_prev_batch):\n",
        "        #     # print(idx,tup)\n",
        "        #     tp_count+=tup[0]\n",
        "        #     tm_count+=tup[1]\n",
        "        #     fp_count+=tup[2]\n",
        "        #     fn_count+=tup[3]\n",
        "\n",
        "\n",
        "\n",
        "        # tp_count+=true_positive_count_IPQ\n",
        "        # tm_count+=total_mention_count_IPQ\n",
        "        # fp_count+=false_positive_count_IPQ\n",
        "        # fn_count+=false_negative_count_IPQ\n",
        "\n",
        "        # precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        # recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        # f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        file1.write(output_str)\n",
        "        file1.close()\n",
        "\n",
        "\n",
        "\n",
        "        self.accuracy_vals=(z_score_threshold,f_measure,precision,recall)\n",
        "\n",
        "        print('Precision:',precision)\n",
        "        print('Recall:',recall)\n",
        "        print('F1:',f_measure)\n",
        "\n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "\n",
        "        # input_to_eval[\"tp\"]=true_positive_holder\n",
        "        # input_to_eval[\"fn\"]=false_negative_holder\n",
        "        # input_to_eval['fp']= false_positive_holder\n",
        "        # input_to_eval[\"total_mention\"]=total_mention_holder\n",
        "\n",
        "        # input_to_eval[\"ambigious_not_in_annot\"]=ambigious_not_in_annotation_holder\n",
        "        # input_to_eval[\"inverted_loss\"]=input_to_eval[\"tp\"]/( input_to_eval[\"fn\"]+input_to_eval[\"ambigious_not_in_annot\"])\n",
        "\n",
        "        return input_to_eval\n",
        "\n",
        "\n",
        "    def recall_correction(self,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder):\n",
        "\n",
        "        corrected_phase2_candidates_holder=[]\n",
        "        index_outer=0\n",
        "        for candidates in phase2_candidates_holder:\n",
        "            unnormalized_candidates=phase2_unnormalized_candidates_holder[index_outer]\n",
        "            corrected_phase2_candidates=[]\n",
        "            for idx, candidate in enumerate(candidates):\n",
        "                unnormalized_candidate=unnormalized_candidates[idx]\n",
        "                # if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates))):\n",
        "                if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))):   #do this only for 3K tweets\n",
        "                    #print(candidate, self.partition_dict[candidate])\n",
        "                    corrected_phase2_candidates.extend(self.partition_dict[candidate])\n",
        "                else:\n",
        "                    if(((candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))&(candidate=='us')&(unnormalized_candidate=='US')):\n",
        "                        # print(index_outer)\n",
        "                        candidate=unnormalized_candidate\n",
        "                    # if((len(candidate.strip().strip(string.punctuation).split())>1)&(candidate.strip().strip(string.punctuation).split()[0].lower() in ['a','an','the'])):\n",
        "                    #     candidate = (' '.join(candidate.strip().strip(string.punctuation).split()[1:])).strip()\n",
        "                    corrected_phase2_candidates.append(candidate)\n",
        "            corrected_phase2_candidates_holder.append(copy.deepcopy(corrected_phase2_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "        \n",
        "        #print(corrected_phase2_candidates_holder)\n",
        "        data_frame_holder['2nd Iteration Candidates']=corrected_phase2_candidates_holder\n",
        "\n",
        "        return corrected_phase2_candidates_holder,data_frame_holder                  \n",
        "\n",
        "\n",
        "    def erode_article(self, entity_string):\n",
        "        if((len(entity_string.strip().strip(string.punctuation).split())>1)&(entity_string.lower().strip().strip(string.punctuation).split()[0] in ['a','an','the'])):\n",
        "            # print(entity_string)\n",
        "            entity_string = ' '.join(entity_string.strip().strip(string.punctuation).split()[1:])\n",
        "            # print(entity_string)\n",
        "        return entity_string.strip()\n",
        "\n",
        "\n",
        "    def set_completeness_in_tweet_frame(self,data_frame_holder,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        #print(candidate_featureBase_DF.head())\n",
        "        good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        merged_g_b= bad_candidates+good_candidates\n",
        "\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_before_labeling_comp.csv\", sep=',', encoding='utf-8')\n",
        "        ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "\n",
        "        if(correction_flag):\n",
        "            phase2_candidates_holder,data_frame_holder=self.recall_correction(candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder)\n",
        "\n",
        "         \n",
        "\n",
        "        \n",
        "        truth_vals=[False if any(x not in merged_g_b for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        intermediate_output_mentions=[list(filter(lambda candidate: ((candidate in good_candidates))|(candidate=='US'), list1)) for list1 in phase2_candidates_holder]\n",
        "\n",
        "        output_mentions=[list(map(lambda candidate: self.erode_article(candidate), list1)) for list1 in intermediate_output_mentions]\n",
        "\n",
        "        # truth_vals=[False if any(x in ambiguous_candidates for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        # for list1 in phase2_candidates_holder:\n",
        "        #     if any(x in ambiguous_candidates  for x in list1):\n",
        "        #         truth_vals.append(False)\n",
        "        #     else:\n",
        "        #         truth_vals.append(True)\n",
        " \n",
        "\n",
        "\n",
        "        #print(truth_vals)\n",
        "        completeness_series = pd.Series( (v for v in truth_vals) )\n",
        "        output_mentions_series = pd.Series( (v for v in output_mentions) )\n",
        "\n",
        "\n",
        "        data_frame_holder['output_mentions']=output_mentions_series\n",
        "        data_frame_holder['completeness']=completeness_series\n",
        "        data_frame_holder[\"current_minus_entry\"]=self.counter-data_frame_holder['entry_batch']\n",
        "\n",
        "        return data_frame_holder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_readable_labels(self,candidate_featureBase_DF):\n",
        "\n",
        "        #candidate_featureBase_DF['status'] = candidate_featureBase_DF['probability'].apply(lambda x: set(x).issubset(good_candidates))\n",
        "        candidate_featureBase_DF['status']='ne'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.55]='g'\n",
        "        candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.55)] = 'a'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
        "\n",
        "        return candidate_featureBase_DF\n",
        "\n",
        "\n",
        "\n",
        "    def normalize(self,word):\n",
        "        strip_op=word\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        if strip_op.endswith(\"'s\"):\n",
        "            li = strip_op.rsplit(\"'s\", 1)\n",
        "            return ''.join(li)\n",
        "        elif strip_op.endswith(\"’s\"):\n",
        "            li = strip_op.rsplit(\"’s\", 1)\n",
        "            return ''.join(li)\n",
        "        else:\n",
        "            return strip_op\n",
        "        #return strip_op\n",
        "\n",
        "    \n",
        "    def isSubstring(self,to_increase_element,id_to_incr,comparison_holder,phase1_holder_holder_copy):\n",
        "        combined_list=comparison_holder[id_to_incr]+phase1_holder_holder_copy[id_to_incr]\n",
        "\n",
        "        for idx,val in enumerate(comparison_holder[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[5]) and to_increase_element[5] != val[5]):\n",
        "                    return True\n",
        "        for idx,val in enumerate(phase1_holder_holder_copy[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[2]) and to_increase_element[5] != val[2]):\n",
        "                    return True   \n",
        "                \n",
        "        return False\n",
        "\n",
        "\n",
        "    def calculate_pmi(self,big,x1,x2,total):\n",
        "        big__= float(big/total)\n",
        "        x1__=float(x1/total)\n",
        "        x2__=float(x2/total)\n",
        "        pmi= math.log(big__/(x1__*x2__),2.71828182845)\n",
        "        pklv=big__*pmi\n",
        "        #return (1/(1+math.exp(-1*pmi)))\n",
        "        npmi= pmi/(-1.0*(math.log(big__,2.71828182845)))\n",
        "        return npmi,pklv\n",
        "        #return pklv\n",
        "\n",
        "    def multiSlice(self,s,cutpoints,good_candidates):\n",
        "        k = len(cutpoints)\n",
        "        multislices=[]\n",
        "        if k == 0:\n",
        "            curr_candidate=self.normalize(' '.join(s))\n",
        "\n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]        \n",
        "        else:\n",
        "            \n",
        "            curr_candidate=self.normalize(' '.join(s[:cutpoints[0]]))\n",
        "            alt_list=[curr_candidate]\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]\n",
        "\n",
        "            alt_list.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1))\n",
        "            multislices.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1) if self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) in good_candidates)\n",
        "\n",
        "            curr_candidate=self.normalize(' '.join(s[cutpoints[k-1]:]))\n",
        "            alt_list.append(curr_candidate)\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices.append(curr_candidate)\n",
        "            # print('::',alt_list)\n",
        "        return multislices\n",
        "\n",
        "\n",
        "\n",
        "    def get_substring_candidates(self,candidate_words,good_candidates):\n",
        "        n = len(candidate_words)\n",
        "        all_partitions=[]\n",
        "        all_partitions_length=[]\n",
        "        cuts = list(range(1,n))\n",
        "        for k in range(n):\n",
        "            # all_partitions_inner=[]\n",
        "            partition_list=[]\n",
        "            partition_length_list=[]\n",
        "            for cutpoints in itertools.combinations(cuts,k):\n",
        "                ret_list=self.multiSlice(candidate_words,cutpoints,good_candidates)\n",
        "                if(ret_list):\n",
        "                    partition_length=sum([len(elem.split()) for elem in ret_list])\n",
        "                    # print('==',ret_list,partition_length)\n",
        "                    if(partition_length==len(candidate_words)):\n",
        "                        return ret_list\n",
        "                    partition_list.append(ret_list)\n",
        "                    partition_length_list.append(partition_length)\n",
        "                    # yield ret_list\n",
        "            # print('------')\n",
        "            if(partition_length_list):\n",
        "                max_index=partition_length_list.index(max(partition_length_list))\n",
        "                all_partitions.append(partition_list[max_index])\n",
        "                all_partitions_length.append(partition_length_list[max_index])\n",
        "        # print(all_partitions)\n",
        "        if(all_partitions_length):\n",
        "            max_index=all_partitions_length.index(max(all_partitions_length))\n",
        "            # print(all_partitions[max_index])\n",
        "            return all_partitions[max_index]\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "\n",
        "    def verify(self, subsequence, CTrie):\n",
        "        return CTrie.__contains__(subsequence)\n",
        "\n",
        "\n",
        "\n",
        "    def check_sequence(self, sequence, l, CTrie):\n",
        "        result=[]\n",
        "        subsequence_length=l\n",
        "        while(subsequence_length>0):\n",
        "            shift=len(sequence)-subsequence_length\n",
        "            verified_subsequence=[]\n",
        "            verified=False\n",
        "            for i in range(0,shift+1):\n",
        "                list1=sequence[i:(i+subsequence_length)]\n",
        "                text=' '.join(str(e[0]) for e in list1)\n",
        "                subsequence=(self.normalize(text)).split()\n",
        "                #print(\"search for\", subsequence)\n",
        "                if self.verify(subsequence, CTrie):\n",
        "                    verified_subsequence.append(i)\n",
        "                    verified_subsequence.append(i+subsequence_length)\n",
        "                    #print(subsequence)\n",
        "                    #print(subsequence,[(verified_subsequence[0]-0),(int(sequence[-1][1])-verified_subsequence[1])])\n",
        "                    verified=True\n",
        "                    break\n",
        "            if(verified):\n",
        "                result.append(sequence[verified_subsequence[0]:verified_subsequence[1]])\n",
        "                if(verified_subsequence[0]-0)>0:\n",
        "                    subequence_to_check=sequence[0:verified_subsequence[0]]\n",
        "                    #since tokens before the starting position of the verified subsequence have already been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length-1))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                if(int(sequence[-1][1])-verified_subsequence[1])>0:\n",
        "                    subequence_to_check=sequence[(verified_subsequence[1]):]\n",
        "                    #since tokens following the end position of the verified subsequence have not been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                return result\n",
        "            else:\n",
        "                subsequence_length-=1\n",
        "        return result\n",
        "\n",
        "    # def flatten(self,mylist, outlist,ignore_types=(str, bytes, int, ne.NE_candidate)):\n",
        "    def flatten(self,mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "        if mylist !=[]:\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.flatten(item, outlist)\n",
        "                else:\n",
        "                    # if isinstance(item,ne.NE_candidate):\n",
        "                    #     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
        "                    #     item.reset_length()\n",
        "                    # else:\n",
        "                    if type(item)!= int:\n",
        "                        item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "            \n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "            # elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "            #     temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "            #     if(temp1):\n",
        "            #         temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "            #     temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "                #print(index, temp)\n",
        "            # elif \"...\" in word:\n",
        "            #     #print(\"here\")\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"...\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"...\")):\n",
        "            #             temp=list(map(lambda elem: elem+'...', temp))\n",
        "            #         else:\n",
        "            #            temp=list(map(lambda elem: elem+'...', temp[:-1]))+[temp[-1]]\n",
        "            #     # temp.append(temp1[-1])\n",
        "            # elif \"..\" in word:\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"..\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"..\")):\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp))\n",
        "            #         else:\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp[:-1]))+[temp[-1]]\n",
        "            #     #temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def get_Candidates(self, sequence, CTrie,flag):\n",
        "        #flag: debug_flag\n",
        "        candidateList=[]\n",
        "        left=0\n",
        "        start_node=CTrie\n",
        "        last_cand=\"NAN\"\n",
        "        last_cand_substr=\"\"\n",
        "        reset=False\n",
        "        right=0\n",
        "        while (right < len(sequence)):\n",
        "            # if(flag):\n",
        "            #     print(right)\n",
        "            if(reset):\n",
        "                start_node=CTrie\n",
        "                last_cand_substr=\"\"\n",
        "                left=right\n",
        "            curr_text=sequence[right][0]\n",
        "            curr_pos=[sequence[right][1]]\n",
        "            #normalized curr_text\n",
        "            curr=self.normalize(sequence[right][0])\n",
        "            cand_str=self.normalize(last_cand_substr+\" \"+curr)\n",
        "            cand_str_wPunct=(last_cand_substr+\" \"+curr_text).lower()\n",
        "            last_cand_sequence=sequence[left:(right+1)]\n",
        "            last_cand_text=' '.join(str(e[0]) for e in last_cand_sequence)\n",
        "            last_cand_text_norm=self.normalize(' '.join(str(e[0]) for e in last_cand_sequence))\n",
        "            if(flag):\n",
        "                print(\"==>\",cand_str,last_cand_text_norm)\n",
        "            if((cand_str==last_cand_text_norm)&((curr in start_node.path.keys())|(curr_text.lower() in start_node.path.keys()))):\n",
        "            #if (((curr in start_node.path.keys())&(cand_str==last_cand_text_norm))|(curr_text.lower() in start_node.path.keys())):\n",
        "                if flag:\n",
        "                    print(\"=>\",cand_str,last_cand_text)\n",
        "                reset=False\n",
        "                if (curr_text.lower() in start_node.path.keys()):\n",
        "                    if (start_node.path[curr_text.lower()].value_valid):\n",
        "                        last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                        last_cand_batch=start_node.path[curr_text.lower()].feature_list[-1]\n",
        "                        last_cand=last_cand_text\n",
        "                    elif(curr in start_node.path.keys()):\n",
        "                        if ((start_node.path[curr].value_valid)):\n",
        "                            last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                            last_cand=last_cand_text\n",
        "                            last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                        else:\n",
        "                            if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                                #print(\"hehe\",cand_str)\n",
        "                                right=left\n",
        "                                reset=True\n",
        "                    else:\n",
        "                        if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                            #print(\"hehe\",cand_str)\n",
        "                            right=left\n",
        "                            reset=True\n",
        "                elif ((start_node.path[curr].value_valid)&(cand_str==last_cand_text_norm)):\n",
        "                    # if flag:\n",
        "                    #     print(\"==\",last_cand_text)\n",
        "                    last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                    last_cand=last_cand_text\n",
        "                    last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                else:\n",
        "                    if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                        #print(\"hehe\",cand_str)\n",
        "                        right=left\n",
        "                        reset=True\n",
        "                if((curr_text.lower() in start_node.path.keys())&(cand_str==last_cand_text_norm)):\n",
        "                    start_node=start_node.path[curr_text.lower()]\n",
        "                    last_cand_substr=cand_str_wPunct\n",
        "                else:\n",
        "                    start_node=start_node.path[curr]\n",
        "                    last_cand_substr=cand_str\n",
        "            else:\n",
        "                #print(\"=>\",cand_str,last_cand_text)\n",
        "                if(last_cand!=\"NAN\"):\n",
        "                    candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "                    last_cand=\"NAN\"\n",
        "                    if(start_node!=CTrie):\n",
        "                        start_node=CTrie\n",
        "                        last_cand_substr=\"\"\n",
        "                        if curr in start_node.path.keys():\n",
        "                            # if(flag):\n",
        "                            #     print(\"here\",curr)\n",
        "                            reset=False\n",
        "                            if start_node.path[curr].value_valid:\n",
        "                                last_cand_text=curr_text\n",
        "                                last_cand_pos=curr_pos\n",
        "                                last_cand=last_cand_text\n",
        "                                last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                            left=right\n",
        "                            start_node=start_node.path[curr]\n",
        "                            last_cand_substr=curr\n",
        "                        else:\n",
        "                            reset=True\n",
        "                    else:\n",
        "                        reset=True\n",
        "                else:\n",
        "                    if(left<right):\n",
        "                        # if(flag):\n",
        "                        #     print(sequence[(left+1):(right+1)])\n",
        "                        #candidateList.extend(self.get_Candidates(sequence[(left+1):(right+1)], CTrie, flag))\n",
        "                        right=left\n",
        "                        # if(flag):\n",
        "                        #     print(\"++\",right)\n",
        "                    reset=True\n",
        "            right+=1\n",
        "        # if(flag):\n",
        "        #     print(last_cand)\n",
        "        if(last_cand!=\"NAN\"):\n",
        "            candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "        return candidateList\n",
        "\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        #self.data_frame_holder=self.data_frame_holder.append(df,ignore_index=True)\n",
        "        #self.data_frame_holder=self.data_frame_holder.reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def join_token_tuples(self,list_of_tuples):\n",
        "        #print(string.punctuation)\n",
        "        combined_str=(' '.join(tuple[0] for tuple in list_of_tuples)).lstrip(string.punctuation).rstrip(string.punctuation).strip()\n",
        "        combined_pos='*'.join(str(tuple[1]) for tuple in list_of_tuples)\n",
        "        combined_tuple=(combined_str,combined_pos,list_of_tuples[0][2],list_of_tuples[0][3],list_of_tuples[0][4],list_of_tuples[0][5],list_of_tuples[0][6])\n",
        "        return combined_tuple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def all_capitalized(self,candidate):\n",
        "        strip_op=candidate\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip())\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        prep_article_list=prep_list+article_list+self.phase2stopwordList\n",
        "        word_list=strip_op.split()\n",
        "        for i in range(len(word_list)):\n",
        "            word=word_list[i]\n",
        "            if((word[0].isupper())|(word[0].isdigit())):\n",
        "                continue\n",
        "            else:\n",
        "                if(word in prep_article_list):\n",
        "                    if (i!=0):\n",
        "                        continue\n",
        "                    else:\n",
        "                        return False\n",
        "                elif(word in conjoiner):\n",
        "                    continue\n",
        "                else:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def check_feature_update(self, candidate_tuple,non_discriminative_flag):\n",
        "        #print(candidate_tuple)\n",
        "        if(non_discriminative_flag):\n",
        "            return 7\n",
        "        candidateText=candidate_tuple[0]\n",
        "        position=candidate_tuple[1]\n",
        "        word_list=candidateText.split()\n",
        "        if candidateText.islower():\n",
        "            return 6\n",
        "        elif candidateText.isupper():\n",
        "            return 5\n",
        "        elif (len(word_list)==1):\n",
        "            #start-of-sentence-check\n",
        "            if self.all_capitalized(candidateText):\n",
        "                if(int(position[0])==0):\n",
        "                    return 4\n",
        "                else:\n",
        "                    return 2\n",
        "            else:\n",
        "                return 3\n",
        "        else:\n",
        "            if(self.all_capitalized(candidateText)):\n",
        "                return 2\n",
        "            else:\n",
        "                return 3\n",
        "\n",
        "\n",
        "    def update_Candidatedict(self,candidate_tuple,non_discriminative_flag,contextual_embedding_vector):\n",
        "        candidateText=candidate_tuple[0]\n",
        "        normalized_candidate=self.normalize(candidateText)\n",
        "        # print('adding:',normalized_candidate)\n",
        "\n",
        "        feature_list=[]\n",
        "        alt_feature_list=[] # this is for the reduced CandidateBase for the Alt_Classifier\n",
        "        if(normalized_candidate in self.CandidateBase_dict.keys()):\n",
        "            feature_list=self.CandidateBase_dict[normalized_candidate]\n",
        "            alt_feature_list=self.CandidateBase_dict_alt[normalized_candidate]\n",
        "        else:\n",
        "            # feature_list=[0]*9 # only syntax\n",
        "            # feature_list=[0]*777 # context embedding: 768\n",
        "\n",
        "            feature_list=[0]*309 # context embedding: 300\n",
        "            feature_list[0]=self.counter\n",
        "            feature_list[1]=len(normalized_candidate.split())\n",
        "\n",
        "            alt_feature_list=[0]*3\n",
        "            alt_feature_list[0]=self.counter\n",
        "            alt_feature_list[1]=len(normalized_candidate.split())\n",
        "\n",
        "        #syntax_feature to update\n",
        "        feature_to_update=self.check_feature_update(candidate_tuple,non_discriminative_flag)\n",
        "        feature_list[feature_to_update]+=1\n",
        "\n",
        "        # add up the context embedding features\n",
        "        # print(len(contextual_embedding_vector))\n",
        "        feature_list[8:-1]= np.add(feature_list[8:-1],contextual_embedding_vector).tolist()\n",
        "\n",
        "        #increment cumulative frequency\n",
        "        feature_list[-1]+=1\n",
        "        self.CandidateBase_dict[normalized_candidate]=feature_list\n",
        "\n",
        "        alt_feature_list[-1]+=1\n",
        "        self.CandidateBase_dict_alt[normalized_candidate]=alt_feature_list\n",
        "\n",
        "        # alt_points=[]\n",
        "        if(normalized_candidate in self.candidateEmbeddingDict.keys()):\n",
        "            self.candidateEmbeddingDict[normalized_candidate].append(contextual_embedding_vector)\n",
        "        else:\n",
        "            self.candidateEmbeddingDict[normalized_candidate]=[contextual_embedding_vector]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def extract(self,tweetBaseInput,CTrie,phase2stopwordList,new_or_old,train_classifier):\n",
        "\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #output_queue\n",
        "            self.data_frame_holder_OQ=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.not_reintroduced=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.CandidateBase_dict= {}\n",
        "\n",
        "            self.CandidateBase_dict_alt={}\n",
        "            self.candidateEmbeddingDict={}\n",
        "\n",
        "            self.ambiguous_candidate_distanceDict_prev={}\n",
        "            self.partition_dict={}\n",
        "            self.good_candidates=[]\n",
        "            self.bad_candidates=[]\n",
        "            self.ambiguous_candidates=[]\n",
        "\n",
        "            self.aggregator_incomplete_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.just_converted_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            #self.data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates'])\n",
        "            self.raw_tweets_for_others=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "\n",
        "            self.accuracy_tuples_prev_batch=[]\n",
        "            self.accuracy_vals=[]\n",
        "            \n",
        "            #frequency_w_decay related information\n",
        "            self.ambiguous_candidates_reintroduction_dict={}\n",
        "\n",
        "            #### other systems\n",
        "            self.accuracy_vals_stanford=[]\n",
        "            self.accuracy_vals_opencalai=[]\n",
        "            self.accuracy_vals_ritter=[]\n",
        "            self.accuracy_vals_neuroner=[]\n",
        "\n",
        "            self.number_of_seen_tweets_per_batch=[]\n",
        "        self.phase2stopwordList=phase2stopwordList\n",
        "        self.number_of_seen_tweets_per_batch.append(len(tweetBaseInput))\n",
        "\n",
        "\n",
        "        #data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase1_holder_holder=[]\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "        \n",
        "        #candidateBase_holder=[]\n",
        "\n",
        "        #this has to be changed to an append function since IPQ already has incomplete tweets from prev batch  \n",
        "        #print(len(tweetBaseInput))\n",
        "        #immediate_processingQueue = pd.concat([self.incomplete_tweets,TweetBase ])\n",
        "        #immediate_processingQueue.to_csv(\"impq.csv\", sep=',', encoding='utf-8')\n",
        "        \n",
        "\n",
        "\n",
        "        #print('In Phase 2',len(immediate_processingQueue))\n",
        "        #immediate_processingQueue=immediate_processingQueue.reset_index(drop=True)\n",
        "        combined_list_here=([]+list(cachedStopWords)+chat_word_list+day_list+month_list+article_list+prep_list)\n",
        "        combined_list_filtered=list(filter(lambda word: word not in (prep_list+article_list+month_list+self.phase2stopwordList), combined_list_here))\n",
        "        #--------------------------------------PHASE II---------------------------------------------------\n",
        "        for index, row in tweetBaseInput.iterrows():\n",
        "\n",
        "            #phase 1 candidates for one sentence\n",
        "            phase1_holder=[]\n",
        "\n",
        "            tweetText=str(row['TweetSentence'])\n",
        "            tweetWordList = row['tweetwordList']\n",
        "            sentID=str(row['sentID'])\n",
        "            tweetID=str(row['tweetID'])\n",
        "            phase1Candidates=str(row['phase1CandidatesWPositions'])\n",
        "            batch=int(row['entry_batch'])\n",
        "            contextual_embeddings_dict= {ind: embedding for ind, embedding in enumerate(row['contextual_embeddings'])}\n",
        "            \n",
        "            # print('====',tweetID,sentID)\n",
        "            # print('tweetWordList:',tweetWordList)\n",
        "            \n",
        "            non_discriminative_flag=False\n",
        "            phase1CandidatesList=[]\n",
        "\n",
        "            # print('phase1Candidates:',phase1Candidates)\n",
        "\n",
        "            if (phase1Candidates !='nan'):\n",
        "                phase1Raw=phase1Candidates.split(\"||\")\n",
        "                phase1Raw = list(filter(None, phase1Raw))\n",
        "\n",
        "\n",
        "                for entities_with_loc in phase1Raw:\n",
        "                    entity_to_store=entities_with_loc.split(\"::\")[0]\n",
        "                    #print(entity_to_store)\n",
        "                    position=entities_with_loc.split(\"::\")[1]\n",
        "                    #print(position)\n",
        "                    phase1_holder.append((entity_to_store,position))\n",
        "                    phase1_holder.clear()\n",
        "                    phase1CandidatesList.append(entity_to_store.lower())\n",
        "\n",
        "                phase1_holder_holder.append(copy.deepcopy(phase1_holder))\n",
        "                \n",
        "\n",
        "            else:\n",
        "                non_discriminative_flag=True\n",
        "                phase1_holder_holder.append([])\n",
        "\n",
        "            \n",
        "            # tweetWordList=self.getWords(tweetText)\n",
        "            # tweetWordList= [(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "            \n",
        "            tweetWordList_stopWords=list(filter (lambda word: ((((word[0].strip()).strip(string.punctuation)).lower() in combined_list_filtered)|(word[0].strip() in string.punctuation)|(word[0].startswith('@'))|(word[0].startswith('#'))), tweetWordList))\n",
        "\n",
        "            # phase 2 candidate tuples without stopwords for a sentence\n",
        "            c=[(y[0],str(y[1]),tweetID,sentID,'ne',batch,time) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "            #c=[(y[0],str(y[1])) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "\n",
        "            \n",
        "            sequences=[]\n",
        "            for k, g in groupby(enumerate(c), lambda element: element[0]-int(element[1][1])):\n",
        "                sequences.append(list(map(itemgetter(1), g)))\n",
        "\n",
        "            # print('phaseII candidates:')\n",
        "            ne_candidate_list=[]\n",
        "            for sequence in sequences:\n",
        "                seq_candidate_list=self.get_Candidates(sequence, CTrie,False)\n",
        "                if(seq_candidate_list):\n",
        "                    for candidate_tuple in seq_candidate_list:\n",
        "                        # print(candidate_tuple)\n",
        "\n",
        "                        # extract candidate token embeddings\n",
        "                        candidate_token_embeddings = torch.stack([contextual_embeddings_dict[int(position)] for position in candidate_tuple[1]])\n",
        "                        # print('candidate with token_embeddings:',candidate_tuple[0],candidate_token_embeddings.shape,len(candidate_tuple[1]))\n",
        "\n",
        "                        # !!necessary because this function during training receives [1,n,768] tensors that it squeezes; so might screw up 1-token sentences\n",
        "                        candidate_embedding = self.entity_phrase_embedder.getEmbedding(candidate_token_embeddings.unsqueeze(0))\n",
        "                        # print('candidate_embedding:',candidate_embedding.shape)\n",
        "\n",
        "                        #inserts into CandidateBase: Syntax and Context Feature Setting\n",
        "                        if not ((float(batch)<self.counter)&(candidate_tuple[-1]<self.counter)):\n",
        "                            self.update_Candidatedict(candidate_tuple,non_discriminative_flag,candidate_embedding.tolist())\n",
        "\n",
        "                    ne_candidate_list.extend(seq_candidate_list)\n",
        "\n",
        "            phase2_candidates=[self.normalize(e[0]) for e in ne_candidate_list]\n",
        "            phase2_candidates_unnormalized=[e[0] for e in ne_candidate_list]\n",
        "\n",
        "            phase2_candidates_holder.append(phase2_candidates)\n",
        "            phase2_unnormalized_candidates_holder.append(phase2_candidates_unnormalized)\n",
        "\n",
        "            dict1 = {'entry_batch':batch, 'tweetID':tweetID, 'sentID':sentID, 'TweetSentence':tweetText, 'phase1Candidates':phase1CandidatesList,'2nd Iteration Candidates':phase2_candidates,'2nd Iteration Candidates Unnormalized':phase2_candidates_unnormalized}\n",
        "\n",
        "            df_holder.append(dict1)\n",
        "            \n",
        "            #-------------------------------------------------------------------END of 1st iteration: RESCAN+CANDIDATE_UPDATION-----------------------------------------------------------\n",
        "\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        # # candidate_records=pd.read_csv('data/training.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_large.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidateBase_dict_filtered={}\n",
        "        # for candidate in self.CandidateBase_dict:\n",
        "        #     if(candidate in candidate_list):\n",
        "        #         candidateBase_dict_filtered[candidate] = self.CandidateBase_dict[candidate]\n",
        "\n",
        "        # # candidateBase_dict_filtered = self.CandidateBase_dict\n",
        "\n",
        "        # candidate_featureBase_DF_filtered=pd.DataFrame.from_dict(candidateBase_dict_filtered, orient='index')\n",
        "        # candidate_featureBase_DF_filtered.columns=self.candidateBaseHeaders[1:]\n",
        "        # candidate_featureBase_DF_filtered.index.name=self.candidateBaseHeaders[0]\n",
        "        # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered.reset_index(drop=False)\n",
        "        # # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered[candidate_featureBase_DF_filtered['cumulative'] >= 5.0]\n",
        "\n",
        "        # print('before:', len(self.CandidateBase_dict),'after:',len(candidateBase_dict_filtered))\n",
        "        \n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "        # candidate_featureBase_DF_filtered['class'] = candidate_featureBase_DF_filtered.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # # candidate_featureBase_DF_filtered['class'] = 0\n",
        "\n",
        "        # candidate_featureBase_DF_filtered.to_csv(\"data/candidate_train_records_large_300d.csv\", sep=',', encoding='utf-8')\n",
        "        \n",
        "        # return #comment out if not collecting records for classifier training\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        # #===============comment out if not training alt_classifier===============\n",
        "\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_large.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidateBase_dict_filtered={}\n",
        "        # for candidate in self.CandidateBase_dict:\n",
        "        #     if(candidate in candidate_list):\n",
        "        #         candidateBase_dict_filtered[candidate] = self.CandidateBase_dict_alt[candidate]\n",
        "\n",
        "        # candidate_featureBase_DF_filtered=pd.DataFrame.from_dict(candidateBase_dict_filtered, orient='index')\n",
        "        # candidate_featureBase_DF_filtered.columns=self.candidateBaseHeaders_alt[1:]\n",
        "        # candidate_featureBase_DF_filtered.index.name=self.candidateBaseHeaders_alt[0]\n",
        "        # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered.reset_index(drop=False)\n",
        "        # # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered[candidate_featureBase_DF_filtered['cumulative'] >= 5.0]\n",
        "\n",
        "        # print('before:', len(self.CandidateBase_dict_alt),'after:',len(candidateBase_dict_filtered))\n",
        "        \n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "        # candidate_featureBase_DF_filtered['class'] = candidate_featureBase_DF_filtered.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # candidate_featureBase_DF_filtered.to_csv(\"data/candidate_train_records_altClassifier.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "\n",
        "        # # # #=============== to do a fresh labeling with all generated candidates\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_altClassifier.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "\n",
        "        # candidate_featureBase_DF_alt = pd.DataFrame.from_dict(self.CandidateBase_dict_alt, orient='index')\n",
        "\n",
        "        # candidate_featureBase_DF_alt.columns = self.candidateBaseHeaders_alt[1:]\n",
        "        # candidate_featureBase_DF_alt.index.name = self.candidateBaseHeaders_alt[0]\n",
        "        # candidate_featureBase_DF_alt  = candidate_featureBase_DF_alt.reset_index(drop=False)\n",
        "\n",
        "        # # candidate_featureBase_DF_alt['class']=-1\n",
        "        # val_list=[]\n",
        "        # for index, row in candidate_featureBase_DF_alt.iterrows():\n",
        "        #     try:\n",
        "        #         val_list.append(candidate_records.loc[row.candidate]['class'])\n",
        "        #     except KeyError:\n",
        "        #         val_list.append(-1)\n",
        "        # candidate_featureBase_DF_alt['class']=val_list\n",
        "        # # candidate_featureBase_DF_alt['class'] = candidate_featureBase_DF_alt.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # candidate_featureBase_DF_alt.to_csv(\"data/candidate_train_records_altClassifier_new.csv\", sep=',', encoding='utf-8')\n",
        "        # return\n",
        "        # #=============== to do a fresh labeling with all generated candidates\n",
        "\n",
        "        # # #=============== to train the alt classifier\n",
        "        if(train_classifier):\n",
        "            self.entity_classifier_alt = EntityClassifierAlt('data/candidate_train_records_altClassifier_new.csv',True,self.device,self.candidateEmbeddingDict,self.CandidateBase_dict_alt)\n",
        "            return\n",
        "\n",
        "        # # #===============comment out if not training alt_classifier===============\n",
        "\n",
        "        # # #=============== for test run with regular Classifier ===============\n",
        "\n",
        "        # candidate_featureBase_DF = pd.DataFrame.from_dict(self.CandidateBase_dict, orient='index')\n",
        "        # candidate_featureBase_DF.columns = self.candidateBaseHeaders[1:]\n",
        "        # candidate_featureBase_DF.index.name = self.candidateBaseHeaders[0]\n",
        "        # candidate_featureBase_DF = candidate_featureBase_DF.reset_index(drop=False)\n",
        "        # return candidate_featureBase_DF,df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "        # #=============== for test run with alt Classifier ===============\n",
        "\n",
        "        candidate_featureBase_DF_alt = pd.DataFrame.from_dict(self.CandidateBase_dict_alt, orient='index')\n",
        "        candidate_featureBase_DF_alt.columns = self.candidateBaseHeaders_alt[1:]\n",
        "        candidate_featureBase_DF_alt.index.name = self.candidateBaseHeaders_alt[0]\n",
        "        candidate_featureBase_DF_alt  = candidate_featureBase_DF_alt.reset_index(drop=False)\n",
        "\n",
        "        self.entity_classifier_alt = EntityClassifierAlt('data/candidate_train_records_altClassifier_new.csv',False,self.device,self.candidateEmbeddingDict,self.CandidateBase_dict_alt)\n",
        "        return candidate_featureBase_DF_alt,df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "\n",
        "        ## self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        ## self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets_for_current_batch)\n",
        "\n",
        "\n",
        "    def finish(self):\n",
        "        return self.accuracy_vals\n",
        "\n",
        "    def finish_other_systems(self):\n",
        "        stanford_f1=[]\n",
        "        stanford_precision=[]\n",
        "        stanford_recall=[]\n",
        "        print(\"*****************************************STANFORD***********************\")\n",
        "        for i in self.accuracy_vals_stanford:\n",
        "            stanford_f1.append(i[0])\n",
        "            stanford_precision.append(i[1])\n",
        "            stanford_recall.append(i[2])\n",
        "            # print(i)\n",
        "        print('stanford_f1:', stanford_f1)\n",
        "        print('stanford_precision:', stanford_precision)\n",
        "        print('stanford_recall:', stanford_recall)\n",
        "\n",
        "        print(sum(stanford_f1)/len(stanford_f1))\n",
        "        print(sum(stanford_precision)/len(stanford_precision))\n",
        "        print(sum(stanford_recall)/len(stanford_recall))\n",
        "\n",
        "        print(\"*****************************************Opencalai***********************\")\n",
        "        opencalai_f1=[]\n",
        "        opencalai_precision=[]\n",
        "        opencalai_recall=[]\n",
        "        for i in self.accuracy_vals_opencalai:\n",
        "            opencalai_f1.append(i[0])\n",
        "            opencalai_precision.append(i[1])\n",
        "            opencalai_recall.append(i[2])\n",
        "        print('opencalai_f1:', opencalai_f1)\n",
        "        print('opencalai_precision:', opencalai_precision)\n",
        "        print('opencalai_recall:', opencalai_recall)\n",
        "\n",
        "        print(sum(opencalai_f1)/len(opencalai_f1))\n",
        "        print(sum(opencalai_precision)/len(opencalai_precision))\n",
        "        print(sum(opencalai_recall)/len(opencalai_recall))\n",
        "        print(\"*****************************************Ritter***********************\")\n",
        "        ritter_f1=[]\n",
        "        ritter_precision=[]\n",
        "        ritter_recall=[]\n",
        "        for i in self.accuracy_vals_ritter:\n",
        "            ritter_f1.append(i[0])\n",
        "            ritter_precision.append(i[1])\n",
        "            ritter_recall.append(i[2])\n",
        "        print('ritter_f1:', ritter_f1)\n",
        "        print('ritter_precision:', ritter_precision)\n",
        "        print('ritter_recall:', ritter_recall)\n",
        "\n",
        "        print(sum(ritter_f1)/len(ritter_f1))\n",
        "        print(sum(ritter_precision)/len(ritter_precision))\n",
        "        print(sum(ritter_recall)/len(ritter_recall))\n",
        "        print(\"*****************************************Neuroner***********************\")\n",
        "        neuroner_f1=[]\n",
        "        neuroner_precision=[]\n",
        "        neuroner_recall=[]\n",
        "        for i in self.accuracy_vals_neuroner:\n",
        "            neuroner_f1.append(i[0])\n",
        "            neuroner_precision.append(i[1])\n",
        "            neuroner_recall.append(i[2])\n",
        "        print('neuroner_f1:', neuroner_f1)\n",
        "        print('neuroner_precision:', neuroner_precision)\n",
        "        print('neuroner_recall:',neuroner_recall)\n",
        "\n",
        "        print(sum(neuroner_f1)/len(neuroner_f1))\n",
        "        print(sum(neuroner_precision)/len(neuroner_precision))\n",
        "        print(sum(neuroner_recall)/len(neuroner_recall))\n",
        "\n",
        "        return (self.accuracy_vals_stanford,self.accuracy_vals_opencalai,self.accuracy_vals_ritter,self.accuracy_vals_neuroner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY3zs23wC6_3"
      },
      "source": [
        "## **External Run of BERTweet Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5tTI3fIl9yC"
      },
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"vinai/bertweet-base\"\n",
        "batch_size = 32\n",
        "# set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTip5Uxs29P8"
      },
      "source": [
        "# # reading the training set\n",
        "# f = open(\"wnut17_train.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_train = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_train))\n",
        "# df_train.to_csv(\"wnut17train.csv\", sep=',', encoding='utf-8',index=False)\n",
        "\n",
        "# # reading the validation set\n",
        "# f = open(\"wnut17_validation.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_validation = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_validation))\n",
        "# df_validation.to_csv(\"wnut17validation.csv\", sep=',', encoding='utf-8',index=False)\n",
        "\n",
        "# # reading the test set\n",
        "# f = open(\"wnut17_test.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_test = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_test))\n",
        "# df_test.to_csv(\"wnut17test.csv\", sep=',', encoding='utf-8',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlQEqoIJqo4l"
      },
      "source": [
        "# class TweetNERDataset(torch.utils.data.Dataset):\n",
        "\n",
        "#     def __init__(self, input, output):\n",
        "\n",
        "#         # print(data[0])\n",
        "#         # self.data = np.asarray(data)\n",
        "#         # self.output = np.asarray(output)\n",
        "\n",
        "#         datasets = load_dataset(\"wnut_17\")\n",
        "\n",
        "#         self.input = input\n",
        "#         self.output = output\n",
        "\n",
        "#         print(type(self.input),type(self.output))\n",
        "\n",
        "#     def prepare_input():\n",
        "        \n",
        "\n",
        "#     def tokenize_and_align_labels(self, example):\n",
        "        \n",
        "#         tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "#         inputId_to_token_dict={}\n",
        "#         for index, token in enumerate(example[\"tokens\"]):\n",
        "#             values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "#             for value in values:\n",
        "#                 try:\n",
        "#                     inputId_to_token_dict[value].append(index)\n",
        "#                 except KeyError:\n",
        "#                     inputId_to_token_dict[value]=[index]\n",
        "#         labels=[]\n",
        "#         for inputID in tokenized_input['input_ids']:\n",
        "#             try:\n",
        "#                 index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "#                 index_to_address=index_list.pop(0)\n",
        "\n",
        "#                 label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "#                 # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "#                 labels.append(label)\n",
        "#                 inputId_to_token_dict[inputID]=index_list\n",
        "#             except KeyError:\n",
        "#                 labels.append(-100)\n",
        "\n",
        "#         assert (len(tokenized_input['input_ids']) == len(labels))\n",
        "#         tokenized_input['labels']=labels\n",
        "        \n",
        "#         return tokenized_input\n",
        "\n",
        "#     def __len__(self):\n",
        "#         assert len(self.input) == len(self.output)\n",
        "#         return len(self.input)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         X = self.input[idx]\n",
        "#         y = self.output[idx]\n",
        "#         return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "b58978082f3240e4b37e171069ca16a1",
            "d37e506ecf80405f92ebd50d619d40b5",
            "e89d6219dc6543c98d8d5e4dbc43b8d7",
            "4ce57f43d9aa48b2bc7f57a373078133",
            "1a9c81d1d0e94f6387c5600dc9163c07",
            "8800f85f1d6f43c99ded82337e8edcb6",
            "30c3ab9f76a24f3f80a2f38eebce120a",
            "0a4cdede55494b1ebd1f53e2521088b8",
            "296311dffe7b4dbca19a45afe9fc9279",
            "0233b0dbf26245fdba0bfbfac2e64bc9",
            "720470eb591c4c3cb0f5d8c20b0093d4",
            "7ff87e8a208e45c499d0d93c5c7a83c9",
            "1cd5e91d89a14a5da207fbe088621f3a",
            "e85ff5b5036d4bd78e087f902231f0a3",
            "efe1b6d43c9b4d0e914c0eb0169ea56d",
            "11c7ed478a714671a359e06ee0e96ed6",
            "fb92d806a1fd4203a74d201fa40c63a6",
            "7586b918ec9944caaea323aa06a7adb9",
            "4c452a88fc2b4beea42fa8b63c4dcb7c",
            "d0a21d564fde46e49ee85c091e838280",
            "0ef80a280a3342b5927f346377c8d8c0",
            "7d63bf8db2a54304b6a32cc6eac9d453",
            "b5b0a83aed2b46ce82c22224142e9fc6",
            "4464da1c5d644a208fb051c7e3ca04dc",
            "b1a5790666e84a7784c1ef185397d920",
            "0392174d3743419dbcffc11b89f7d0c4",
            "2bd75ddc68a34449a5b6bda88ef64ac7",
            "5b76ebd709a64215ad0efce0b2e587cc",
            "6433580aec374bf1a98ba8cb8aa88218",
            "f59074ae6baf41b29a9ff0af6436dbd8",
            "938b54b019f04b9e95cafd878d0b1e05",
            "7087a331b3c34b2fa45a92a27cbbb6c0",
            "fecbd237361f4e458a56edbdaca0aed6",
            "daa9ccca1cb944999802a1ddc262f57c",
            "21c200891aff49c8a9dafb756a33e283",
            "8cf9e8fd9219430b8cb82ffc856ee495",
            "5d758e0e598e4c4c94b1b341f75fbf69",
            "4f751bd50b924aba992ccb0c1b6cb0b2",
            "b37d079e4e1a424b896dd46c1a1b9e22",
            "7ff00446f5754330b3370d8604d39304",
            "aeb35acd796e4a00a72e554d870ae070",
            "23fafc05458a4e08bde49e5bec871652",
            "e8fc1cd5b3424e0890705be08a038c50",
            "5345ded755c74ceaaa24ab5cd0a3a6ed",
            "40f05371ce9e4e54aae13db8138a772f",
            "ec27633defb44fe5b2204c3f6fc29457",
            "dc62479acd514814850b4d54c535d221",
            "55e9e1032d654d86ae2853d8496e9f45",
            "dd517a47260344458dc14cf955c74d75",
            "23721ecae5434f83acc28d19a14157f3",
            "8597679268444a8e9ff4d3aaded9c056",
            "9842c48ac08a4cc4945e64a947b72a3c",
            "427b8e0a331a4b20a7c8fe3ecfc790c2",
            "cc188f774ef349e69e0e435da8570792",
            "9e98ddbf46e646279c178b1e1834000a",
            "4891df1fda81478d88718d7342f9fa52",
            "bd4ebc1134c14726913d4daf28fef999",
            "b3a121ad8fca422fbb8060023a6b7958",
            "e5b637ce8f27451ba9c484b905d68397",
            "7ae6cc2e4cd243239337d393d2f9f81d",
            "170977bc0f244ed8a6cfcceb245aaa0e",
            "235c6f43e7cf488ea0ff584b6e9dfe2a",
            "55b470cbef24434c82baae5d8cfc8214",
            "0558bd426e0c459f9fffff75dce7c2e3",
            "77ffcc54927842bbb7dc180ae8c5ed5c",
            "2070595f6b464b849e6d3158656db65f",
            "56afbabc75b54de1809dd5517f3be060",
            "8920e243c40848d8ae9ab00addbdb5c6",
            "deb2bdb228064c95baf21fd35cdc8170",
            "cf8798aed733458bbb273f3336f87f9d",
            "2e52b6f76b104c8ea326b6b73c589ab9",
            "b91063c7d40144158a4fe9f0c3efe4e3",
            "e978a9e11def4eb2bd01e271b99cb53a",
            "c0200f8c4d674506a22876bc7e274d43",
            "3c12ff3d88904c7d95e6cdddbd0efa7c",
            "444957267a624f34859f2e6191b8c4d1",
            "e5e2c948011b4e49a84d67051b1f7ccc",
            "8bb6cc163e874b6789bbe30c92f1ad7d",
            "ff174c2cdb35432787787967fbece211",
            "8de4f1985b9b40609784eea7c398ffc0",
            "3ddb63a6bf2c4b17b4f573e46394f4ce",
            "73c8dd7ce9754492b48e0d32cafbf2a1",
            "d7b10c7001f840288dbbee9e08f364d0",
            "14c72e78bd074d98a058f80cb11cd8a2",
            "531a5f270bff4f4e9b002f3dd3b34a7c",
            "2ae088d3e94f4b5390d8e09899cfe4c7",
            "32ef384732644d71958298a8712b0eaf",
            "d821ad90189846d9bc96f68ed7332b45",
            "ff92f61d5aae40d0a18c3a18df5da959",
            "35a98736a7a44128bbbf22de205f5758",
            "d82f3ca2a4584425a340531d33631029",
            "0d693e0827944840a305eeb9f62bdf91",
            "1d6abd6125e94fc28705610617158361",
            "9ffc0a35f0594a37a4b2f08ce53fc520",
            "88937c5d220c4a7cb8966bd0ea1f912c",
            "7ec94d706292439283ec14a1f1dfd255",
            "517f74c01d614157a189f3ef8754e8e1",
            "bf4b0ca1104a4aff853754e0e351ddcd",
            "bd3685ecaf824583bcb836f8d59998cc",
            "1f430146b9694fe5ac8238725277ad29",
            "eac8af198a6f4c28855a4a3e2c4b304e",
            "6590fd96609c41e9892079643de2a596",
            "3066e3a61b9448e4b25fbef0b9f4ab49",
            "c89addbaa7554a0a96b52359d855e359",
            "7db5611d00e0452e8e0843ddd0071da9",
            "0a0f98716019434fa198362b4703ae97",
            "3786f8393bfa494eaed13088d4fe189e",
            "e15951575a8342cdbac0928001acc456",
            "bce3a9f59aef4e3e88f6542e6b535ed3",
            "a0bd4c7a4f674d038315a37f395482b6",
            "d9c6974e42894cd49edcaac074620468",
            "88db54b1f3394a66a7a9213907d1eebc",
            "69ffab3da94e4664b2b19690e88932da",
            "a4aed5e717cd4b40a884685c312d4890",
            "21802c2ac10d4e73b72b8407835de26e",
            "3d5fdb8601c145dfb3e1460735ba59dd",
            "5a1fa38d2aa64566b801f5bf076d2390",
            "9e6c8ac616b142e794ab5a3ab5166af8",
            "8f8450875c574907839dcb81a7e4caee",
            "c931c293f29f4b14a373e6eb4aa032c1",
            "c36762ac548f432fbe57a7d627fa2082"
          ]
        },
        "id": "WDmC8lnI-99G",
        "outputId": "138dd233-0c1f-4674-9c1b-130cbf8990fd"
      },
      "source": [
        "\n",
        "# train_dataset = load_dataset('csv', data_files=\"wnut17train.csv\")\n",
        "# validation_dataset = load_dataset('csv', data_files='wnut17validation.csv')\n",
        "# test_dataset = load_dataset('csv', data_files=\"wnut17test.csv\")\n",
        "\n",
        "datasets = load_dataset(\"wnut_17\")\n",
        "\n",
        "# datasets = load_dataset(\"conll_2003\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b58978082f3240e4b37e171069ca16a1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ff87e8a208e45c499d0d93c5c7a83c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset wnut_17/wnut_17 (download: 782.18 KiB, generated: 1.66 MiB, post-processed: Unknown size, total: 2.43 MiB) to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5b0a83aed2b46ce82c22224142e9fc6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daa9ccca1cb944999802a1ddc262f57c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/185k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40f05371ce9e4e54aae13db8138a772f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4891df1fda81478d88718d7342f9fa52",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/66.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56afbabc75b54de1809dd5517f3be060",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bb6cc163e874b6789bbe30c92f1ad7d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff92f61d5aae40d0a18c3a18df5da959",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f430146b9694fe5ac8238725277ad29",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9c6974e42894cd49edcaac074620468",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaoyxjMX8bTP"
      },
      "source": [
        "# training_set = TweetNERDataset(datasets[\"train\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z17jPrtnKuu",
        "outputId": "45d7b7ff-6899-4068-cd37-1de70fe54b0f"
      },
      "source": [
        "# train_dataset\n",
        "datasets\n",
        "# train_dataset[\"train\"].features[f\"ner_tags\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 3394\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1009\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1287\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "96b969775a994a98b7785ab242c12578",
            "d385f83df47b4d46b3463134666f9e10",
            "0095c1dccfe34082ac5c68aa4e26fd94",
            "4fe15cc87f664a9390cdc554d9f8edc7",
            "53492322777146e99fc92bf0644e5ba9",
            "bf8b344e9d564da384bf9ed3db42d85c",
            "c312c92ddec849db8e996335fc1c3268",
            "a706be44d88b43f59716759a18795fae",
            "6f34bfb41f7f4020a2898908bd7dd102",
            "6032d7bc68284db1b0f1236fe2dbc2dc",
            "5d09e9f401ef4ea7bad910be3f650008",
            "7f623a0a41e14ed8ac43074d41d5a397",
            "cae1096a39674bd09188e271b933c7ab",
            "fff98770b1b344668751a9d30764752a",
            "62bd37b23955464587bd2f8ccfbb8a04",
            "a5a4b98834f443999d212cc44b0c3b79",
            "78ec5a48b3b54924918c8d1362b77b39",
            "8a25b4fe0af54596905605ac4d5a6fc0",
            "dda2a39081c84298bcd116b2f910b0d4",
            "718a318d654546d4a07522e47aaf6975",
            "c7566c193cea431cbd633665e5ee9721",
            "ad1648f6ea2545e297b6ef4d6b2878d0",
            "5bb91d4d3ac34e1b9490e6b680c5166f",
            "0aa4a002302b4879809a8abc8e4331a3",
            "5977b385e038422bb8dfcae39af1792d",
            "c16645e62d2f47f18b42a3ba2d25ff10",
            "25400abde7524a7b9a2252a5c6f84605",
            "8a6e9a128eac487f91661fdeea2a67b3",
            "c4698846bac14b4c902c9797ddb291f9",
            "553a8742afce4dd7a43b1ba9b0d956d8",
            "d146a8044fe14e1b8607ec33e28fac07",
            "7ad12689a5b841bbbd572ad644fdd91e",
            "b5801355a9a74f8aa78d2a417602f858"
          ]
        },
        "id": "bkvoRJbVbIeX",
        "outputId": "8fff4786-73e8-4c33-8faa-1b6f8b369d3d"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "label_all_tokens = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96b969775a994a98b7785ab242c12578",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f623a0a41e14ed8ac43074d41d5a397",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/824k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bb91d4d3ac34e1b9490e6b680c5166f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csG7slfdkVwg",
        "outputId": "0fd50f12-301f-4ebb-ec2e-4683c974739e"
      },
      "source": [
        "print(datasets['train'][-1])\n",
        "print(type(datasets['train']))\n",
        "print(type(datasets['train'][-1]))\n",
        "\n",
        "# last_record_dict = {'id': '3394', 'ner_tags': [1, 2, 0, 1, 2, 0, 0], 'tokens': ['Bill', 'Nye', 'explains', 'Global', 'Warming', 'to', 'adults']}\n",
        "# datasets['train'].append(last_record_dict)\n",
        "\n",
        "# last_id = '3394'\n",
        "# datasets['train']['id'].append(last_id)\n",
        "\n",
        "# last_ner_tags = [1, 2, 0, 1, 2, 0, 0]\n",
        "# datasets['train']['ner_tags'].append(last_ner_tags)\n",
        "\n",
        "# last_tokens = ['Bill', 'Nye', 'explains', 'Global', 'Warming', 'to', 'adults'] #Bill Nye explains Global Warming to adults\n",
        "# datasets['train']['tokens'].append(last_tokens)\n",
        "\n",
        "# example = datasets[\"train\"][4]\n",
        "# print(example[\"tokens\"])\n",
        "# tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "# # input_ids = tokenizer.encode(example[\"tokens\"])\n",
        "# # tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "# print(tokenized_input)\n",
        "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "# print(tokens)\n",
        "# # len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])\n",
        "# print({x : tokenizer.encode(x, add_special_tokens=False) for x in example[\"tokens\"]})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '3393', 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Sat', 'Jan', '10', '2015', '12:46', ':', '39', 'GMT+0000', '(', 'UTC', ')']}\n",
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5n31Mh1_l04"
      },
      "source": [
        "# print(datasets['train'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyth64PVbIht"
      },
      "source": [
        "expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location',\n",
        "                     8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "BIO_dict={'O':0,'B':1,'I':2}\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "        \n",
        "    tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "    inputId_to_token_dict={}\n",
        "    for index, token in enumerate(example[\"tokens\"]):\n",
        "        values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "        for value in values:\n",
        "            try:\n",
        "                inputId_to_token_dict[value].append(index)\n",
        "            except KeyError:\n",
        "                inputId_to_token_dict[value]=[index]\n",
        "    labels=[]\n",
        "    for inputID in tokenized_input['input_ids']:\n",
        "        try:\n",
        "            index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "            index_to_address=index_list.pop(0)\n",
        "\n",
        "            label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "            # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "            labels.append(label)\n",
        "            inputId_to_token_dict[inputID]=index_list\n",
        "        except KeyError:\n",
        "            labels.append(-100)\n",
        "\n",
        "    assert (len(tokenized_input['input_ids']) == len(labels))\n",
        "    tokenized_input['labels']=labels\n",
        "    \n",
        "    return tokenized_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "f38bb9e35bde4ffba605fe9d3b636c72",
            "f7765100db584589804f3dee6a0baff0",
            "62a47a6f62044f06afff56d113a3e43b",
            "7f6f579e529c4e058a47e3fabb6d27dd",
            "09cbb0b0901f4d66afa6cfc28612cd9b",
            "9cd56cce302d44f19ee1994e72058672",
            "d3ffa76468494a21ab52207e62cf55aa",
            "cd333546f6a44a53ba9ffb3beb5c9a59",
            "553e1c577ea644b090662eab7c5e39b1",
            "e80c94d1c8734757a14700cea4b31c4b",
            "61bbf9fa76c3456fa062532b6fe8ea7a",
            "85c74c61e91946ceb7dc62531e952699",
            "ed822e59b45d4a5c98dfce10b4e4af70",
            "c5d6f44644384395817d53d75c77964c",
            "14f57d4d7f034b548714852654b345b6",
            "6105d45936064347b0a024d26e884a87",
            "4bd560eb9da64b1d978a00e73dabf5a4",
            "da04f81baf6642e692f97db54e6f5626",
            "6de7505cb8494025960313b847974fa4",
            "58cbceb0f2ac4f06a9d38a00eaa76430",
            "7b2a2b4060b74cfd99337967528be074",
            "1ae39bdaaaa849a3b15dc28b72e29feb",
            "45e1ee5cd7ab499ebbdc3dffc45f8274",
            "7432a8a16bfa4532ad23ec1eb659916f",
            "3c5835d9159743a2b5cea4f30bc216c1",
            "84ff79dd745f48ec93699c456325926e",
            "ed1b39eee5ed4872986e5135398bd64c",
            "f21840f7a6864c4f9e3a683290db8deb",
            "a1d5a324dba84780b9ecc05010b2208e",
            "2e2c73ea81b544b6894a826488cf50a8",
            "b6ac753ae6224b9b86dec91787e627a5",
            "1467fbad2aa34957bdbd2f1b4ef4ddca",
            "397b3c99295d48d680cfcef492154659"
          ]
        },
        "id": "6q5SzJpD4cId",
        "outputId": "a974cf20-e4b2-49f8-c59f-d210f335baa4"
      },
      "source": [
        "# tokenize_and_align_labels(datasets['train'][2])\n",
        "tokenized_datasets = datasets.map(tokenize_and_align_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f38bb9e35bde4ffba605fe9d3b636c72",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3394 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85c74c61e91946ceb7dc62531e952699",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1009 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45e1ee5cd7ab499ebbdc3dffc45f8274",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1287 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XthRN5iUh_Kq",
        "outputId": "58d87b92-44b2-4416-a519-b0b9dda23869"
      },
      "source": [
        "tokenized_datasets['train']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3394\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz4Y6EFnvB_e",
        "outputId": "088a9052-0d18-4004-e4cd-e35b3ad3f57f"
      },
      "source": [
        "print(tokenized_datasets['train'][-1])\n",
        "print(type(tokenized_datasets['train']))\n",
        "print(type(tokenized_datasets['train'][-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [0, 5391, 4427, 251, 753, 5288, 3085, 22, 2997, 8541, 335, 11122, 57, 8860, 60, 2], 'id': '3393', 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Sat', 'Jan', '10', '2015', '12:46', ':', '39', 'GMT+0000', '(', 'UTC', ')']}\n",
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLWFWwewCBjL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3d329dc879044cc590af8dd5ad722a6c",
            "6b02c1c6231a43869c91ed9ee76c0a16",
            "785802b23d954f5dba3b220f08a1ef0f",
            "f42c36307a1c4e13af95b28c7b2a06d6",
            "de1d0543e800472180f8e1126df0bf5c",
            "b19f47e6dc184e498177d312e05c8820",
            "757f03c8803b4bd2bf90a5cc25f9844f",
            "8f3134d01fba412d879b3614d4273e1c",
            "d1bc429bf18d41e4bd0a4b4392a6adc0",
            "70c213468f254fc9bcc37e7fa5ff08e7",
            "738aa81909584903809c835a91d5b98f"
          ]
        },
        "outputId": "d479560c-caa2-4ced-f47c-0561f988ee6a"
      },
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d329dc879044cc590af8dd5ad722a6c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhh4V16GoHKx"
      },
      "source": [
        "# tokenized_datasets['train'].features[\"labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNzwMflzkDi3",
        "outputId": "15ee3020-a509-4a58-d1d5-520736812784"
      },
      "source": [
        "# label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "\n",
        "label_list = ['O','B','I']\n",
        "\n",
        "print(label_list)\n",
        "print(len(label_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B', 'I']\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV4eL-hsIafT"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # print(p.shape)\n",
        "    output, labels = p\n",
        "\n",
        "    # print(len(predictions))\n",
        "    # print(predictions[0].shape)\n",
        "    # for elem in predictions[1]:\n",
        "    #   print(elem.shape)\n",
        "\n",
        "    predictions, _ = output\n",
        "    \n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNcDvqvUkgkc"
      },
      "source": [
        "# config = AutoConfig.from_pretrained(model_checkpoint, output_hidden_states=True)\n",
        "# config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817,
          "referenced_widgets": [
            "01a82f8c427e4598b2ae3242cc02bd56",
            "9e2f783123cf4f15aeeb2aee771f036a",
            "46eaf8fc331f45eca9c48913217ef99c",
            "84e936e3aae34390bf79d2b89aa3050d",
            "0fb5fa84cb7d4a8d8d47f8cc86541bf8",
            "dee7ba04afae41a48200a492682467a2",
            "576c0fdd439f4cf5a798b28a83e4f989",
            "24a5b8085d184e92b5f430115cf825d4",
            "37759e89fffd490c9f69ea387874e0cd",
            "edd9f91bdaea494e8c6777ce2a1344cb",
            "b116730687d64e0ca3121b4445d9217c"
          ]
        },
        "id": "H9akEbBY2ZZl",
        "outputId": "8b22a194-db88-48dc-fb15-7bae4fe820cf"
      },
      "source": [
        "alt_model = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(label_list))\n",
        "alt_model.config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01a82f8c427e4598b2ae3242cc02bd56",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/517M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
              "  \"architectures\": [\n",
              "    \"RobertaForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 130,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"output_hidden_states\": true,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
              "  \"transformers_version\": \"4.11.3\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 64001\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRnD-mNZUaia",
        "outputId": "a144c62d-c3aa-41a9-e52d-3fa050e3d4cd"
      },
      "source": [
        "batch_size=32\n",
        "alt_training_args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "alt_training_args"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=1,\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_pin_memory=True,\n",
              "ddp_find_unused_parameters=None,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "do_eval=True,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_steps=None,\n",
              "evaluation_strategy=IntervalStrategy.EPOCH,\n",
              "fp16=False,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "gradient_accumulation_steps=1,\n",
              "gradient_checkpointing=False,\n",
              "greater_is_better=None,\n",
              "group_by_length=False,\n",
              "hub_model_id=None,\n",
              "hub_strategy=HubStrategy.EVERY_SAVE,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "label_names=None,\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=2e-05,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=False,\n",
              "local_rank=-1,\n",
              "log_level=-1,\n",
              "log_level_replica=-1,\n",
              "log_on_each_node=True,\n",
              "logging_dir=test-ner/runs/Oct22_23-38-02_d80d81eb0c1b,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=500,\n",
              "logging_strategy=IntervalStrategy.STEPS,\n",
              "lr_scheduler_type=SchedulerType.LINEAR,\n",
              "max_grad_norm=1.0,\n",
              "max_steps=-1,\n",
              "metric_for_best_model=None,\n",
              "mp_parameters=,\n",
              "no_cuda=False,\n",
              "num_train_epochs=3,\n",
              "output_dir=test-ner,\n",
              "overwrite_output_dir=False,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=32,\n",
              "per_device_train_batch_size=32,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=False,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "remove_unused_columns=True,\n",
              "report_to=['tensorboard'],\n",
              "resume_from_checkpoint=None,\n",
              "run_name=test-ner,\n",
              "save_on_each_node=False,\n",
              "save_steps=500,\n",
              "save_strategy=IntervalStrategy.STEPS,\n",
              "save_total_limit=None,\n",
              "seed=42,\n",
              "sharded_ddp=[],\n",
              "skip_memory_metrics=True,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_legacy_prediction_loop=False,\n",
              "warmup_ratio=0.0,\n",
              "warmup_steps=0,\n",
              "weight_decay=0.01,\n",
              "xpu_backend=None,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aACODJ6AWVDG"
      },
      "source": [
        "alt_trainer = Trainer(\n",
        "    alt_model,\n",
        "    alt_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxw-OLa6mitu"
      },
      "source": [
        "**Model output is a tuple, when all hidden states are returned, i.e. output_hidden_states =True in config: (output, hidden-layers)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "-Uryn4hHX06d",
        "outputId": "748596d6-845b-45d0-dcd8-a4ca37e13cd9"
      },
      "source": [
        "alt_trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running training *****\n",
            "  Num examples = 3394\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 321\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='321' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [321/321 03:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.179689</td>\n",
              "      <td>0.603113</td>\n",
              "      <td>0.515378</td>\n",
              "      <td>0.555805</td>\n",
              "      <td>0.943622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.150795</td>\n",
              "      <td>0.751064</td>\n",
              "      <td>0.586866</td>\n",
              "      <td>0.658889</td>\n",
              "      <td>0.953315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.154067</td>\n",
              "      <td>0.753372</td>\n",
              "      <td>0.650042</td>\n",
              "      <td>0.697903</td>\n",
              "      <td>0.956348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1009\n",
            "  Batch size = 32\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1009\n",
            "  Batch size = 32\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1009\n",
            "  Batch size = 32\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=321, training_loss=0.12012283230124976, metrics={'train_runtime': 185.8085, 'train_samples_per_second': 54.798, 'train_steps_per_second': 1.728, 'total_flos': 208934163215748.0, 'train_loss': 0.12012283230124976, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU3-mGQJICbx"
      },
      "source": [
        "# tokenizer.save_pretrained('test-ner/')\n",
        "# alt_model.save_pretrained('test-ner/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRvhsCdn-NxN"
      },
      "source": [
        "## **Extracting Embeddings to train the Phrase Embedder on STS Dataset (NOT REQD TO RUN)-- unless training the phrase Embedder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfxtWYA50eDp"
      },
      "source": [
        "# sys.path.insert(0,'/content/gdrive/My Drive/BERTweet-ner')\n",
        "# print(sys.path)\n",
        "\n",
        "# sys.path.append('/content/gdrive/My Drive/BERTweet-ner')\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRg-uD1iS5Mm"
      },
      "source": [
        "class PhraseEmbeddingDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, input, output):\n",
        "\n",
        "        # print(data[0])\n",
        "        # self.data = np.asarray(data)\n",
        "        # self.output = np.asarray(output)\n",
        "\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "\n",
        "        print(type(self.input),type(self.output))\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.input) == len(self.output)\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.input[idx]\n",
        "        y = self.output[idx]\n",
        "        return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cqFnxb0l-1J"
      },
      "source": [
        "def get_STS_Data_embeddings(dataset):\n",
        "    predictions=[]\n",
        "    tokenized_sentences=[]\n",
        "    count=0\n",
        "    entity_embeddings=[]\n",
        "    with torch.no_grad():\n",
        "        for record in dataset:\n",
        "            # print(record)\n",
        "            # record=record.lower()\n",
        "            tokenized_input=tokenizer(record)\n",
        "            initial_input_ids = torch.tensor([tokenizer.encode(record)])\n",
        "            token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in record.split()}\n",
        "            input_ids = initial_input_ids.to(device)\n",
        "            tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "            # output = model(input_ids)\n",
        "\n",
        "            output = alt_model(input_ids)\n",
        "\n",
        "            if(count<5):\n",
        "                print(len(output.hidden_states))\n",
        "                print(output.hidden_states[-1].shape)\n",
        "            \n",
        "            token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "\n",
        "            prediction = (torch.argmax(output.logits, axis=2))\n",
        "            prediction = prediction.cpu().numpy().reshape(-1)\n",
        "            prediction_labels=[label_list[l].split('-')[0] for l in prediction]\n",
        "            \n",
        "#             print(token_embeddings.shape)\n",
        "            prediction_labels, entity_aware_embeddings=collate_token_label_embedding(record.split(), token_dict, prediction_labels[1:-1],token_embeddings)\n",
        "            \n",
        "            assert (len(prediction_labels)==len(record.split()))\n",
        "            assert (len(entity_aware_embeddings)==len(record.split()))\n",
        "\n",
        "            predictions.append(prediction_labels)\n",
        "            entity_embeddings.append(torch.stack(entity_aware_embeddings))\n",
        "            tokenized_sentences.append(token_dict.keys())\n",
        "\n",
        "            if(count<5):\n",
        "                print(len(entity_aware_embeddings))\n",
        "\n",
        "            count+=1\n",
        "\n",
        "    print(len(predictions),len(tokenized_sentences),len(entity_embeddings))\n",
        "    return entity_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFKMsh5Arns8"
      },
      "source": [
        "def get_sts_data(filename):\n",
        "    stsDataDictList=[]\n",
        "    stsData_columns =['sentence1', 'sentence2', 'score']\n",
        "    f=open(\"data/stsbenchmark/\"+filename+\".csv\",'r')\n",
        "    file_text= f.read()\n",
        "    lines=file_text.split('\\n')\n",
        "    for line in lines:\n",
        "        if(line):\n",
        "            fields=line.split('\\t')\n",
        "    #         print(len(fields))\n",
        "            dataDict={'sentence1':fields[5],'sentence2':fields[6],'score':float(fields[4])/5.0}\n",
        "    #         print(dataDict)\n",
        "            stsDataDictList.append(dataDict)\n",
        "    stsData=pd.DataFrame(stsDataDictList)\n",
        "    return stsData\n",
        "# stsTrainData.columns =['genre', 'filename', 'year', 'unidentified', 'score', 'sentence1', 'sentence2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvMv5O0MmFVG"
      },
      "source": [
        "stsTrainData = get_sts_data('sts-train')\n",
        "print(len(stsTrainData))\n",
        "print(stsTrainData.columns)\n",
        "\n",
        "stsDevData = get_sts_data('sts-dev')\n",
        "print(len(stsDevData))\n",
        "print(stsDevData.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fSzSvqFmVEW"
      },
      "source": [
        "# first pass it through NER Engine and get the contextual embeddings\n",
        "\n",
        "#Training Data\n",
        "#For Source Sentences:\n",
        "source_sentence_embeddings_train = get_STS_Data_embeddings(stsTrainData['sentence1'].tolist())\n",
        "#For Target Sentences:\n",
        "target_sentence_embeddings_train = get_STS_Data_embeddings(stsTrainData['sentence2'].tolist())\n",
        "assert len(source_sentence_embeddings_train)==len(target_sentence_embeddings_train)\n",
        "\n",
        "#Validation Data\n",
        "#For Source Sentences:\n",
        "source_sentence_embeddings_dev = get_STS_Data_embeddings(stsDevData['sentence1'].tolist())\n",
        "#For Target Sentences:\n",
        "target_sentence_embeddings_dev = get_STS_Data_embeddings(stsDevData['sentence2'].tolist())\n",
        "assert len(source_sentence_embeddings_dev)==len(target_sentence_embeddings_dev)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TXrkbXVIXp3"
      },
      "source": [
        "#Printing some shapes\n",
        "print(len(source_sentence_embeddings_train), len(target_sentence_embeddings_train))\n",
        "print(source_sentence_embeddings_train[0].shape)\n",
        "embeddingSize=list(source_sentence_embeddings_train[0][0].shape)[0]\n",
        "# print(embeddingSize)\n",
        "# print(type(list(embeddingSize)[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2WxtYySGMHA"
      },
      "source": [
        "# Datasets and DataLoaders\n",
        "training_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_train, target_sentence_embeddings_train))),stsTrainData['score'].tolist())\n",
        "training_generator = torch.utils.data.DataLoader(training_set, shuffle=True)\n",
        "\n",
        "validation_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_dev, target_sentence_embeddings_dev))),stsDevData['score'].tolist())\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfFolw3IhrA"
      },
      "source": [
        "x,y =training_set.__getitem__(0)\n",
        "print(type(x))\n",
        "print(x[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dje0rlO66WDC"
      },
      "source": [
        "def save_ckp(state, is_best, checkpoint_dir):\n",
        "    # f_path = checkpoint_dir + '/checkpoint.pt' \n",
        "    f_path = checkpoint_dir + '/checkpoint_model300.pt' #_model300\n",
        "    torch.save(state, f_path)\n",
        "    # if is_best:\n",
        "    #     best_fpath = best_model_dir +'/best_model.pt'\n",
        "    #     shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGbFUV5PcX8Q"
      },
      "source": [
        "# !pip3 install pytorchtools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7rLtDdOREIH"
      },
      "source": [
        "# import EarlyStopping\n",
        "# from pytorchtools import EarlyStopping\n",
        "\n",
        "# Initialize network\n",
        "phraseEmbeddingModel = PhraseEmbedding(embeddingSize, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# initialize the early_stopping object\n",
        "# early_stopping = EarlyStopping(patience=patience, verbose=True, path='entityEmbedding/model_checkpoints/checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM4tsJ6l0BxX"
      },
      "source": [
        "checkpoint_dir='entityEmbedding/model_checkpoints'\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 200\n",
        "patience = 5\n",
        "\n",
        "# Train Network\n",
        "history_training= []\n",
        "history_validation = []\n",
        "best_loss = np.float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_batch_loss=[]\n",
        "    for batch_idx, (data, target) in enumerate(training_generator):\n",
        "        target = torch.tensor(float(target)).to(device=device)\n",
        "        out = phraseEmbeddingModel(data)\n",
        "        # print(out.item())\n",
        "        # if(not math.isnan(out.item())):\n",
        "            # print(data)\n",
        "        loss = criterion(out, target)\n",
        "        # print(loss.item())\n",
        "        training_batch_loss.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(phraseEmbeddingModel.parameters(), 1.0)\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "    combined_training_loss = np.mean(training_batch_loss)\n",
        "    print('combined epoch training loss:', combined_training_loss)\n",
        "    history_training.append(combined_training_loss)\n",
        "\n",
        "    #Validation: DO NOT BACKPROPAGATE HERE\n",
        "    validation_batch_loss = []\n",
        "    print_only_one=True\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(validation_generator):\n",
        "            target = torch.tensor(float(target)).to(device=device)\n",
        "            if(print_only_one):\n",
        "                # print(len(data[0]),len(data[1]))\n",
        "                # print(type(data))\n",
        "                print_only_one=False\n",
        "            out = phraseEmbeddingModel(data)\n",
        "            loss = criterion(out, target)\n",
        "            validation_batch_loss.append(loss.item())\n",
        "            # print(validation_batch_loss)\n",
        "    combined_validation_loss= np.mean(validation_batch_loss)\n",
        "    history_validation.append(combined_validation_loss)\n",
        "    # if(((epoch+1)%10==0)|(epoch == (num_epochs-1))):\n",
        "    print('\\nEpoch',str(epoch+1),' Validation Loss:',combined_validation_loss)\n",
        "    print('=====================================================================\\n')\n",
        "    \n",
        "    if(combined_validation_loss<best_loss):\n",
        "        best_loss = combined_validation_loss\n",
        "        print('saving this checkpoint')\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        save_ckp(checkpoint, True, checkpoint_dir)\n",
        "        no_improvement_counter=0\n",
        "    else:\n",
        "        no_improvement_counter+=1\n",
        "        if(no_improvement_counter>patience):\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhKf3SBXPjI"
      },
      "source": [
        "# checkpoint = {\n",
        "#     'epoch': epoch + 1,\n",
        "#     'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "#     'optimizer': optimizer.state_dict()\n",
        "# }\n",
        "# checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint.pt' #768\n",
        "checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint_model300.pt' #300\n",
        "# model_dir = 'best-model'\n",
        "# save_ckp(checkpoint, True, checkpoint_dir, model_dir)\n",
        "\n",
        "# load the last checkpoint with the best model\n",
        "entityPhraseEmbedder, optimizer, start_epoch = load_ckp(checkpoint_path, phraseEmbeddingModel, optimizer)\n",
        "print('Loading model from epoch:', start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhVLIVUy57ZX"
      },
      "source": [
        "## **Training the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rck4IiG65lkt"
      },
      "source": [
        "#to train the Entity Classifiers\n",
        "# tweets_unpartitoned=pd.read_csv('data/deduplicated_test.csv',sep =';',keep_default_na=False)\n",
        "tweets_unpartitoned=pd.read_csv('data/large_training_file.csv',sep =',',keep_default_na=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCjtUKR6IOl",
        "outputId": "bb896e0a-9c81-4ef9-8824-a9bb07684ee2"
      },
      "source": [
        "# import localNER as localNER (self, sentenceTokenizer, nerTokenizer, nerEngine, label_list, device):\n",
        "# local_NER_Module= localNER.LocalNERModule(sentenceTokenizer, tokenizer, alt_model, label_list, device)\n",
        "\n",
        "#tokenizer here is the BERT model's tokenizer\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, tokenizer, alt_model, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Local NER Engine!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I9OVbA6Q1sR"
      },
      "source": [
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'classifier-train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Qa9DQDpWIG",
        "outputId": "2c5a10fc-f8a2-47e7-c55e-ea23cff0af4b"
      },
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets are in memory...\n",
            "39732 39732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThM_gzA8CqPd",
        "outputId": "edd15e8c-29b5-4dd1-f982-11eaa16c822e"
      },
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH3mGcUI9p-Q",
        "outputId": "2470761f-74bb-4a08-a17e-ec84d4932e74"
      },
      "source": [
        "tweet_to_sentences_w_annotation={}\n",
        "# df_out_holder_Phase1=[]\n",
        "total_time=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1\n",
        "\n",
        "# for g, tweet_batch in tweets_unpartitoned.groupby(np.arange(length) //batch_size):\n",
        "phaseI_timein=time.time()\n",
        "g=0\n",
        "tuple_of= local_NER_Module.extract(tweet_batch,g)\n",
        "phaseI_timeout=time.time()\n",
        "\n",
        "print('local emd time',(phaseI_timeout-phaseI_timein))\n",
        "\n",
        "tweet_base=tuple_of[0]\n",
        "contextual_embeddings=tuple_of[1]\n",
        "candidate_base=tuple_of[2]\n",
        "elapsedTime= tuple_of[4] - tuple_of[3]\n",
        "phase2stopwordList=tuple_of[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=tuple_of[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n",
        "\n",
        "# candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['paul manaford']\n",
            "entities_from_sentence: ['michael flynn']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['donald trump']\n",
            "entities_from_sentence: ['ray scollin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rachel', 'carter page']\n",
            "entities_from_sentence: ['page', 'comey']\n",
            "entities_from_sentence: ['flynn', 'woolsey']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page trump']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['orlando bloom', 'carter page', 'lifetime']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'trump', 'kushner', 'patsy']\n",
            "entities_from_sentence: ['page', 'sessions']\n",
            "entities_from_sentence: ['page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'ex-trump advisor', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'ties']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['trump jr erik']\n",
            "entities_from_sentence: ['carter page', 'dept eavesdropping', 'jr']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'wapo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russian']\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['watergate']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page', 'trumpikins']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['people', 'page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['trumpf']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'carter page', 'russia']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump campaign', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fisa', 'fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['russian traitor']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'obama']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'navy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'us-rus']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rachel', 'wash post']\n",
            "entities_from_sentence: ['trumps', 'carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['rachel']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['comey', 'mccabe']\n",
            "entities_from_sentence: ['obama', 'fbi']\n",
            "entities_from_sentence: ['spicer blew', 'page story']\n",
            "entities_from_sentence: ['nunes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'paul manafort', 'pence']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump', 'news']\n",
            "entities_from_sentence: ['russian connection']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['clinton fdn', 'hrc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump']\n",
            "entities_from_sentence: ['carter page', 'obama']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['rachel maddow msnbc', 'fbi', 'carter page']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['flynn']\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['page', 'reported']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: ['page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['russians']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump associates']\n",
            "entities_from_sentence: ['carter page', 'mlk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['page', 'trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['carter page', 'trump', 'jeff sessions']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter pageγç']\n",
            "entities_from_sentence: ['trump', 'prez palace', 'syria']\n",
            "entities_from_sentence: ['fbi', 'donald trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hillary clinton', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['jared kushner', 'kislyak', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'carter page']\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi', 'carter page']\n",
            "entities_from_sentence: ['carter page', 'russia', 'fbi', 'donald trump']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump', 'wapo']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['comey']\n",
            "entities_from_sentence: ['washington times', 'fbi', 'carter page', 'fisa']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'doj', 'carter page', 'russian', 'campaign']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page', 'trump']\n",
            "entities_from_sentence: ['campaign-russia']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'gork', 'bannon next', 'ivanka', 'impeachment']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['carter page', 'flynn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['carter page', 'martin luther king']\n",
            "entities_from_sentence: ['carter cameo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'steele dossier']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guy nervous']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'donald trump', 'carter pageγçª']\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'nunes', 'page sean', 'holocaust centers', 'bout time', 'think']\n",
            "entities_from_sentence: ['trump', 'north korea', 'carter page']\n",
            "entities_from_sentence: ['kislyak', 'putin', 'trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page', 'trump', 'blaming', 'clinton']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dems', 'page']\n",
            "entities_from_sentence: ['hrc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi', 'carter page', 'russiaγçª']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['comey']\n",
            "entities_from_sentence: ['washington times', 'fbi', 'carter page', 'fisa']\n",
            "entities_from_sentence: ['rachel maddow']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['potus', 'comey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['maddow', 'carter page', 'russia', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'family']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['carter page', 'russia', 'trump']\n",
            "entities_from_sentence: ['fisa', 'carter page']\n",
            "entities_from_sentence: ['donald trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['flynn', 'trump tower']\n",
            "entities_from_sentence: ['carter page', 'russia']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['cnn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['republican']\n",
            "entities_from_sentence: ['carter']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['flynn', 'sessions', 'russians', 'fbi', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian']\n",
            "entities_from_sentence: ['dnc', 'obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama administration', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter pageγç']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page', 'privatization']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: ['washington post', 'fbi', 'carter page', 'trump', 'may']\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['page biography']\n",
            "entities_from_sentence: ['trump', 'carter page']\n",
            "entities_from_sentence: ['adviser']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson', 'us', 'ukraine']\n",
            "entities_from_sentence: ['spicer', 'hitler']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['devin nunes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tower', 'interpretation takes']\n",
            "entities_from_sentence: ['carter page', 'fisa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'trump', 'russia.but']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fisa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page', 'russian', 'advisor']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['charles simmons']\n",
            "entities_from_sentence: ['tinkle']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'potus']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'org plans', 'kong', 'γçª']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nra', 'putin', 'moscow']\n",
            "entities_from_sentence: ['eric trump', 'philippines-saying', 'γçª']\n",
            "entities_from_sentence: ['eric trump', 'don jr', 'joey', 'socks']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumps']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'potus']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lawrence']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vlad and the rethugs']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'dakota']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tiffany']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['clueless']\n",
            "entities_from_sentence: ['using']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'cia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'eric']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dumb and dumber']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north carolina state capitol']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['joe']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['stγçª']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sally hemings']\n",
            "entities_from_sentence: ['dad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ss']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['scott']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['money', 'imminent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['trump', 'assad']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['turkey']\n",
            "entities_from_sentence: ['tweedledum', 'tweedledee', 'joey', 'mo socks']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tsar-a-lago']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['cheeto', 'teflon don']\n",
            "entities_from_sentence: ['gop', 'rnc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russians', 'congress', 'dept']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['idiot trump brothers']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['beavis', 'butthead']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter', 'bert lance']\n",
            "entities_from_sentence: ['kremlin', 'russian', 'donald trumpγçös']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rep swalwell']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ny board of elections']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donnie jr']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pooty']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['twitter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pamela anderson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['grizzly']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanks', 'bank']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions']\n",
            "entities_from_sentence: ['definitely']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['trump', 'paul manafort']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pysanky design']\n",
            "entities_from_sentence: ['congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'panama', 'canadian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['george hamilton']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'congress', 'state dept']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'panama', 'canadian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['potus']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putins', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'usa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wilbur ross']\n",
            "entities_from_sentence: ['peloponnesian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin', 'staged']\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'don jr', 'philippines']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumpino family']\n",
            "entities_from_sentence: ['eric trump', 'don con block.he']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['schumer', 'trump']\n",
            "entities_from_sentence: ['syria', 'nk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: ['komrade kushners', 'gitmo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumps']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['tillerson', 'moscow', 'talk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mafia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rubios speedboat']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'msm']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['herr trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump-duterte']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kabuki theatre', 'spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nra', 'potus']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['seattle']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mar a lago']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['apprentice-white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cia', 'trump campaign']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['duterte']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'exkgb', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spawn', 'daddy', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ks']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['omar sharif']\n",
            "entities_from_sentence: ['omar']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'hotel collection']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['james bond']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['canada']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['eric trump', 'telegraph']\n",
            "entities_from_sentence: ['donald trump', 'foundation', 'eric in 2010']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bevis butthead']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fredo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['giuliani']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['beavis', 'butthead']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vern']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'qusay']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['keeps']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['california.let']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jamali', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['russia', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['marmel']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['marmel']\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: ['karma']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['toronto']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['booker']\n",
            "entities_from_sentence: ['congress', 'trumps']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', '45']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['apple', 'google']\n",
            "entities_from_sentence: ['eric', '45', '45s']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'cia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['eric trump', 'don jr', 'philippines']\n",
            "entities_from_sentence: ['scott dworkinγçª']\n",
            "entities_from_sentence: ['eric trump', 'hotel collection', 'γçª']\n",
            "entities_from_sentence: ['donald trump', 'foundation', 'eric in 2010']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lacedaemon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['korea', 'japan', 'pacific']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['msm']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump', 'syria', 'trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['comrade trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['john mellencamp']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sonny']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['chaffetz', 'hillary']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'syria-way', 'rites']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['barraford daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['snl', 'south park']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us', 'nk', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivugnka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['billboards']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['possible', 'campaign']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scotus', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rachel']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jfk']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['deflect']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syrian', '3m']\n",
            "entities_from_sentence: ['ally']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['secretary potemkin']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tribune']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mccain']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['name']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['election']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['snl']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['syria', 'north', 'korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['find']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'russia']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['eric trump', 'soho buyers']\n",
            "entities_from_sentence: ['don jr', 'eric trump', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trumpγçös', 'uruguay']\n",
            "entities_from_sentence: ['eric trump', 'north', 'dakota']\n",
            "entities_from_sentence: ['eric trump', 'org plans', 'kong']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'philippines-saying']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'don jr', 'joey', 'socks']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumpy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syrian', 'dnc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['erik prince', 'devos']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wiz', 'wiz']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['beudgen daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['project']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['alibi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['swayed', 'according']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['says']\n",
            "entities_from_sentence: ['jeffrey lord']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['donald trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['45', 'us', 'strikes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kim jong un']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lurch']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'trumps', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['conquerors of space']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'ivank', 'adolph trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['americas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hillsboro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'kids']\n",
            "entities_from_sentence: ['menendez']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['fbi', 'congress']\n",
            "entities_from_sentence: ['shawn']\n",
            "entities_from_sentence: ['jackie']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['betsy devos']\n",
            "entities_from_sentence: ['erik prince']\n",
            "entities_from_sentence: ['blackwater']\n",
            "entities_from_sentence: ['seychelles']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putins']\n",
            "entities_from_sentence: ['gulf of tonkin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson', 'russia']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ddn', 'jr', 'eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kushner']\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump tower', 'spectrum health']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sounding']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'aaron burr']\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['isis']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obamacare']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mexico']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['us', 'china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['comey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rex tillerson', 'us', 'ukraine']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['corporate america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'tillerson', 'moscow']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scott dworkin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['golf']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: ['trump', 'fbi', 'congress']\n",
            "entities_from_sentence: ['cnn']\n",
            "entities_from_sentence: ['impeachtrump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['skippy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['russia', 'medvedev', 'lavrov']\n",
            "entities_from_sentence: ['tillerson', 'haley', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hamilton']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['graham']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scaramucci']\n",
            "entities_from_sentence: ['alexander hamilton']\n",
            "entities_from_sentence: ['harry potter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['menace ejuaculated']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['quinn cummings', 'quinncy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['quinn cummings']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['isis']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obamacare']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mexico']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['fbi', 'congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['patton']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'us', 'syrian', 'bashar-al assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump-syria']\n",
            "entities_from_sentence: ['putin', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian rogues gallery']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian']\n",
            "entities_from_sentence: ['russian', 'foreign min', 'amb kislyakγçös', 'sessions', 'kushner', 'manafort']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['set']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vlad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sounding']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fm', 'zarif']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['flynn']\n",
            "entities_from_sentence: ['nunes']\n",
            "entities_from_sentence: ['bannon', 'nsc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian', 'putinγçös']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['beavis', 'buttheads']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia-approving', 'montenegro', 'nato']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'geppetto']\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['apple']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'djt']\n",
            "entities_from_sentence: ['djt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['socks cinque', 'tsar-a-lago', 'new']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['koreans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rex', 'patented', 'reach']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hamilton']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lawn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: ['kansas dems']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: ['eric trump', 'winery']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vlad co']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'maga']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['secretary tillerson', 'fm lavrov']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['montenegro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nyet']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: ['rex tillerson', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump tower', 'seattle']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['yates']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['michael flynn']\n",
            "entities_from_sentence: ['los angeles times']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rex tillerson', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['clinton', 'balkans']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: ['trump', 'china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hillary']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rex tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'everyday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump transition team']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'dump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['peeing']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['congress']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pepsi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['rex tillerson', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'strike', 'ties']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['snow']\n",
            "entities_from_sentence: ['usay', 'qusay trump', 'organization']\n",
            "entities_from_sentence: ['trump ent']\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['general', 'kaspersky']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'tomahawks', 'syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['twitter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rexypoo', 'pooty', 'come']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mar-a-lago']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin', 'assad']\n",
            "entities_from_sentence: ['trump-russia']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['media']\n",
            "entities_from_sentence: ['trump', 'russia', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['trump', 'obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'syria', 'trump', 'white house']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'syria']\n",
            "entities_from_sentence: ['trump', 'assaud', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['twitter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white', 'house']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['n korea-deflection']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america', 'vox']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumps']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['uday']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ron estes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fame']\n",
            "entities_from_sentence: ['mensch']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson', 'russian', 'syria', 'putin', 'us']\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bannon']\n",
            "entities_from_sentence: ['trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pence']\n",
            "entities_from_sentence: ['potus']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'ven']\n",
            "entities_from_sentence: ['citco']\n",
            "entities_from_sentence: ['putin', 'american oil']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scott dworkin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assaud', 'russia']\n",
            "entities_from_sentence: ['afghanistan', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['45', 'putin']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rachel', 'fbi', 'russian', 'win']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump-russia']\n",
            "entities_from_sentence: ['trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scott dworkin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['djt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'popes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['devos']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['oxyclean']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nra']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['rob']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['capital']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house', 'pirozhkis']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'syria']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['smellyanne']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumps']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumpers']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spain']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cassandrarules', 'wilson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['louise']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['patriot']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rollercoaster', 'trader joe']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['citizen', 'storm d c']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'north korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korean', 'china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jr']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hrc', 'says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['first family']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assange']\n",
            "entities_from_sentence: ['samantha bee', 'hrc', 'women', 'world']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bromancer tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump children']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['needs']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['artie shaw his orchestra', 'rosalie']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spain']\n",
            "entities_from_sentence: ['controlled']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kendall jenner']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korean', 'china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cambridge analytica', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['daddy', 'ivanka']\n",
            "entities_from_sentence: ['trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumps', 'wh', 'america']\n",
            "entities_from_sentence: ['mcclintock', 'obama', 'rice']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north koreaγçös']\n",
            "entities_from_sentence: ['voters', 'cong']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ford']\n",
            "entities_from_sentence: ['trabant']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fredo', 'putin']\n",
            "entities_from_sentence: ['dao', 'united']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['laurie penny']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donnie']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assad', 'hitler', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['djt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: ['tulsi']\n",
            "entities_from_sentence: ['rep gabbard']\n",
            "entities_from_sentence: ['wag the dog']\n",
            "entities_from_sentence: ['putin', 'drump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['papanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united airlines']\n",
            "entities_from_sentence: ['spicer', 'trump', 'congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wag the dog']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donny']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['garland', 'trump']\n",
            "entities_from_sentence: ['kremlin']\n",
            "entities_from_sentence: ['sean spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['blackwater']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sarin', 'assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['drumpf']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korea', 'navy approach']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['center']\n",
            "entities_from_sentence: ['jeff sessions']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jeff sessions']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions lied']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['coulter putin']\n",
            "entities_from_sentence: ['statedept', 'secretary tillerson', 'fm lavrov']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['michael flynn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lied']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['knows']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['target']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nunes', 'white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['peggy noonan']\n",
            "entities_from_sentence: ['sean spicer', '45']\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: ['assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america', 'eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: ['us-russia']\n",
            "entities_from_sentence: ['trump-putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kandas', 'georgia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['potus']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['chuck schumer', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['moscow mule']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dashanne stokes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kim']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['operation']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['betsy devos']\n",
            "entities_from_sentence: ['erik prince']\n",
            "entities_from_sentence: ['blackwater']\n",
            "entities_from_sentence: ['seychelles']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pope']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['michael flynn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rosneft']\n",
            "entities_from_sentence: ['steele dossier']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['casey neistat', 'united airlines']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['drumpf']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wh', 'russia']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'hitler']\n",
            "entities_from_sentence: ['rachel maddow', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gets']\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: ['lois lerner']\n",
            "entities_from_sentence: ['religion of peace']\n",
            "entities_from_sentence: ['sean spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['sean spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rex tillerson', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['washington post', 'nyt', 'donald trump', 'putin @user']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kellyanne conway', 'sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scary']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'investigation']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mattis', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'spain']\n",
            "entities_from_sentence: ['spain', 'us']\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: ['malala yousafzai']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['faux news']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean pricer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['erik prince', 'lybia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin made']\n",
            "entities_from_sentence: ['lied']\n",
            "entities_from_sentence: ['trump', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['germany']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia', 'get']\n",
            "entities_from_sentence: ['sean spicer', 'eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['canada']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bunny']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['silly spice']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spiceboy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mar-a-lago']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united airlines']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['florida']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assange', 'dump', 'bury']\n",
            "entities_from_sentence: ['trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['irs']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: [\"we_haven't_forgotten_about_russia\"]\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['moscow']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mo']\n",
            "entities_from_sentence: ['population']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin', 'assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['news']\n",
            "entities_from_sentence: ['london']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'stalin']\n",
            "entities_from_sentence: ['puti']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['administration']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: ['infowars']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['german', 'hitler']\n",
            "entities_from_sentence: ['eric', 'joey', 'socks cinque']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['adolf hitler']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['spicer', 'trump']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['holocaust center']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer holocaust centers']\n",
            "entities_from_sentence: ['spicer', '100']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['holocaust center']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'hitler', \"munich b'nai brith\"]\n",
            "entities_from_sentence: ['svetlana']\n",
            "entities_from_sentence: ['military']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['45']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['orange julius']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['brian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white']\n",
            "entities_from_sentence: ['spicer', 'san bernadino']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donny']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bannon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['twitler boat']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: ['chechnya']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china', 'n korean']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['holocaust']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'n russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putinγ']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rachel maddow', 'collusion']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: ['chicago']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'american']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'ny']\n",
            "entities_from_sentence: ['jenkins daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['svetlana']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['republicans']\n",
            "entities_from_sentence: ['stepping stones daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['american']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tx']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['message']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'tomahawk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'bannon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['stock']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['alex jones', 'sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'sarin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rubin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['chinese']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sweden']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['stock', 'used']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['stock', 'used']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['stock', 'used']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bombing']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'tomahawk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white', 'house']\n",
            "entities_from_sentence: ['eric', 'trump', 'rusn']\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'missiles']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['said']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tweeter']\n",
            "entities_from_sentence: ['mein fuhrer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jefferson beauregard sessions']\n",
            "entities_from_sentence: ['jefferson beauregard']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donnie']\n",
            "entities_from_sentence: ['jews']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pelosi', 'sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson', 'moscow', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['melissa mccarthy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dillon daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tax']\n",
            "entities_from_sentence: ['obama', 'trump', 'gop']\n",
            "entities_from_sentence: ['wh', 'russia']\n",
            "entities_from_sentence: ['sean', 'trump', 'rusγçª']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['said']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dave']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['annefrankcenter']\n",
            "entities_from_sentence: ['kim']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['georgia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['american']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wichita']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['chinese', 'trump', 'pro-russia']\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: ['white house', 'bunny']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fired']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'syria', 'russia']\n",
            "entities_from_sentence: ['syria', 'russia']\n",
            "entities_from_sentence: ['don', 'uncle', 'vlad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer talk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['comey', 'spain', 'rssn', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'rest']\n",
            "entities_from_sentence: ['rump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rachel']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tyranny']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler', 'trump']\n",
            "entities_from_sentence: ['trump', 'took', 'presidential']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bannon', 'reichstag']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'hitler']\n",
            "entities_from_sentence: ['german']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['republican', 'north korea']\n",
            "entities_from_sentence: ['fired']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: ['sean spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['indigo swing', 'blue suit boogie']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['brilliant']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nunes', 'wh']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'missile']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['beudgen daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['don']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'christie-another']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['news']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ben carson', 'band']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['twitter', 'ic']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions']\n",
            "entities_from_sentence: ['sean']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rome']\n",
            "entities_from_sentence: ['msm']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vladimir']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['brought', 'distraction']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['boss hogg', 'spicer']\n",
            "entities_from_sentence: ['chechnya', 'holocaust']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['sean']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicey']\n",
            "entities_from_sentence: ['eric', 'eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['fukushima']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka', 'veruka salt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'syria', 'assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['frederick douglass']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['45']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['45', 'spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions', 'bannon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: ['woolsey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['southboundriser']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: ['donald']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bullhorns']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bullhorns']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['schiff']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['certainly']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['booty']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pepsi', 'united', 'sean']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['germany']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bannon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['texas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['isis']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'holocaust center']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['djt', 'america']\n",
            "entities_from_sentence: ['dc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bears police', 'mojave desert schools']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: ['outfit']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nk']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wag the dog', 'eric']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'unencumbered']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lies']\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maddow', 'trump']\n",
            "entities_from_sentence: ['hiding']\n",
            "entities_from_sentence: ['saipan casino']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanky']\n",
            "entities_from_sentence: ['alfalfa', 'buckwheat']\n",
            "entities_from_sentence: ['butch']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['driving', 'st']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['issa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nut house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'trump university']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'government']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumpy']\n",
            "entities_from_sentence: ['sarin', 'assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scotus', 'citizens united']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: ['morpheus', 'neo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['puppets']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maddow']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white', 'russia', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['moron', 'eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['seat']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maddow']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['russian']\n",
            "entities_from_sentence: ['tin of pu']\n",
            "entities_from_sentence: ['steve']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maddow']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: ['cover']\n",
            "entities_from_sentence: ['devin']\n",
            "entities_from_sentence: ['get', 'treason']\n",
            "entities_from_sentence: ['msteryis']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nasa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['norad', 'santa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'trump', 'russia-says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gop', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['consulted', 'instead']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['combover']\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kushner', 'melissa']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['message']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['republicans']\n",
            "entities_from_sentence: ['chris farley', 'tommyboy']\n",
            "entities_from_sentence: ['tommyboy']\n",
            "entities_from_sentence: ['ivan']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hiltler', 'hitler', 'hitler']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kushner']\n",
            "entities_from_sentence: ['melissa the sow']\n",
            "entities_from_sentence: ['tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jared kushner', 'steve bannon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'israel']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['show']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['andy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mitch mcconnell']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump administration']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bannon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kushner']\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wing']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jungle press']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'missiles']\n",
            "entities_from_sentence: ['russians', 'cambridge analytics']\n",
            "entities_from_sentence: ['tower']\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scott bannonwas', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['american']\n",
            "entities_from_sentence: ['russians']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin', 'us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['45']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['dc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama', 'trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['don']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['west', 'virginia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'jews', 'concentration']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['warn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['yahoo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'trump-russia']\n",
            "entities_from_sentence: ['nazi', 'bannon', 'white', 'spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white', 'russia', 'syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'goebbels']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['adam jones']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump administration']\n",
            "entities_from_sentence: ['trump campaign', 'fbi', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'world', 'duck', 'friends']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rachel maddow', 'trumpγçös']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson', 'moscow', 'syria']\n",
            "entities_from_sentence: ['eric trump', 'russia']\n",
            "entities_from_sentence: ['administration @user']\n",
            "entities_from_sentence: ['comey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicey']\n",
            "entities_from_sentence: ['sean spicer', 'hitler-assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syrian']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pat hamm', 'liked']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['italy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['administration']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: [\"anita o'day\", 'boogie blues']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['brooklyn']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['coulter', 'hannity']\n",
            "entities_from_sentence: ['trump', 'spicer']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pied piper', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lie', 'jets']\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united', 'spicey']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'united airlines']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['regimes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maddow']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['approves']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumprussia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assad']\n",
            "entities_from_sentence: ['isis']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nazis']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer', 'hitler', 'holocaust centers', 'ceo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cathedral of christ the saviour']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['don']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions']\n",
            "entities_from_sentence: ['white']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'syria', 'obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fox news']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jared kushner', 'ivanka', 'daddy', 'say', 'later']\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ulta beauty']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maxin', 'pelosi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rex tillerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (129 > 128). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'white house', 'cnn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scott dworkin']\n",
            "entities_from_sentence: ['eric trump', 'org plans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['news']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scott dworkin']\n",
            "entities_from_sentence: ['eric trumpγçös', 'uruguay']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['callista gingrich', 'ambassador', 'soonγçª']\n",
            "entities_from_sentence: ['donald', 'ivanka eric', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['huxley daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['jimmy carter', 'amy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nordstrom']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['twitter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['united hq']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'twitter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['scott dworkin']\n",
            "entities_from_sentence: ['don jr', 'eric trump', 'russian']\n",
            "entities_from_sentence: ['priebus', 'bannon', 'kushner']\n",
            "entities_from_sentence: ['cnn']\n",
            "entities_from_sentence: ['eric', 'pappa trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['middle east']\n",
            "entities_from_sentence: ['corral']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'congress']\n",
            "entities_from_sentence: ['china']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['left', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lucretia borgia', 'sudan']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['johnson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump--this']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['daddy', 'ivanka']\n",
            "entities_from_sentence: ['eric', 'trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['yellen']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'tirade']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bannon', 'trump']\n",
            "entities_from_sentence: ['ivanka trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['reagan', 'iran']\n",
            "entities_from_sentence: ['case']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bundren']\n",
            "entities_from_sentence: ['faulkner']\n",
            "entities_from_sentence: ['chelsea', 'clinton']\n",
            "entities_from_sentence: ['judicial watch', 'atf']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bill clintoon']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['ivanka trump', 'syria']\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['nra']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['duke', 'page']\n",
            "entities_from_sentence: ['israel']\n",
            "entities_from_sentence: ['david duke']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['youtube']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['usa', 'relations']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['walker']\n",
            "entities_from_sentence: ['walker']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['ivanka', 'et']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venice bridge']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'doonbeg']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fate of the furious']\n",
            "entities_from_sentence: ['syria', 'egypt']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['general', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumps']\n",
            "entities_from_sentence: ['following']\n",
            "entities_from_sentence: ['hillary', 'chelseaγçös', 'syria']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['williams']\n",
            "entities_from_sentence: ['military']\n",
            "entities_from_sentence: ['sacramento bee']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rubin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['swayed', 'decision', 'says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['twitter', 'china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['martin']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['kgb', 'us', 'brexit']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kevin smith']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hee park']\n",
            "entities_from_sentence: ['says']\n",
            "entities_from_sentence: ['eric trump', 'congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['istanbul']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump foundation']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bardugo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['warren']\n",
            "entities_from_sentence: ['eric trump', 'united states']\n",
            "entities_from_sentence: ['syria', 'trump', 'says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['morrison']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rose']\n",
            "entities_from_sentence: ['dashner']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tillerson', 'russia', 'assad']\n",
            "entities_from_sentence: ['trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['atletico madrid', 'leicester', 'fernando torres', 'fans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bee-itch', 'usa']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['barnum of presidents']\n",
            "entities_from_sentence: ['eric trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mar-a-lago resort', 'florida']\n",
            "entities_from_sentence: ['elmo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bashevis singer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donaldwashington']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['orben']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['strasser']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pond']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka', 'jared']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'daddy', 'kids']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['ivana', 'member']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rnc', 'ronna romney']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['devos']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['virginia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka trump', 'syria', 'eric says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'jr']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hillary']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka', 'eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['general', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['iveruca salt', 'tomahawk']\n",
            "entities_from_sentence: ['vatican']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['archuleta']\n",
            "entities_from_sentence: ['ivanka trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['cillizza', 'eric trump', 'ivanka', 'wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['syrian']\n",
            "entities_from_sentence: ['vladimir putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['murat']\n",
            "entities_from_sentence: ['eric trump', 'clan']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'tens millions']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['doonbeg']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['anthony burkett']\n",
            "entities_from_sentence: ['eric', 'brood']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['american jesus', 'doonbeg']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spenser']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'easter bunny']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['charlize theron', 'atomic blonde']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['morrow lindbergh']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gilbert']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['e cummings']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['boston globe']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vreeland']\n",
            "entities_from_sentence: ['trump-pope francis', 'italy', 'expects']\n",
            "entities_from_sentence: ['donald trumps', 'uk', 'theresa may', 'syria', 'times', 'india']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['kansas', 'trump']\n",
            "entities_from_sentence: ['ted cruz']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['democrats', 'trump', 'senate democratic', 'charles schumer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka influenced', 'strikes', 'believesγçª']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['oceanic', 'administration show', 'club']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['cillizza', 'eric trump', 'ivanka', 'wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ttower']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['yanfolila']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['syria', 'ivaka']\n",
            "entities_from_sentence: ['nordstrom']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'helped']\n",
            "entities_from_sentence: ['eric trump', 'influenced']\n",
            "entities_from_sentence: ['eric trump', 'influenced']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america', 'eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: ['syria', 'assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gingrich']\n",
            "entities_from_sentence: ['virtual']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['salzberg']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['snl']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'donald purchased']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['pres bush']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama', 'sasha', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['toomer']\n",
            "entities_from_sentence: ['roger goodell', 'nfl']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['djt', 'san bernadino']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sperling daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['suggests ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['saudi', 'arabian']\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: ['us', 'russia', 'partnership']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia.she', 'pro']\n",
            "entities_from_sentence: ['fredo']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rieu']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['super']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sarin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hardy']\n",
            "entities_from_sentence: ['hillary']\n",
            "entities_from_sentence: ['hasan minhaj who called', 'donald white', 'whcd']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka', 'house']\n",
            "entities_from_sentence: ['eric trump', 'tomahawks', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka trump']\n",
            "entities_from_sentence: ['eric', 'trump']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pleasant']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['devil']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maclane']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['strikes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trotsky']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'sarin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'syria', 'news']\n",
            "entities_from_sentence: ['potus', 'flotus']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'white house', 'cnn']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'support']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dion jordan']\n",
            "entities_from_sentence: ['matz stockman']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'syria', 'fox', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['clickhole']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['orwell']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china', 'un', 'usa', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['malia', 'chelsea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['moore']\n",
            "entities_from_sentence: ['sweden']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['paris hilton']\n",
            "entities_from_sentence: ['donald trump', 'north korea']\n",
            "entities_from_sentence: ['bentonville college']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gl ck']\n",
            "entities_from_sentence: ['america', 'eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['israeli-american']\n",
            "entities_from_sentence: ['tom carper', 'dan scavino']\n",
            "entities_from_sentence: ['schuette']\n",
            "entities_from_sentence: ['america', 'strike']\n",
            "entities_from_sentence: ['trump', 'obama', 'syria']\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['oluwaseun']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'black ops 2', 'lust for life bayonetta']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nelson']\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'syria']\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: ['family']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'obama', 'bush', 'clinton']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dc post office']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['de saint exup ry']\n",
            "entities_from_sentence: ['co']\n",
            "entities_from_sentence: ['clare']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump broken']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'post']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['strikes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['europeunion', 'america', 'strike']\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: ['airlines']\n",
            "entities_from_sentence: ['compared']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['richard pryor']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['abc', 'erick trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['easter']\n",
            "entities_from_sentence: ['mar-a-lago']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['uhuru', 'chebukati', 'rai', 'syria russia egypt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump administration', 'eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'china']\n",
            "entities_from_sentence: ['apple']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tory burch', 'kate spade', 'jong un']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['florida']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['trump family', 'usa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['raila odinga', 'uhuru', 'ruto', 'odm']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['tlump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pulitzer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['agatha christie']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trey gowdy']\n",
            "entities_from_sentence: ['river phoenix', 'pizzagate']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['anisette']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['affordable care']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lewis']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['morgan']\n",
            "entities_from_sentence: ['solomon']\n",
            "entities_from_sentence: ['carter']\n",
            "entities_from_sentence: ['γçÿnorth']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['orange menace']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['gabbard', 'iraq']\n",
            "entities_from_sentence: ['american horror story']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fitz', 'olivia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric', 'ivanka trump', 'lying']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['controls']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ken buck', 'co', '04']\n",
            "entities_from_sentence: ['general flynn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mattpetersonlpl']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['white house correspondents association', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'influenced decision']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['ivanka', 'syria']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mure']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['l jackson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['drew']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'white']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lynn barnes']\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'family']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['chelsea clinton', 'hillary']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'air']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'russia']\n",
            "entities_from_sentence: ['trump', 'obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hillary']\n",
            "entities_from_sentence: ['trump', 'russia', 'complies']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['cillizza', 'eric trump', 'ivanka', 'house is atγçª']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['daily']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trumpγçös', 'white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['resort']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russian', 'trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['charlize theron']\n",
            "entities_from_sentence: ['prof', 'cooperates']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['game of thrones', 'donald', 'westeros']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['airlines', 'jimmy kimmel']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lesnar', 'wwe']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['doonbeg']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'news']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rubin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['alphabet soup agency']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'ivanka', 'trump', 'men']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['whitewolf']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump', 'north korea']\n",
            "entities_from_sentence: ['swedish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america', 'eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'apprentice']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tolkien']\n",
            "entities_from_sentence: ['fritz trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivankaγçös']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sanders']\n",
            "entities_from_sentence: ['ivanka', 'eric trump', 'encouraged', 'attack']\n",
            "entities_from_sentence: ['breitbart', 'saudi arabia']\n",
            "entities_from_sentence: ['us', 'gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pentagon']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vasu']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'means']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['neocons']\n",
            "entities_from_sentence: ['syrian army']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['drumpf']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump models']\n",
            "entities_from_sentence: ['trump-putin']\n",
            "entities_from_sentence: ['nbc']\n",
            "entities_from_sentence: ['korea', 'china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ann beard']\n",
            "entities_from_sentence: ['syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['chelsea clinton']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['percy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['austin']\n",
            "entities_from_sentence: ['pete']\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['says']\n",
            "entities_from_sentence: ['justice gorsuchγçös']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['donald j trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['evil']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bernadino senseless']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'nordstrom']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['malala yousafzai']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nordstroms']\n",
            "entities_from_sentence: ['sebastian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivanka', 'mad']\n",
            "entities_from_sentence: ['putin', 'mrlitvinenko']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['ivanka', 'donald', 'jr']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sahara desert']\n",
            "entities_from_sentence: ['eric trump', 'syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['l tiempo']\n",
            "entities_from_sentence: ['neil gorsuch', 'anthony kennedy', 'syria', 'russia', 'egypt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['district']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'russia']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lokos']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donny jr']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['ivanka trump', 'syria', 'eric says']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['shah']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['samson']\n",
            "entities_from_sentence: ['adam']\n",
            "entities_from_sentence: ['dumelo', 'v8']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ozymandias']\n",
            "entities_from_sentence: ['bysshe shelley']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'putin']\n",
            "entities_from_sentence: ['mattis', 'assads']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['devin nunes']\n",
            "entities_from_sentence: ['donald trump', 'shonda rhimes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trumpet']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['potus']\n",
            "entities_from_sentence: ['chemical']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us gop', 'saudi arabia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: ['nk', 'china', 'tired']\n",
            "entities_from_sentence: ['giant baby man']\n",
            "entities_from_sentence: ['russia', 'syria']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gayle']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin', 'idiots']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hillary clinton', 'katy perry']\n",
            "entities_from_sentence: ['gaza']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['halberstam']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'russia']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hibernating bears']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['basket']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['marley']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gmf', 'derek chollet']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lindsey graham']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['neil gorsuch']\n",
            "entities_from_sentence: ['clinton']\n",
            "entities_from_sentence: ['trump', 'kansas', 'gop']\n",
            "entities_from_sentence: ['eric trump', 'heavily', 'motherhood']\n",
            "entities_from_sentence: ['us', 'damascus', 'putin']\n",
            "entities_from_sentence: ['population']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['clark']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric trump', 'ivanka encouraged', 'cnni']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['meyer']\n",
            "entities_from_sentence: ['amai']\n",
            "entities_from_sentence: ['trust', 'others']\n",
            "entities_from_sentence: ['house']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kansas', 'anti-trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'refugees']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sotto']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric', 'pornhub']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['redhead']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['michelle obama', 'left']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['barthes']\n",
            "entities_from_sentence: ['eric trump', 'putin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['lovelace']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['huff post']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['msm']\n",
            "entities_from_sentence: ['charlize theron', 'commies']\n",
            "entities_from_sentence: ['eric trump', 'dailytelegraph']\n",
            "entities_from_sentence: ['trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['vladimir']\n",
            "entities_from_sentence: ['united airlines', 'jimmy kimmel']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['brock lesnar', 'wwe']\n",
            "entities_from_sentence: ['lana del rey', 'lust for life']\n",
            "entities_from_sentence: ['mac demarcoγçös']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['facebook messenger']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['congress', 'connection']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sprudz']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: ['colorado']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['trump suggests', 'influenced', 'airstrikesγç']\n",
            "entities_from_sentence: ['china']\n",
            "entities_from_sentence: ['ivanka', 'eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['joe wilsonγçös', 'hill']\n",
            "entities_from_sentence: ['orange is the new black']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'white house']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['k hler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['erik']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['montgomery']\n",
            "entities_from_sentence: ['clickhole']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['california']\n",
            "entities_from_sentence: ['clickhole', 'administration', 'trumpγçª γçª']\n",
            "entities_from_sentence: ['ivanka', 'house']\n",
            "entities_from_sentence: ['texas']\n",
            "entities_from_sentence: ['house', 'freedom caucus', 'gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['del rey', 'lust for life']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mac demarcoγçös']\n",
            "entities_from_sentence: ['syria', 'trump']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'trump-russia']\n",
            "entities_from_sentence: ['messenger']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gorka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['iranian-american', 'arya marvazy']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['roach']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka trump']\n",
            "entities_from_sentence: ['vlad', 'julian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['veruca']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'twitter', 'brian flood']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['europeunion']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump administration', 'mike pence']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hogue']\n",
            "entities_from_sentence: ['ivanka', 'pop', 'hillary']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['turkey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'house correspondents dinner']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['trump', 'kansas']\n",
            "entities_from_sentence: ['donald jr']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump', 'ivanka encouraged', 'launch']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump', 'talks']\n",
            "entities_from_sentence: ['malala yousafzai', 'donald trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'anymore']\n",
            "entities_from_sentence: ['french']\n",
            "entities_from_sentence: ['black ops 2', 'eric trump', 'life']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['murat']\n",
            "entities_from_sentence: ['egypt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['connection']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fox']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['twain']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['roosevelt']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ferrigno']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['verma', 'ky']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['murat']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['milne']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['raspberry', 'ringpops cough']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cleia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rivard']\n",
            "entities_from_sentence: ['malia obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tomahawk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin']\n",
            "entities_from_sentence: ['hawaii', 'donald trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['malanias', 'syria']\n",
            "entities_from_sentence: ['trumps']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['evanka', 'corona']\n",
            "entities_from_sentence: ['trump', 'mexico']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['beavis butthead']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['samuel beckett']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ferrazzi']\n",
            "entities_from_sentence: ['eric trump', 'dailytelegraph']\n",
            "entities_from_sentence: ['trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'ivanka']\n",
            "entities_from_sentence: ['hillary']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['valerie jarret']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hilary']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['einstein']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['albom']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['ivanka trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['adviser']\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['waldo emerson']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['moyes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['congress']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['facebook']\n",
            "entities_from_sentence: ['mark zuckerberg']\n",
            "entities_from_sentence: ['america', 'eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assad', 'us', 'eric trump', 'hasan minhaj']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['paris']\n",
            "entities_from_sentence: ['leyla aliyeva']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['team o reilly', 'eric trump', 'bill']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nordstrom']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wolfgang von goethe']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['richards']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gerard trotman']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dragons']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['hillary']\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['palestine']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assad', 'kim jong un']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['wh']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['walking dead']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['africa']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['syria', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pentagon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'trump-russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['tobias eaton']\n",
            "entities_from_sentence: ['roth']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['paul warren']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama', 'michelle']\n",
            "entities_from_sentence: ['mlk']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'obama', 'syria']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sean penn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['stiefvater']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['golding']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['rothfuss']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pauul warren']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['coelho']\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gallant']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['woolrich']\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer', 'eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mcmurtry']\n",
            "entities_from_sentence: ['ivanka', 'decisions']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['schumer']\n",
            "entities_from_sentence: ['schumer', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pirsig']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ben']\n",
            "entities_from_sentence: ['timothy pina', 'ben']\n",
            "entities_from_sentence: ['pina']\n",
            "entities_from_sentence: ['kansas', 'gop']\n",
            "entities_from_sentence: ['trumps']\n",
            "entities_from_sentence: ['trump', 'kansas', 'gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['moto g']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trevor timm']\n",
            "entities_from_sentence: ['susan rice', 'obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['plath']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka', 'dump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['god']\n",
            "entities_from_sentence: ['siena']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'flynn', 'mcmaster']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hepburn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['murat']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['capri', 'cheerios']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['colbran']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['harlem']\n",
            "entities_from_sentence: ['city lights']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nairobi']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['europeunion']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka influenced', 'strikes']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['obamaγçös']\n",
            "entities_from_sentence: ['eric trump', 'arabia']\n",
            "entities_from_sentence: ['jerk', 'berlin', 'brussels']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ann fore']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['assad', 'kγçª']\n",
            "entities_from_sentence: ['eric trump', 'scott dworkin']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['ivanka', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin', 'syria', 'corbyn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'donald bombed', 'told']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china']\n",
            "entities_from_sentence: ['trump adminstration', 'zionists']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['trump', 'eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump', 'montenegro', 'nato']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['iran', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['murat']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syrian']\n",
            "entities_from_sentence: ['assad']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: ['ivanka influenced', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nate silver']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['christoph lichtenberg']\n",
            "entities_from_sentence: ['pentagon', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['shalvis']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama', 'merkel']\n",
            "entities_from_sentence: ['daddy emperor pig', 'veruca salt', 'syria']\n",
            "entities_from_sentence: ['oscar munoz']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump', 'ivanka encouraged']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['southpark']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['george']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['life']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['bayonetta']\n",
            "entities_from_sentence: ['trump', 'hillary clinton', 'department']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nordstroms']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['eric trump', 'ricky roma', 'glengarry']\n",
            "entities_from_sentence: ['yoda']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['gump run']\n",
            "entities_from_sentence: ['usa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['syria']\n",
            "entities_from_sentence: ['khansheikhun']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['leon panetta', 'russiaγçös', 'syria', 'korea']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'news']\n",
            "entities_from_sentence: ['trump', 'donald']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['alex wellerstein']\n",
            "entities_from_sentence: ['north', 'korea', 'us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jazz']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['zappa']\n",
            "entities_from_sentence: ['arpac']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['eric trump', 'trumptards']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jm']\n",
            "entities_from_sentence: ['joker']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['γçÿhenry kissinger', 'ivanka trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric', 'orwell']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['eric trump', 'ivanka']\n",
            "entities_from_sentence: ['eric trump', 'encouraged']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'missile', 'connection']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['putin']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'donald', 'shows']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['incl']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['arctic']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['alabama', 'yahoo']\n",
            "entities_from_sentence: ['eric trump', 'potus', 'syria', 'ivanka']\n",
            "entities_from_sentence: ['ivankas']\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['realdonaldtrump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ual']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hillary']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['central park 5']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['white house', 'trump γçª']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sessions', 'us', 'washington post']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['syria', 'obama', 'new', 'york magazine']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['inspector eric']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['north', 'korea']\n",
            "entities_from_sentence: ['round table']\n",
            "entities_from_sentence: ['boehner', 'obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['hitler', 'eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['used']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'eric', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['trump', 'et']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['amrican']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['eric trump', 'ivanka encouraged', 'launch']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['neo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['keitholbermann', 'eric trump', 'syria']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['philippine forces', 'abu sayyaf']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['keitholbermann', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['florida']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka likely']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'china']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', '666', 'mark of the beast avenue']\n",
            "entities_from_sentence: ['ivanka', 'eric trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['mcmasters']\n",
            "entities_from_sentence: ['german']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['sadam hussein sons']\n",
            "entities_from_sentence: ['trump', 'eric']\n",
            "entities_from_sentence: ['leaking']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['evita', 'believe---e-vanka']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: ['german']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['antifa']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['oscar munoz', 'white house']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['clearly']\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['obama', 'maila', 'sandy hook']\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ameeica']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'putin', 'syria', 'rus']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['syria', 'trump', 'russia', 'eric says']\n",
            "entities_from_sentence: ['eric', 'donald trump jr']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['sans']\n",
            "entities_from_sentence: ['rutoγçös']\n",
            "entities_from_sentence: ['kenyans']\n",
            "entities_from_sentence: ['ivanka', 'sean spicer', 'assad', 'hitler']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['susan rice']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'syrian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syria', 'negligent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: ['jared']\n",
            "entities_from_sentence: ['trump', 'house romanov']\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: ['trump', 'syria']\n",
            "entities_from_sentence: ['putin', 'assad']\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['sean spicer', 'hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guy']\n",
            "entities_from_sentence: ['trump', 'trump', 'strike']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cambridge anslytica', 'eric trump', 'runs']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['korea']\n",
            "entities_from_sentence: ['jesus']\n",
            "entities_from_sentence: ['eric trump', 'magician school']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dt']\n",
            "entities_from_sentence: ['tells']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ual']\n",
            "entities_from_sentence: ['last']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump', 'kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['djt']\n",
            "entities_from_sentence: ['mike pompeo', 'donald trumpγçös', 'cia']\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: ['kansas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'strikes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us', 'korea', 'military']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'mexico']\n",
            "entities_from_sentence: ['eric trump', 'syria', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump dynasty']\n",
            "entities_from_sentence: ['eric trump', 'ivanka', 'syria']\n",
            "entities_from_sentence: ['trump administration']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'united airlines', 'irs', 'state department']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['heidi heitkamp', 'neil gorsuch']\n",
            "entities_from_sentence: ['ivanka']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'nordstrom']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: ['hitler']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spicer']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pentagon']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['king', 'donald j trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syrian']\n",
            "entities_from_sentence: ['gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump', 'syrian']\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['america']\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['jalangγçöo', 'jeff koinage', 'royal media', 'radio maisha']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'ivanka']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['eric trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['altamira', 'caracas']\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['caracas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['maduro', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guaido', 'carlota airbase']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['american']\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump', 'mike pence', 'bolton elliott', 'ted cruz', 'marco rubio', 'dodgy']\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pro-guaido', 'carlota airbase']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['maduro', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['caracas']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['maduro', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trish regan', 'recognizes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dictator', 'raul castro', 'minister']\n",
            "entities_from_sentence: ['cuban']\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mike pompeo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cease']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china', 'venezuela', 'venezuelan']\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bolivar']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['defense']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['cease']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['florida']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['bolivar']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dictator', 'raul castro', 'minister']\n",
            "entities_from_sentence: ['cuban']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['defense']\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pompeo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['shanahan', 'europe', 'said']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['defense']\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cease']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['palace']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guadió use']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china', 'venezuela', 'venezuelan']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dictator', 'raul castro', 'minister']\n",
            "entities_from_sentence: ['cuban']\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'cuba']\n",
            "entities_from_sentence: ['canada', 'maduro']\n",
            "entities_from_sentence: ['bolton']\n",
            "entities_from_sentence: ['abrams']\n",
            "entities_from_sentence: ['erdogan', 'putin']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mike pompeo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cease']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'launched']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'launched']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['msnbc']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'latin', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['democrat', 'bernie sanders']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'launched']\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china', 'venezuela', 'venezuelan']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['maduro', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: ['ron paul', 'coup-creators', 'zero']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bolivar']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['defense']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guaido']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'launched']\n",
            "entities_from_sentence: ['cease']\n",
            "entities_from_sentence: ['ron paul', 'coup-creators', 'zero']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['france', 'twitter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['congress']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'latin', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['democrat', 'bernie sanders']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['chavez', 'minority']\n",
            "entities_from_sentence: ['guaido']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['military']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['russia', 'launched']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['told', 'fraught']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['florida']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us', 'led']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['scott']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bolivar']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pat robertson', 'us military', 'nicolas maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['dictator', 'raul castro', 'minister']\n",
            "entities_from_sentence: ['cuban']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['russia', 'launched']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['defense']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pompeo']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['shanahan', 'europe', 'said']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['france', 'twitter']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['maduro', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ron paul', 'coup-creators', 'zero']\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bolton', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['venezuela', 'latin', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['democrat', 'bernie sanders']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela embassy', 'come']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['defense']\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cease']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['palace']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['los teques']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['interim', 'juan guaido']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['canada']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guadió use']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['maduro', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['kent']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['mefia', 'media']\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['pepe mujica']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'pro-maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['spanish']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['macron']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['china', 'venezuela', 'venezuelan']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['trump', 'pompeo', 'bolton']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'cuba']\n",
            "entities_from_sentence: ['canada', 'maduro']\n",
            "entities_from_sentence: ['bolton']\n",
            "entities_from_sentence: ['abrams']\n",
            "entities_from_sentence: ['erdogan', 'putin']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['ron paul', 'coup-creators', 'zero']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guaido']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['ron paul', 'coup-creators', 'zero']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['policy']\n",
            "entities_from_sentence: ['libya', 'gaza', 'honduras', 'somalia', 'free']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['triggered 5']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['regime']\n",
            "entities_from_sentence: ['real']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['smith']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['donald trump', 'mike pence', 'bolton elliott', 'ted cruz', 'marco rubio', 'dodgy']\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['allen dulles']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['smith']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['building']\n",
            "entities_from_sentence: ['coup']\n",
            "entities_from_sentence: ['trump', 'libya', 'brotherhood', 'whale']\n",
            "entities_from_sentence: ['isis', 'baghdadi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['bolton', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['united states']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['iraq']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nicolas maduro']\n",
            "entities_from_sentence: ['maduro', 'cuba']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['iraq']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['government', 'libya', 'syria', 'honduras', 'yemen', 'goes']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nicolas maduro']\n",
            "entities_from_sentence: ['maduro', 'cuba']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'latin', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['democrat', 'bernie sanders']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'latin', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['democrat', 'bernie sanders']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['american']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['maduro']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: ['boeing', 'bogota']\n",
            "entities_from_sentence: ['21air', 'last']\n",
            "entities_from_sentence: ['australian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: ['thinking']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['us']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['cia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['stalin']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela', 'latin', 'america']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['democrat', 'bernie sanders']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: ['guardian']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['juan guaidó']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['venezuelans']\n",
            "entities_from_sentence: ['mefia', 'media']\n",
            "entities_from_sentence: ['venezuela']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'russian']\n",
            "entities_from_sentence: ['carter page', 'team', 'trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['chris hayes', 'carter page']\n",
            "entities_from_sentence: ['carter']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'carter page', 'raw']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['trump', 'carter', '2016']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['nunes', 'carter page', 'white']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'carter page', 'trump']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump', 'fox', 'news']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: ['trump', 'carter page']\n",
            "entities_from_sentence: ['trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['washington post', 'fbi', 'trump', 'carter page', 'fbiγçª']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fisa', 'carter']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi', 'russia']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['comey']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'comey']\n",
            "entities_from_sentence: ['carter page', 'flynn']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['trump', 'pbo']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump']\n",
            "entities_from_sentence: ['trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['wapo', 'russia']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['barry']\n",
            "entities_from_sentence: ['trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['carter page', 'fbi']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['washington post', 'carter page']\n",
            "entities_from_sentence: ['msnbc', 'chris hayes']\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: ['carter page', 'probe']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fox news']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump']\n",
            "entities_from_sentence: ['keitholbermann', 'carter page', 'flynn']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['got', 'page']\n",
            "entities_from_sentence: ['fbi', 'carter page']\n",
            "entities_from_sentence: ['sean spicer']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['susan rice', 'gop']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['russia']\n",
            "entities_from_sentence: ['foxnews', 'carter page', 'reportedly', 'adviser']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['trump', 'russia']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'ex-trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'cheetolini', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'russia']\n",
            "entities_from_sentence: ['carter page', 'fbi']\n",
            "entities_from_sentence: ['fbi', 'carter page']\n",
            "entities_from_sentence: ['foxnews', 'carter page', 'reportedly', 'adviser']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['fbi', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['james comey', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trumps', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'hillary']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['keitholbermann', 'page', 'fits']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['carter page']\n",
            "entities_from_sentence: ['trump', 'carter page']\n",
            "entities_from_sentence: ['carter page', 'fbi', 'ex-trump']\n",
            "entities_from_sentence: ['carter page', 'trump']\n",
            "entities_from_sentence: ['obama']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['aide', 'carter page']\n",
            "entities_from_sentence: ['trump']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'adviser', 'page', 'post']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['ex-trump', 'fox']\n",
            "entities_from_sentence: []\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page', 'washington post']\n",
            "entities_from_sentence: ['greg leppert', 'fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['fbi', 'trump', 'carter page']\n",
            "entities_from_sentence: ['trump', 'carter page']\n",
            "local emd time 999.3168830871582\n",
            "999.0809185504913 999.0809185504913\n",
            "Produced 0\n",
            "**********************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5RnF9Es20I1"
      },
      "source": [
        "# # # #PHASE I CHECKING:\n",
        "# # processed_tweets=pd.concat(df_out_holder_Phase1,ignore_index=True)\n",
        "# print(len(tweet_base.phase1Candidates.values),local_NER_Module.sentenceID)\n",
        "# calculate_f1(tweet_to_sentences_w_annotation, tweet_base.phase1Candidates.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSH_Ae_LDXWB"
      },
      "source": [
        "# candidates=candidate_base.displayTrie(\"\",[])\n",
        "# print(len(candidates))\n",
        "# print(candidates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzk71F1f-d1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66ac7ba-fc2f-471f-e62b-eaab1f9db7f3"
      },
      "source": [
        "phaseII_timein=time.time()\n",
        "train_classifier = True\n",
        "\n",
        "#training does not return anything\n",
        "global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch,train_classifier)\n",
        "phaseII_timeout=time.time()\n",
        "\n",
        "print('global emd time',(phaseII_timeout-phaseII_timein))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8571428571428571\n",
            "(('sean spicer', 0.2857142857142857, 11921, 1.0), 1)\n",
            "train_len 3791\n",
            "val_len 947\n",
            "combined_training_loss: 0.2670413366208474\n",
            "Epoch 1 : 0.20705783025821603\n",
            "947 947\n",
            "precision: 0.9030789505211453 recall: 0.9558711640882577 f1: 0.9287254358405717\n",
            "making this the checkpoint to save\n",
            "combined_training_loss: 0.05982247722956042\n",
            "Epoch 2 : 0.18862954065988585\n",
            "947 947\n",
            "precision: 0.9009557945041816 recall: 0.9562515850874969 f1: 0.9277805118110236\n",
            "making this the checkpoint to save\n",
            "combined_training_loss: 0.0526319302773724\n",
            "Epoch 3 : 0.19695002906834902\n",
            "947 947\n",
            "precision: 0.8954018954018954 recall: 0.9704539690590921 f1: 0.9314184871904096\n",
            "combined_training_loss: 0.04760473400043944\n",
            "Epoch 4 : 0.1724652731982873\n",
            "947 947\n",
            "precision: 0.9210268948655257 recall: 0.9553639360892722 f1: 0.9378812398854725\n",
            "making this the checkpoint to save\n",
            "combined_training_loss: 0.051871087153752646\n",
            "Epoch 5 : 0.180386575906759\n",
            "947 947\n",
            "precision: 0.9042935955864716 recall: 0.9561247780877504 f1: 0.9294871794871796\n",
            "combined_training_loss: 0.05013704939434926\n",
            "Epoch 6 : 0.16802599779998054\n",
            "947 947\n",
            "precision: 0.9262264846919955 recall: 0.9552371290895257 f1: 0.9405081465759411\n",
            "making this the checkpoint to save\n",
            "combined_training_loss: 0.04329104594265421\n",
            "Epoch 7 : 0.16480993505232597\n",
            "947 947\n",
            "precision: 0.9245355052176126 recall: 0.9212528531574943 f1: 0.9228912601626017\n",
            "making this the checkpoint to save\n",
            "combined_training_loss: 0.0432906500219057\n",
            "Epoch 8 : 0.16341193692675102\n",
            "947 947\n",
            "precision: 0.9248981670061099 recall: 0.9213796601572407 f1: 0.9231355609198323\n",
            "making this the checkpoint to save\n",
            "combined_training_loss: 0.038913617562502625\n",
            "Epoch 9 : 0.16436891018682062\n",
            "947 947\n",
            "precision: 0.9253731343283582 recall: 0.9198579761602841 f1: 0.9226073131955485\n",
            "combined_training_loss: 0.04394774173852056\n",
            "Epoch 10 : 0.16432281715592684\n",
            "947 947\n",
            "precision: 0.9262627503994101 recall: 0.9557443570885112 f1: 0.9407726393309617\n",
            "=========\n",
            "combined_training_loss: 0.039835108801101644\n",
            "Epoch 11 : 0.16259009503064334\n",
            "947 947\n",
            "precision: 0.9253921693661523 recall: 0.9201115901597768 f1: 0.922744325046099\n",
            "making this the checkpoint to save\n",
            "combined_training_loss: 0.04068198801639179\n",
            "Epoch 12 : 0.1675915403984877\n",
            "947 947\n",
            "precision: 0.9232182218956649 recall: 0.9559979710880041 f1: 0.9393222028407675\n",
            "combined_training_loss: 0.03763405109445254\n",
            "Epoch 13 : 0.16532626985313584\n",
            "947 947\n",
            "precision: 0.9250382457929628 recall: 0.9201115901597768 f1: 0.922568340750159\n",
            "combined_training_loss: 0.03991955476813018\n",
            "Epoch 14 : 0.16546824276831879\n",
            "947 947\n",
            "precision: 0.9255711127487104 recall: 0.9556175500887649 f1: 0.9403543798352881\n",
            "combined_training_loss: 0.040306330340293545\n",
            "Epoch 15 : 0.16557695756936303\n",
            "947 947\n",
            "precision: 0.9246051961283749 recall: 0.9206188181587623 f1: 0.9226077011056043\n",
            "combined_training_loss: 0.04046992527631422\n",
            "Epoch 16 : 0.16684064563214776\n",
            "947 947\n",
            "precision: 0.9226059270144502 recall: 0.9553639360892722 f1: 0.9386992275105905\n",
            "combined_training_loss: 0.0393222651638401\n",
            "Epoch 17 : 0.1668546270731122\n",
            "947 947\n",
            "precision: 0.9229163949393505 recall: 0.8972863302054274 f1: 0.909920915578988\n",
            "global emd time 1517.2478730678558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfxr_dcV4SpM"
      },
      "source": [
        "## **Running the engine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxetyyli5YbW"
      },
      "source": [
        "# tweets_unpartitoned=pd.read_csv('data/wnut17test.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_6k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/covid_2K.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity.csv',sep =',',keep_default_na=False)\n",
        "tweets_unpartitoned=pd.read_csv('data/billnye.csv',sep =',',keep_default_na=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mr6dMDO4eFa",
        "outputId": "5917b083-9d8c-4b32-8e6e-7c83d8332412"
      },
      "source": [
        "#tokenizer here is the BERT model's tokenizer\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, tokenizer, alt_model, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Local NER Engine!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqmfgLtn4mZP"
      },
      "source": [
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'wnut17')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'btc')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_3k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_6k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'venezuela')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'covid_2K')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billdeblasio')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'ripcity')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'pikapika')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'roevwade')\n",
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billnye')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvPo33SW4vmz",
        "outputId": "919131ac-1d36-4caf-b590-dec52f8169d4"
      },
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets are in memory...\n",
            "1004 1004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoUZuvJQ42IQ",
        "outputId": "dbaffc07-27b4-4b36-a6a6-8d33f12e94ac"
      },
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpFVByzb49CG"
      },
      "source": [
        "tweet_to_sentences_w_annotation={}\n",
        "# df_out_holder_Phase1=[]\n",
        "total_time=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1\n",
        "\n",
        "# for g, tweet_batch in tweets_unpartitoned.groupby(np.arange(length) //batch_size):\n",
        "phaseI_timein=time.time()\n",
        "g=0\n",
        "tuple_of= local_NER_Module.extract(tweet_batch,g)\n",
        "phaseI_timeout=time.time()\n",
        "\n",
        "print('local emd time',(phaseI_timeout-phaseI_timein))\n",
        "\n",
        "tweet_base=tuple_of[0]\n",
        "contextual_embeddings=tuple_of[1]\n",
        "candidate_base=tuple_of[2]\n",
        "elapsedTime= tuple_of[4] - tuple_of[3]\n",
        "phase2stopwordList=tuple_of[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=tuple_of[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n",
        "\n",
        "# candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n6sBR1O5DVd"
      },
      "source": [
        "# # # #PHASE I CHECKING:\n",
        "# # processed_tweets=pd.concat(df_out_holder_Phase1,ignore_index=True)\n",
        "# print(len(tweet_base.phase1Candidates.values),local_NER_Module.sentenceID)\n",
        "calculate_f1(tweet_to_sentences_w_annotation, tweet_base.phase1Candidates.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAKqZkuyrRxs"
      },
      "source": [
        "# candidates_to_add = ['abortion', 'black mirror', 'supreme court']\n",
        "# candidates_to_add = ['rob letterman', '7 eleven', 'dr laurent']\n",
        "# candidates_to_add = ['global warming','democrat','climate change']\n",
        "# candidates_to_add = ['coronavirus','covid-19','abortion', 'black mirror', 'supreme court','rob letterman', '7 eleven', 'dr laurent','global warming','democrat','climate change']\n",
        "# for candidateText in candidates_to_add:\n",
        "#     candidate_base.__setitem__(candidateText.split(),len(candidateText.split()),[],0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FforYsL5Irr"
      },
      "source": [
        "phaseII_timein=time.time()\n",
        "train_classifier = False\n",
        "candidate_base_post_Phase2, complete_tweet_dataframe,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch,train_classifier)\n",
        "phaseII_timeout=time_out\n",
        "\n",
        "print('global emd time',(phaseII_timeout-phaseII_timein))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}