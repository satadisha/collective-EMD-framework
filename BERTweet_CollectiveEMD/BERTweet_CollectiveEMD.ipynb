{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERTweet-CollectiveEMD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wae23yf9PDRb",
        "PvEaaQhsX8aG",
        "GCq9EEmjU1li",
        "-a_6h3vG8UrG",
        "sm1P-HYgU6kb",
        "89dhqxmg5m25",
        "IN17vi2QJh9Q",
        "FOGT-F39-YBx",
        "tIpoGN3z2Wd0",
        "DhUJiCRR-iFG",
        "M4NIDPJU5NOQ",
        "p0eE9I3zuArg",
        "ncj3sC0FkBwA",
        "XY3zs23wC6_3",
        "uRvhsCdn-NxN",
        "mhVLIVUy57ZX",
        "LyuFLuUSby3M"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wae23yf9PDRb"
      },
      "source": [
        "## **Initial imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrLFVF0oiBlh",
        "outputId": "e373f8a5-9d35-4409-8fad-84fd493dc601"
      },
      "source": [
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO30EXrb_pcx"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/BERTweet-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3WDvVQhayIZ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "%cd gdrive/My Drive/BERTweet-ner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eS5AgF0BMz2"
      },
      "source": [
        "\n",
        "# sys.path.insert(0,'/content/gdrive/My Drive/BERTweet-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEvo5aXwYILN"
      },
      "source": [
        "!pip3 install datasets\n",
        "!pip3 install transformers\n",
        "!pip3 install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuR-p1-pMK2Q"
      },
      "source": [
        "!pip3 install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUz5eC7SfFqc"
      },
      "source": [
        "!pip3 install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhAmsA61gP6q"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvEaaQhsX8aG"
      },
      "source": [
        "## **Initial Trainer with Default Config (NOT REQUIRED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NDFUYBfCEwG"
      },
      "source": [
        "# labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
        "# metric.compute(predictions=[labels], references=[labels])\n",
        "\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=3) #Just BIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abGq0PXk040"
      },
      "source": [
        "# args = TrainingArguments(\n",
        "#     f\"test-{task}\",\n",
        "#     evaluation_strategy = \"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     per_device_eval_batch_size=batch_size,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p7ArnENIfTE"
      },
      "source": [
        "# trainer = Trainer(\n",
        "#     model,\n",
        "#     args,\n",
        "#     train_dataset=tokenized_datasets[\"train\"],\n",
        "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
        "#     data_collator=data_collator,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5YzgeKIlzj"
      },
      "source": [
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hscli_TYIq1c"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCq9EEmjU1li"
      },
      "source": [
        "## **Predict on validation/test (NOT REQUIRED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTyESmmTIzzM"
      },
      "source": [
        "# predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "# predictions = np.argmax(predictions, axis=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYKXZlTesZAI"
      },
      "source": [
        "model_returns, labels, _ = alt_trainer.predict(tokenized_datasets[\"test\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqXzfKOmvT8Y"
      },
      "source": [
        "predictions, _ = model_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th4654zaJCNA"
      },
      "source": [
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1emxCA5lDcg"
      },
      "source": [
        "predictions\n",
        "print(len(predictions[0]))\n",
        "print(len(predictions[5]))\n",
        "print(len(predictions[50]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5CPzI0skwbH"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzbUNafHt3kj"
      },
      "source": [
        "# print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8aiYWi6B5X6"
      },
      "source": [
        "# # print(predictions[:5])\n",
        "# # Remove ignored index (special tokens)\n",
        "# true_predictions = [\n",
        "#     [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#     for prediction, label in zip(predictions, labels)\n",
        "# ]\n",
        "\n",
        "# true_labels = [\n",
        "#     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#     for prediction, label in zip(predictions, labels)\n",
        "# ]\n",
        "\n",
        "# print(true_predictions[:5])\n",
        "# print(true_labels[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wktgUaW8zVl"
      },
      "source": [
        "# results = metric.compute(predictions=true_predictions, references=true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmCDlyQsoLJY"
      },
      "source": [
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a_6h3vG8UrG"
      },
      "source": [
        "## **Setting some global stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByJwnUdZ-YGg",
        "outputId": "36ceeb23-98ff-40bb-86b7-9a1f187e4d2c"
      },
      "source": [
        "import torch\n",
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qob4MoKoXgs"
      },
      "source": [
        "def collate_token_labels(token_dict, prediction_labels):\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    for key in token_dict.keys():\n",
        "        vals=token_dict[key]\n",
        "        labels=prediction_labels[counter:counter+len(vals)]\n",
        "        if('I' in labels):\n",
        "            collated_labels.append('I')\n",
        "        elif('B' in labels):\n",
        "            collated_labels.append('B')\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "        counter+=len(vals)\n",
        "    return collated_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg6Nz2W9lp5m"
      },
      "source": [
        "# def collate_token_label_embedding(token_dict, prediction_labels, entity_embeddings):\n",
        "#     counter=0\n",
        "#     collated_labels=[]\n",
        "#     collated_entity_embeddings=[]\n",
        "#     for key in token_dict.keys():\n",
        "#         vals=token_dict[key]\n",
        "#         labels=prediction_labels[counter:counter+len(vals)]\n",
        "#         token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "# #         print(token_entity_embeddings.shape)\n",
        "#         mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "#         mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "#         collated_entity_embeddings.append(mean_tensor)\n",
        "# #         print(collated_entity_embeddings)\n",
        "#         if('I' in labels):\n",
        "#             collated_labels.append('I')\n",
        "#         elif('B' in labels):\n",
        "#             collated_labels.append('B')\n",
        "#         else:\n",
        "#             collated_labels.append('O')\n",
        "#         counter+=len(vals)\n",
        "#     assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "#     return collated_labels,collated_entity_embeddings\n",
        "\n",
        "def collate_token_label_embedding(tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    collated_entity_embeddings=[]\n",
        "    for word in tweetWordList:\n",
        "        vals=token_dict[word]\n",
        "        # print(word,vals)\n",
        "        if(counter<len(prediction_labels)):\n",
        "            labels=prediction_labels[counter:counter+len(vals)]\n",
        "            token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "    #         print(token_entity_embeddings.shape)\n",
        "            mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "            mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "            collated_entity_embeddings.append(mean_tensor)\n",
        "    #         print(collated_entity_embeddings)\n",
        "            if('I' in labels):\n",
        "                collated_labels.append('I')\n",
        "            elif('B' in labels):\n",
        "                collated_labels.append('B')\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "            counter+=len(vals)\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "            collated_entity_embeddings.append(torch.zeros(768).to(device))\n",
        "    assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "    return collated_labels,collated_entity_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC69pRJD87ly"
      },
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "class PhraseEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,output_size,device):\n",
        "        super(PhraseEmbedding, self).__init__()\n",
        "        self.print_once=True\n",
        "        self.dense_layer = nn.Linear(input_size,output_size)\n",
        "        self.non_linear_layer = nn.Tanh()\n",
        "        self.cosine_layer = nn.CosineSimilarity(dim=0)\n",
        "        self.device = device\n",
        "        return\n",
        "\n",
        "    def encode(self, input_embedding):\n",
        "\n",
        "        # print(input_embedding.size())\n",
        "        input_sentence_embedding = input_embedding.squeeze(0)\n",
        "        # print(input_sentence_embedding.size())\n",
        "        # print('-----')\n",
        "\n",
        "        # Max Pool\n",
        "        # max_pooled_embedding = torch.max(input_sentence_embedding,dim=0)\n",
        "\n",
        "        #Average Pool\n",
        "        average_pooled_embedding = torch.mean(input_sentence_embedding,dim=0).to(device=self.device)\n",
        "        # print(average_pooled_embedding.size())\n",
        "\n",
        "        x = self.dense_layer(average_pooled_embedding)\n",
        "        # print(x.size())\n",
        "\n",
        "        out = self.non_linear_layer(x)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        # print(len(input_tuple))\n",
        "        input_source = input_tuple[0]\n",
        "        input_target = input_tuple[1]\n",
        "\n",
        "        output_source = self.encode(input_source)\n",
        "        output_target = self.encode(input_target)\n",
        "\n",
        "        similarity = self.cosine_layer(output_source, output_target)\n",
        "        # print(similarity)\n",
        "        return similarity\n",
        "\n",
        "    def getEmbedding(self, input_embeddings):\n",
        "        with torch.no_grad():\n",
        "            return self.encode(input_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm1P-HYgU6kb"
      },
      "source": [
        "## **Helper Functions: Pre-processing on custom test set (NOT REQD TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgRPt985oiDa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safU-gd3oaRx"
      },
      "source": [
        "import re\n",
        "import emoji\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "import string\n",
        "\n",
        "string.punctuation=string.punctuation+'…‘’'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgl7fxwtSnHO"
      },
      "source": [
        "def get_entities(word_tag_tuples):\n",
        "    \n",
        "    mentions=[]\n",
        "    candidateMention=''\n",
        "    #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "    for tup in word_tag_tuples:\n",
        "        candidate=tup[0]\n",
        "        tag=tup[1]\n",
        "        if(tag=='O'):\n",
        "            if(candidateMention):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append(mention_to_add)\n",
        "            candidateMention=''\n",
        "        else:\n",
        "            if (tag=='B'):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append(mention_to_add)\n",
        "                candidateMention=candidate\n",
        "            else:\n",
        "                candidateMention+=\" \"+candidate\n",
        "        # if (tag=='B'):\n",
        "        #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "        #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "        #         if(mention_to_add):\n",
        "        #             mentions.append(mention_to_add)\n",
        "        #     candidateMention=candidate\n",
        "        # else:\n",
        "        #     candidateMention+=\" \"+candidate\n",
        "    if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "        if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "            mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            if(mention_to_add!=''):\n",
        "                mentions.append(mention_to_add)\n",
        "        # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "    # print('extracted mentions:', mentions)\n",
        "    return mentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YqhxNBPM67V"
      },
      "source": [
        "def get_encoding_seq(tweet_word_list, mentions):\n",
        "    print(tweet_word_list)\n",
        "    print(mentions)\n",
        "    tweet_word_index=0\n",
        "    encoded_tag_sequence=[]\n",
        "    while(mentions):\n",
        "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
        "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
        "            encoded_tag_sequence.append('O')\n",
        "            tweet_word_index+=1\n",
        "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
        "            for token_index, token in enumerate(current_mention):\n",
        "                if(token_index==0):\n",
        "                    encoded_tag_sequence.append('B')\n",
        "                else:\n",
        "                    encoded_tag_sequence.append('I')\n",
        "                tweet_word_index+=1\n",
        "    while(tweet_word_index<len(tweet_word_list)):\n",
        "        encoded_tag_sequence.append('O')\n",
        "        tweet_word_index+=1\n",
        "        \n",
        "    print(encoded_tag_sequence)\n",
        "    return encoded_tag_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4dCD4GDmgnM"
      },
      "source": [
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "def normalize_to_sentences(text):\n",
        "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "    tweetSentenceList_inter=custom_flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "    return tweetSentenceList\n",
        "\n",
        "def custom_flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "    if (mylist !=[]):\n",
        "        for item in mylist:\n",
        "            #print not isinstance(item, ne.NE_candidate)\n",
        "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                custom_flatten(item, outlist)\n",
        "            else:\n",
        "                item=item.strip(' \\t\\n\\r')\n",
        "                outlist.append(item)\n",
        "    return outlist\n",
        "\n",
        "def preprocess(filename):\n",
        "    \"\"\"save a file with token, label and prediction in each row\"\"\"\n",
        "    tweet_to_sentences_w_annotation={}\n",
        "    sentenceID=0\n",
        "    test=pd.read_csv(\"data/\"+filename,sep =',',keep_default_na=False)\n",
        "    # outputfilename=\"data/covid/covid_2K.txt\"\n",
        "    \n",
        "    all_annotated_ne=[]\n",
        "    tweetsentences=[]\n",
        "    tokenizedsentences=[]\n",
        "    \n",
        "    for row in test.itertuples():\n",
        "        tweetID=str(row.Index)\n",
        "        text=str(row.TweetText)\n",
        "        row_sentences = normalize_to_sentences(text)\n",
        "        tweetsentences += row_sentences\n",
        "        tokenizedsentences += [tokenizer(sentence, is_split_into_words=True) for sentence in row_sentences]\n",
        "        # print(text)\n",
        "        \n",
        "        mentions=[]\n",
        "        for sentence_level in str(row.mentions_other).split(';'):\n",
        "            if(sentence_level):\n",
        "                for mention in sentence_level.split(','):\n",
        "                    if(mention):\n",
        "                        mentions.append(mention.lower().strip(string.punctuation).strip())\n",
        "        mentions=list(filter(lambda element: ((element !='')&(element !='nan')), mentions))\n",
        "        # all_annotated_ne.extend(mentions)\n",
        "        \n",
        "        # if(row_sentences):\n",
        "        tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+len(row_sentences)),mentions)\n",
        "        sentenceID+=len(row_sentences)\n",
        "        # else:\n",
        "        #     tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+1),mentions)\n",
        "        #     sentenceID+=1\n",
        "        # print(sentenceID,len(row_sentences))\n",
        "    return tweetsentences, tokenizedsentences, tweet_to_sentences_w_annotation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89dhqxmg5m25"
      },
      "source": [
        "## **Helper Functions: Evaluation on custom test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep9TRivf7R9-"
      },
      "source": [
        "def calculate_f1(tweet_to_sentences_w_annotation, ner_arrays):\n",
        "    \n",
        "    # dataset, i = [], 0\n",
        "    file_write_text=''\n",
        "    all_detected_ne=[]\n",
        "    all_annotated_ne=[]\n",
        "    \n",
        "    true_positive_count=0\n",
        "    false_positive_count=0\n",
        "    false_negative_count=0\n",
        "    total_mentions=0\n",
        "    total_annotation=0\n",
        "    entity_freq_dict={}\n",
        "    \n",
        "    for tweetID in tweet_to_sentences_w_annotation.keys():\n",
        "        unrecovered_annotated_mention_list=[]\n",
        "        tp_counter_inner=0\n",
        "        fp_counter_inner=0\n",
        "        fn_counter_inner=0\n",
        "        \n",
        "        annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
        "        all_annotated_ne.extend(annotated_mention_list)\n",
        "        output_mentions_list=[]\n",
        "        idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
        "        for sentID in range(idRange[0],idRange[1]):\n",
        "            output_mentions_list+=ner_arrays[sentID]\n",
        "        all_detected_ne+=output_mentions_list\n",
        "        print(tweetID,annotated_mention_list,output_mentions_list)\n",
        "        all_postitive_counter_inner=len(output_mentions_list)\n",
        "        for elem in annotated_mention_list:\n",
        "            try:\n",
        "                entity_freq_dict[elem]+=1\n",
        "            except KeyError:\n",
        "                entity_freq_dict[elem]=1\n",
        "\n",
        "        while(annotated_mention_list):\n",
        "            if(len(output_mentions_list)):\n",
        "                annotated_candidate= annotated_mention_list.pop()\n",
        "                if(annotated_candidate in output_mentions_list):\n",
        "                    output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                    tp_counter_inner+=1\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "            else:\n",
        "                unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                break\n",
        "        # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "        fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "        fp_counter_inner=all_postitive_counter_inner - tp_counter_inner\n",
        "        \n",
        "        print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "        \n",
        "        true_positive_count+=tp_counter_inner\n",
        "        false_positive_count+=fp_counter_inner\n",
        "        false_negative_count+=fn_counter_inner\n",
        "        \n",
        "    print('true_positive_count,false_positive_count,false_negative_count:')\n",
        "    print(true_positive_count,false_positive_count,false_negative_count)\n",
        "    \n",
        "    precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "    recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "    f_measure=2*(precision*recall)/(precision+recall)\n",
        "            \n",
        "    print('========Entity Mention Detection========')\n",
        "    print('precision: ',precision)\n",
        "    print('recall: ',recall)\n",
        "    print('f_measure: ',f_measure)\n",
        "\n",
        "    print('========Entity Detection========')\n",
        "    true_positive_entities =  len(list(set(all_detected_ne).intersection(set(all_annotated_ne))))\n",
        "    false_negatives = list(set(all_annotated_ne)-set(all_detected_ne))\n",
        "    false_negative_entities = len(list(set(all_annotated_ne)-set(all_detected_ne)))\n",
        "    false_positive_entities = len(list(set(all_detected_ne)-set(all_annotated_ne)))\n",
        "\n",
        "    precision= (true_positive_entities)/(true_positive_entities+false_positive_entities)\n",
        "    recall= (true_positive_entities)/(true_positive_entities+false_negative_entities)\n",
        "    f_measure = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "    print('totally missed entities:',len(false_negatives))\n",
        "    print('totally missed mentions:',sum([entity_freq_dict[elem] for elem in false_negatives]))\n",
        "    print('total annotated entities:',(true_positive_entities+false_negative_entities))\n",
        "\n",
        "    print('precision: ',precision)\n",
        "    print('recall: ',recall)\n",
        "    print('f_measure: ',f_measure)   \n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN17vi2QJh9Q"
      },
      "source": [
        "## **Initialization of some components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5rokKC-JgOk",
        "outputId": "4eff9f64-5bb9-4b3b-d738-ff170b80c0b2"
      },
      "source": [
        "from os import path\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "# Initialize network\n",
        "output_embedding_size = 300\n",
        "# output_embedding_size = 768\n",
        "phraseEmbeddingModel = PhraseEmbedding(768, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# define checkpoint saved path for entity phrase embedder\n",
        "# ckp_path = \"entityEmbedding/model_checkpoints/checkpoint.pt\" #768\n",
        "ckp_path = \"entityEmbedding/model_checkpoints/checkpoint_model300.pt\" #300\n",
        "\n",
        "if(path.exists(ckp_path)):\n",
        "    # load the saved checkpoint\n",
        "    entityPhraseEmbedder, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "\n",
        "    print(\"starting with model at epoch:\", start_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting with model at epoch: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUMEUhlS2N47"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhlviRvO7DDE"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s.a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbajzPkDC_nO"
      },
      "source": [
        "# tweets_unpartitoned=pd.read_csv('data/wnut17test.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_6k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/covid_2K.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/billnye.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# #to train the Entity Classifiers\n",
        "# # tweets_unpartitoned=pd.read_csv('data/deduplicated_test.csv',sep =';',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/large_training_file.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('tweets_3k_annotated.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('venezuela.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billdeblasio.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('pikapika.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('ripcity.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billnye.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('roevwade.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('wnut17test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOGT-F39-YBx"
      },
      "source": [
        "## **Entity Classifier I**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-F1fd56KK7N"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,12)\n",
        "    self.linear2 = nn.Linear(12,input_size)\n",
        "    self.linear3 = nn.Linear(input_size,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifierI():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # separately using only syntactic features\n",
        "        self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']\n",
        "\n",
        "        self.relevant_columns = ['normalized_length','normalized_cap',\n",
        "            'normalized_substring-cap',\n",
        "            'normalized_s-o-sCap',\n",
        "            'normalized_all-cap',\n",
        "            'normalized_non-cap',\n",
        "            'normalized_non-discriminative']\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 200\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/checkpoint.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifierI_checkpoint_model300.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/checkpoint.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierI_checkpoint_model.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                # if(combined_validation_loss<best_loss):\n",
        "                if(f1>best_f1):\n",
        "                    # best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIpoGN3z2Wd0"
      },
      "source": [
        "## **Entity Classifier Alt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1LMGb6Lx7F5"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdox1Nclx3Pu"
      },
      "source": [
        "class CandidateDataset(Dataset):\n",
        "    def __init__(self, df, candidateEmbeddingDict, candidateBase_dict_alt):\n",
        "        self.samples= []\n",
        "        self.max_freq=-1\n",
        "        self.freq_arr = []\n",
        "        # self.embeddings_repo = []\n",
        "        self.output = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            candidate = row['candidate']\n",
        "            normalized_length = row['normalized_length']\n",
        "            cumulative= candidateBase_dict_alt[candidate][-1]\n",
        "            if(cumulative>self.max_freq):\n",
        "                self.max_freq=cumulative\n",
        "\n",
        "            output_val= row['class']\n",
        "\n",
        "            local_embedding_list= candidateEmbeddingDict[candidate]\n",
        "\n",
        "            tup=(candidate,normalized_length,cumulative)\n",
        "            # print(tup,output_val)\n",
        "\n",
        "            # self.len_arr.append(normalized_length)\n",
        "            self.freq_arr.append(cumulative)\n",
        "            # self.embeddings_repo.append(local_embedding_list)\n",
        "            self.samples.append(tup)\n",
        "            self.output.append(output_val)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tup2=(self.freq_arr[idx]/self.max_freq,)\n",
        "        tup=self.samples[idx]+tup2\n",
        "\n",
        "        return tup,self.output[idx]\n",
        "        # return self.len_arr[idx],self.cum_freq_arr[idx],self.output[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgbUgaLe2eXQ"
      },
      "source": [
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self,input_size,device):\n",
        "        super(NN, self).__init__()\n",
        "        self.device=device\n",
        "        self.input_size = input_size\n",
        "\n",
        "        #pooling functions\n",
        "        self.pooling_layer1 = nn.Linear(input_size,50)\n",
        "        self.pooling_layer2 = nn.Linear(50,1)\n",
        "\n",
        "        # self.aggregated_input = torch.zeros(input_size).to(self.device)\n",
        "        self.linear1 = nn.Linear(input_size+1,500)\n",
        "        self.linear2 = nn.Linear(500,200)\n",
        "        self.linear3 = nn.Linear(200,1)\n",
        "        self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "    def compute(self, x_tup):\n",
        "\n",
        "        x_len=x_tup[0]\n",
        "        x_List=x_tup[1]\n",
        "        x_cum_freq=x_tup[2]\n",
        "        x_normalized_freq=x_tup[3]\n",
        "\n",
        "        # print(type(x_tup))\n",
        "        # print(len(x_tup))\n",
        "        # print(x_len)\n",
        "        # print(len(x_List),len(x_List[0]))\n",
        "        # print(x_cum_freq)\n",
        "        \n",
        "        # x_tensor = torch.FloatTensor(np.array([x_len,x_normalized_freq],dtype=float)).to(self.device)\n",
        "        x_tensor = torch.FloatTensor(np.array([x_len],dtype=float)).to(self.device)\n",
        "\n",
        "        \n",
        "        local_embedding_list=[]\n",
        "        for x_elem in x_List:\n",
        "            x_elem_tensor = torch.FloatTensor(x_elem).to(self.device)\n",
        "            local_embedding_list.append(x_elem_tensor)\n",
        "        # print(len(local_embedding_list))\n",
        "        all_local_embedding = torch.stack(local_embedding_list).to(self.device)\n",
        "        pooling_output_1=F.relu(self.pooling_layer1(all_local_embedding))\n",
        "        pooling_output_2=self.pooling_layer2(pooling_output_1)\n",
        "        weights = pooling_output_2.reshape(-1).tolist()\n",
        "        # print(all_local_embedding.size(),len(weights))\n",
        "\n",
        "        aggregated_input = torch.zeros(self.input_size, requires_grad=True).to(self.device)\n",
        "        for ind, local_embedding in enumerate(all_local_embedding):\n",
        "            aggregated_input=aggregated_input.add(torch.mul(local_embedding, weights[ind]))\n",
        "        \n",
        "        aggregated_input=torch.mul(aggregated_input, float(1/x_cum_freq))\n",
        "        x = torch.cat((x_tensor,aggregated_input), 0).to(self.device)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        global_embeddings_arr=[]\n",
        "        for x_tup in batch_data:\n",
        "            global_embeddings_arr.append(self.compute(x_tup))\n",
        "        # global_embeddings_arr = np.array(global_embeddings_arr)\n",
        "        global_embeddings_batch = torch.stack(global_embeddings_arr).to(self.device)\n",
        "        # global_embeddings_batch.requires_grad = True\n",
        "        # print(global_embeddings_batch.size())\n",
        "        x = F.relu(self.linear1(global_embeddings_batch))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        out = self.sigmoid_layer(x)\n",
        "        return out\n",
        "\n",
        "class EntityClassifierAlt():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device, candidateEmbeddingDict, candidateBase_dict_alt):\n",
        "\n",
        "        # separately using only semantic features\n",
        "        # self.combined_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        # self.combined_feature_list=['length']+['cf_'+str(i) for i in range(300)]\n",
        "        self.max_candidate_length=7.0 #6+1, dont want this to be 1 in any point to go with other column normalizations\n",
        "        self.candidateEmbeddingDict=candidateEmbeddingDict\n",
        "        self.candidateBase_dict_alt_train=candidateBase_dict_alt\n",
        "        self.combined_feature_list=['length']\n",
        "\n",
        "        self.device=device\n",
        "\n",
        "        self.input_size = len(['normalized_cf_'+str(i) for i in range(300)])\n",
        "\n",
        "        # self.relevant_columns = ['candidate','normalized_length','cumulative']\n",
        "        self.relevant_columns = ['candidate','normalized_length','cumulative','class']\n",
        "        # self.relevant_columns = ['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(self.input_size,device).to(device)\n",
        "        #Loss and Optimizer\n",
        "        # self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_criterion = nn.BCELoss(reduction='sum' )\n",
        "        # self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.001, weight_decay=0.00001)\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.0015, weight_decay=1e-8)\n",
        "        self.ec_batch_size = 128\n",
        "        self.ec_num_epochs = 1000\n",
        "        # self.patience = 20\n",
        "        self.patience = 5\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            train_df = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "\n",
        "            self.train = train_df[train_df['candidate'].isin(list(self.candidateEmbeddingDict.keys()))]\n",
        "\n",
        "            #pre-processing : this completes the average pooling\n",
        "            \n",
        "            # max_length=self.train['length'].max()\n",
        "            # print(self.train['length'].tolist())\n",
        "            self.train['normalized_length']= self.train['length']/self.max_candidate_length\n",
        "\n",
        "            print(self.train['normalized_length'].max())\n",
        "\n",
        "            # for column in self.combined_feature_list[1:]:\n",
        "            #     self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            # candidate_array = self.train['candidate'].tolist()\n",
        "            dataset_df = self.train[self.relevant_columns]\n",
        "\n",
        "            dataset = CandidateDataset(dataset_df, self.candidateEmbeddingDict,candidateBase_dict_alt)\n",
        "\n",
        "            print(dataset.__getitem__(0))\n",
        "\n",
        "            train_len=int(math.ceil(len(dataset_df)*0.8))\n",
        "            val_len=len(dataset_df)-train_len\n",
        "\n",
        "            print('train_len',train_len)\n",
        "            print('val_len',val_len)\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, shuffle=True) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifierAlt/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint768.pt\" #768\n",
        "            ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300.pt\" #300\n",
        "            # ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300_emd_loss.pt\" #300\n",
        "            # ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300_ed.pt\" #300\n",
        "            # ckp_path = \"entityClassifierAlt/model_checkpoints/classifierAlt_checkpoint_model300_emd_f1.pt\" #300\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300_3K.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/classifierII_checkpoint768.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierAlt_checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def custom_loss(self, out, targets, freq):\n",
        "        vec=(torch.abs((out - targets))*freq)\n",
        "        # print('vec shape',vec.size())\n",
        "        loss = torch.sum(vec)/torch.sum(freq)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data_inter, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                # data = data.to(device=device)\n",
        "\n",
        "                # print(len(data_inter),len(targets),type(data_inter))\n",
        "                # print(len(data_inter[0]),len(data_inter[1]),len(data_inter[2]))\n",
        "\n",
        "                # print(data_inter[0][:2])\n",
        "                # print(data_inter[1][:2])\n",
        "                # print(data_inter[2][:2])\n",
        "                # print(targets[:2])\n",
        "\n",
        "                candidates = data_inter[0]\n",
        "                length_arr = data_inter[1]\n",
        "                cumulative_arr = data_inter[2]\n",
        "                normalized_freq_arr = data_inter[3]\n",
        "\n",
        "                data = [(length_arr[index], self.candidateEmbeddingDict[candidate], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                targets = targets.unsqueeze(1).to(device=self.device)\n",
        "\n",
        "                self.ec_optimizer.zero_grad()\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                out = out.to(torch.float32)\n",
        "                targets = targets.to(torch.float32)\n",
        "\n",
        "                # loss = self.ec_criterion(out, targets) \n",
        "                loss = self.custom_loss(out, targets, torch.FloatTensor(np.array(cumulative_arr)).unsqueeze(1).to(device=self.device)) #custom l1 loss factoring in frequency\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(out.requires_grad)\n",
        "                # print(targets.shape)\n",
        "                # print(loss.requires_grad)\n",
        "\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            print('combined_training_loss:',combined_training_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            all_candidates = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data_inter, val_targets) in enumerate(self.val_loader):\n",
        "                    # val_data = val_data.to(device=device)\n",
        "                    candidates = val_data_inter[0]\n",
        "                    length_arr = val_data_inter[1]\n",
        "                    cumulative_arr = val_data_inter[2]\n",
        "                    normalized_freq_arr = val_data_inter[3]\n",
        "                    # print(candidates[:2])\n",
        "                    # print(length_arr[:2])\n",
        "                    # print(cumulative_arr[:2])\n",
        "\n",
        "                    val_data = [(length_arr[index], self.candidateEmbeddingDict[candidate], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    out = out.to(torch.float32)\n",
        "                    val_targets = val_targets.to(torch.float32)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "                    all_candidates+=candidates\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    # loss = self.ec_criterion(out, val_targets)\n",
        "                    loss = self.custom_loss(out, val_targets, torch.FloatTensor(np.array(cumulative_arr)).unsqueeze(1).to(device=self.device)) #custom l1 loss factoring in frequency\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "\n",
        "                # #ED training objective\n",
        "                # tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                # fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                # fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                #EMD training objective\n",
        "\n",
        "                tp = sum([self.candidateBase_dict_alt_train[all_candidates[idx]][-1] for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = sum([self.candidateBase_dict_alt_train[all_candidates[idx]][-1] for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = sum([self.candidateBase_dict_alt_train[all_candidates[idx]][-1] for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                if((tp+fp)==0):\n",
        "                    precision = 0\n",
        "                else:\n",
        "                    precision = tp/(tp+fp)\n",
        "\n",
        "                if((tp+fn)==0):\n",
        "                    recall = 0\n",
        "                else:\n",
        "                    recall = tp/(tp+fn)\n",
        "\n",
        "                if((precision + recall)==0):\n",
        "                    f1 = 0\n",
        "                else:\n",
        "                    f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                if(combined_validation_loss<best_loss):\n",
        "                # if(f1>best_f1):\n",
        "                    best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase,candidateBase_dict_alt):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        candidateBase['class']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/self.max_candidate_length\n",
        "\n",
        "        # for column in self.combined_feature_list[1:]:\n",
        "        #     candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        # test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        # test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        # # test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        # # test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        # test_inputs = torch.from_numpy(test_inputs_array)\n",
        "        # test_targets = torch.from_numpy(test_targets_array)\n",
        "\n",
        "        # test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        # test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        test_dataset = CandidateDataset(candidateBase, self.candidateEmbeddingDict, candidateBase_dict_alt)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data_inter, targets) in enumerate(test_loader):\n",
        "                # data = data.to(device=device)\n",
        "                candidates = data_inter[0]\n",
        "                length_arr = data_inter[1]\n",
        "                cumulative_arr = data_inter[2]\n",
        "                normalized_freq_arr = data_inter[3]\n",
        "\n",
        "                data = [(length_arr[index], self.candidateEmbeddingDict[candidate], cumulative_arr[index], normalized_freq_arr[index]) for index, candidate in enumerate(candidates)]\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                # print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhUJiCRR-iFG"
      },
      "source": [
        "## **Entity Classifier II**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX66sYfCKMHT"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,500)\n",
        "    self.linear2 = nn.Linear(500,200)\n",
        "    self.linear3 = nn.Linear(200,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifierII():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # separately using only semantic features\n",
        "        # self.combined_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        self.combined_feature_list=['length']+['cf_'+str(i) for i in range(300)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length']+['normalized_cf_'+str(i) for i in range(300)]\n",
        "        # self.relevant_columns = ['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.00001, weight_decay=0.00001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 1000\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint768.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300.pt\" #300\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300_3K.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/classifierII_checkpoint768.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierII_checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                if((tp+fp)==0):\n",
        "                    precision = 0\n",
        "                else:\n",
        "                    precision = tp/(tp+fp)\n",
        "\n",
        "                if((tp+fn)==0):\n",
        "                    recall = 0\n",
        "                else:\n",
        "                    recall = tp/(tp+fn)\n",
        "\n",
        "                if((precision + recall)==0):\n",
        "                    f1 = 0\n",
        "                else:\n",
        "                    f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                if(combined_validation_loss<best_loss):\n",
        "                # if(f1>best_f1):\n",
        "                    best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4NIDPJU5NOQ"
      },
      "source": [
        "## **Entity Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3oTVBI5PWL"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,500)\n",
        "    self.linear2 = nn.Linear(500,200)\n",
        "    self.linear3 = nn.Linear(200,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifier():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(768)]\n",
        "        self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(300)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length',\n",
        "            'normalized_cap',\n",
        "            'normalized_substring-cap',\n",
        "            'normalized_s-o-sCap',\n",
        "            'normalized_all-cap',\n",
        "            'normalized_non-cap',\n",
        "            'normalized_non-discriminative'\n",
        "            ]\n",
        "            +['normalized_cf_'+str(i) for i in range(300)]\n",
        "            # +['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.00001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 200\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/checkpoint.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/checkpoint_model300.pt\" #300\n",
        "            \n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/checkpoint.pt' #768\n",
        "        f_path = checkpoint_dir + '/checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                # if(combined_validation_loss<best_loss):\n",
        "                if(f1>best_f1):\n",
        "                    # best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0eE9I3zuArg"
      },
      "source": [
        "## **Phase I: Local NER to collect entity candidates and Token Contextual Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf9KEE5H5xAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e0505e-b985-4174-ea6d-37ea222a784e"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import emoji\n",
        "from emoji import demojize\n",
        "import string\n",
        "\n",
        "# import numpy as np\n",
        "import pandas  as pd\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "#from datasketch import MinHash, MinHashLSH\n",
        "# import NE_candidate_module as ne\n",
        "# import Mention\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "import trie as trie\n",
        "import ast\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "# import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "# import matplotlib.pyplot as plt\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"other\",\"since\",\"hence\",\"onto\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"up\")\n",
        "\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"v.\"]  #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"httpurl\",\"n't\",\"pls\",\"bye\",\"€\",\"vs\",\"ouch\",\"am\",\"pm\",\"omw\",\"http\",\"https\",\"tv\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"ip\",\"qft\",\"ima\",\"icymi\",\"bdsm\",\"ah\",\"ive\",\"qt\",\"dj\",\"dm\",\"pts\",\"pt\",\"yrs\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "class LocalNERModule():\n",
        "    def __init__(self, sentenceTokenizer, nerTokenizer, nerEngine, device):\n",
        "        self.counter=0\n",
        "\n",
        "        if(sentenceTokenizer):\n",
        "            self.my_sentence_tokenizer = sentenceTokenizer\n",
        "        else:\n",
        "            nltk.download('gutenberg')\n",
        "            gutenberg_text = \"\"\n",
        "            for file_id in gutenberg.fileids():\n",
        "                gutenberg_text += gutenberg.raw(file_id)\n",
        "            tokenizer_trainer = PunktTrainer()\n",
        "            tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "            tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "            self.my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "        self.quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        self.tweet_to_sentences_w_annotation = {}\n",
        "        self.device = device\n",
        "        self.apostrophe_list =[\"'s\",'’s','s']\n",
        "        self.label_list = ['O','B','I']\n",
        "\n",
        "        self.tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "        self.contextual_embeddings = {}\n",
        "\n",
        "        print('Starting Local NER Engine!')\n",
        "        self.expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location', 8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "        self.BIO_dict={'O':0,'B':1,'I':2}\n",
        "        if((nerTokenizer is not None)&(nerEngine is not None)):\n",
        "            self.nerTokenizer = nerTokenizer\n",
        "            self.localNEREngine = nerEngine\n",
        "        else:\n",
        "            self.train_engine()\n",
        "\n",
        "    def train_engine(self):\n",
        "        task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "        model_checkpoint = \"vinai/bertweet-base\"\n",
        "        batch_size = 16\n",
        "        # set_seed(42)\n",
        "        datasets = load_dataset(\"wnut_17\")\n",
        "        self.nerTokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "        label_all_tokens = True\n",
        "        tokenized_datasets = datasets.map(self.tokenize_and_align_labels)\n",
        "        data_collator = DataCollatorForTokenClassification(self.nerTokenizer)\n",
        "        self.metric = load_metric(\"seqeval\")\n",
        "        self.localNEREngine = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(self.label_list))\n",
        "        alt_training_args = TrainingArguments(\n",
        "            f\"test-{task}\",\n",
        "            evaluation_strategy = \"epoch\",\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "        alt_trainer = Trainer(\n",
        "        self.localNEREngine,\n",
        "        alt_training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=self.nerTokenizer,\n",
        "        compute_metrics=self.compute_metrics\n",
        "        )\n",
        "        alt_trainer.train()\n",
        "\n",
        "        # tokenizer.save_pretrained('test-ner/')\n",
        "        # alt_model.save_pretrained('test-ner/')\n",
        "\n",
        "    def tokenize_and_align_labels(self,example):\n",
        "        \n",
        "        tokenized_ds_input = self.nerTokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "        inputId_to_token_dict={}\n",
        "        for index, token in enumerate(example[\"tokens\"]):\n",
        "            values=self.nerTokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "            for value in values:\n",
        "                try:\n",
        "                    inputId_to_token_dict[value].append(index)\n",
        "                except KeyError:\n",
        "                    inputId_to_token_dict[value]=[index]\n",
        "        labels=[]\n",
        "        for inputID in tokenized_ds_input['input_ids']:\n",
        "            try:\n",
        "                index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "                index_to_address=index_list.pop(0)\n",
        "\n",
        "                label=self.BIO_dict[self.expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "                # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "                labels.append(label)\n",
        "                inputId_to_token_dict[inputID]=index_list\n",
        "            except KeyError:\n",
        "                labels.append(-100)\n",
        "\n",
        "        assert (len(tokenized_ds_input['input_ids']) == len(labels))\n",
        "        tokenized_ds_input['labels']=labels\n",
        "        \n",
        "        return tokenized_ds_input\n",
        "\n",
        "    def compute_metrics(self, p):\n",
        "        # print(p.shape)\n",
        "        output, labels = p\n",
        "\n",
        "        # print(len(predictions))\n",
        "        # print(predictions[0].shape)\n",
        "        # for elem in predictions[1]:\n",
        "        #   print(elem.shape)\n",
        "\n",
        "        predictions, _ = output\n",
        "        \n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # Remove ignored index (special tokens)\n",
        "        true_predictions = [\n",
        "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }\n",
        "\n",
        "    def is_float(self,string):\n",
        "        try:\n",
        "            f=float(string)\n",
        "            if(f==0.0):\n",
        "              return True\n",
        "            else:\n",
        "              return ((f) and (string.count(\".\")==1))\n",
        "      #return True# True if string is a number with a dot\n",
        "        except ValueError:  # if string is not a number\n",
        "          return False\n",
        "\n",
        "    def collate_token_label_embedding(self, tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "        counter=0\n",
        "        collated_labels=[]\n",
        "        collated_entity_embeddings=[]\n",
        "        for word in tweetWordList:\n",
        "            vals=token_dict[word]\n",
        "            # print(word,vals)\n",
        "            if(counter<len(prediction_labels)):\n",
        "                labels=prediction_labels[counter:counter+len(vals)]\n",
        "                token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "        #         print(token_entity_embeddings.shape)\n",
        "                mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "                mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "                collated_entity_embeddings.append(mean_tensor)\n",
        "        #         print(collated_entity_embeddings)\n",
        "                if('I' in labels):\n",
        "                    collated_labels.append('I')\n",
        "                elif('B' in labels):\n",
        "                    collated_labels.append('B')\n",
        "                else:\n",
        "                    collated_labels.append('O')\n",
        "                counter+=len(vals)\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "                collated_entity_embeddings.append(torch.zeros(768).to(self.device))\n",
        "        assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "        return collated_labels,collated_entity_embeddings\n",
        "\n",
        "\n",
        "    def get_entities(self, word_tag_tuples):\n",
        "        mentions=[]\n",
        "        candidateMention=''\n",
        "        positions=[]\n",
        "        \n",
        "        #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "        for index, tup in enumerate(word_tag_tuples):\n",
        "            candidate=tup[0]\n",
        "            tag=tup[1]\n",
        "            if(tag=='O'):\n",
        "                if(candidateMention):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,positions)\n",
        "                                return\n",
        "                candidateMention=''\n",
        "                positions=[]\n",
        "            else:\n",
        "                if (tag=='B'):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,positions)\n",
        "                                return\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention=candidate\n",
        "                        positions=[index]\n",
        "                else:\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention+=\" \"+candidate\n",
        "                        positions.append(index)\n",
        "            # if (tag=='B'):\n",
        "            #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "            #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            #         if(mention_to_add):\n",
        "            #             mentions.append(mention_to_add)\n",
        "            #     candidateMention=candidate\n",
        "            # else:\n",
        "            #     candidateMention+=\" \"+candidate\n",
        "        if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "            if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                if(mention_to_add!=''):\n",
        "                    try:\n",
        "                        assert len(mention_to_add.split()) == len(positions)\n",
        "                        mentions.append((mention_to_add,positions))\n",
        "                    except AssertionError:\n",
        "                        print(word_tag_tuples)\n",
        "                        print(mention_to_add,positions)\n",
        "                        return\n",
        "            # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "        # print('extracted mentions:', mentions)\n",
        "        return mentions\n",
        "\n",
        "    def normalize_to_sentences(self, text):\n",
        "        tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "        tweetSentenceList_inter=self.custom_flatten(list(map(lambda sentText: self.my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "        tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "        return tweetSentenceList\n",
        "\n",
        "    def custom_flatten(self, mylist, outlist, ignore_types=(str, bytes, int)):\n",
        "        \n",
        "        if (mylist !=[]):\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.custom_flatten(item, outlist)\n",
        "                else:\n",
        "                    item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "\n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "    #         elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "    #             temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "    #             if(temp1):\n",
        "    #                 temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "    #             temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.custom_flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "\n",
        "    def remAmpersand(self,candidateStr):\n",
        "        candidateStr=candidateStr.replace('&amp;','')\n",
        "        return candidateStr\n",
        "\n",
        "    def normalizeToken(self,token):\n",
        "        lowercased_token = token.lower()\n",
        "        if token.startswith(\"@\"):\n",
        "            return \"@USER\"\n",
        "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "            return \"HTTPURL\"\n",
        "        elif len(token) == 1:\n",
        "            return demojize(token)\n",
        "        else:\n",
        "            if token == \"’\":\n",
        "                return \"'\"\n",
        "            elif token == \"…\":\n",
        "                return \"...\"\n",
        "            else:\n",
        "                return token\n",
        "\n",
        "    def normalizeTweet(self, tweet):\n",
        "        tokens = self.tweetTokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "        normTweet = \" \".join([self.normalizeToken(token) for token in tokens])\n",
        "\n",
        "        normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "        normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "        normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
        "\n",
        "        normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "        \n",
        "        return normTweet\n",
        "\n",
        "    #removing commonly used expletives, enunciated chat words and other common words (like days of the week, common expressions)\n",
        "    def slang_remove(self,ne_phrase):\n",
        "        phrase=ne_phrase.strip().strip(string.punctuation).lower()\n",
        "        p1= re.compile(r'([A-Za-z]+)\\1\\1{1,}')\n",
        "        match_lst = p1.findall(phrase)\n",
        "        if phrase in article_list:\n",
        "            return True\n",
        "        elif phrase in day_list:\n",
        "            return True\n",
        "        elif phrase in month_list:\n",
        "            return True\n",
        "        elif match_lst:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def all_slang(self, ne_phrase):\n",
        "        ne_words=ne_phrase.split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list\n",
        "        is_invalid=0\n",
        "\n",
        "        for word in ne_words:\n",
        "            if(word.strip().strip(string.punctuation).lower() in combined):\n",
        "                is_invalid+=1\n",
        "        if(is_invalid==len(ne_words)):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def start_end_token_check(self, ne_candidate):\n",
        "        positions = ne_candidate[1]\n",
        "        ne_words=ne_candidate[0].split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list+prep_list+article_list\n",
        "        # print(combined)\n",
        "        # print(ne_words)\n",
        "        start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "        end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        while((len(ne_words)>1)&((start_word in combined)|(end_word in combined))):\n",
        "            if(start_word in combined):\n",
        "                ne_words.pop(0)\n",
        "                positions.pop(0)\n",
        "            if(end_word in combined):\n",
        "                ne_words.pop()\n",
        "                positions.pop()\n",
        "            if(len(ne_words)>1):\n",
        "                start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "                end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        ne_phrase = (' '.join(ne_words)).strip()\n",
        "        # print(ne_phrase)\n",
        "        return (ne_phrase,positions)\n",
        "\n",
        "    def set_stopword_exceptions(self,words):\n",
        "        combined=cachedStopWords+prep_list+article_list+day_list\n",
        "        for word in words:\n",
        "            if word in combined:\n",
        "                self.swSet.add(word)\n",
        "\n",
        "    def extract(self, batch, batch_number):\n",
        "        print(\"Running Local NER now\")\n",
        "        time_in=time.time()\n",
        "        self.batch=batch\n",
        "        self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'TweetSentence','tweetwordList', 'phase1Candidates','start_time','entry_batch'))\n",
        "        df_holder=[]\n",
        "        self.swSet= set()\n",
        "        annotations_available=False\n",
        "\n",
        "        if('mentions_other' in self.batch.columns.tolist()):\n",
        "            annotations_available=True\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates','correct_candidates_tweet'))\n",
        "            #dict1 = {'tweetID':0, 'sentID':0, 'hashtags':'first', 'user':'user', 'TweetSentence':'sentence', 'phase1Candidates':'phase1Out','start_time':'now','entry_batch':'batch_number'}\n",
        "            self.CTrie=trie.Trie(\"ROOT\")\n",
        "            self.phase2stopWordList=[]\n",
        "            self.sentenceID = 0\n",
        "            self.f=0\n",
        "\n",
        "        for row in self.batch.itertuples():\n",
        "\n",
        "            now = datetime.datetime.now()\n",
        "            tweetID=str(row.Index)\n",
        "            text=str(row.TweetText)\n",
        "            row_sentences = self.normalize_to_sentences(text)\n",
        "\n",
        "            annnotated_mentions=[]\n",
        "\n",
        "            if(annotations_available):\n",
        "                for sentence_level in str(row.mentions_other).split(';'):\n",
        "                    if(sentence_level):\n",
        "                        for mention in sentence_level.split(','):\n",
        "                            if(mention):\n",
        "                                annnotated_mentions.append(mention.lower().strip(string.punctuation).strip())\n",
        "                annnotated_mentions=list(filter(lambda element: ((element !='')&(element !='nan')), annnotated_mentions))\n",
        "\n",
        "            self.tweet_to_sentences_w_annotation[tweetID]=((self.sentenceID,self.sentenceID+len(row_sentences)),annnotated_mentions)\n",
        "            self.sentenceID+=len(row_sentences)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for sen_index, sentence in enumerate(row_sentences):\n",
        "\n",
        "                    # print('tuple index:',tweetID,sen_index)\n",
        "                    phase1Out=\"\"\n",
        "                    sentence = self.normalizeTweet(sentence)\n",
        "                    # tweetWordList=self.getWords(sentence)\n",
        "                    tweetWordList = sentence.split()\n",
        "                    enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "                    entities_from_sentence=[]\n",
        "                    entity_aware_embeddings=[]\n",
        "\n",
        "                    if(len(tweetWordList)>0):\n",
        "\n",
        "                        # print(test_record)\n",
        "                        # tokenized_input=tokenizer(test_record)\n",
        "                        # initial_input_ids = torch.tensor([tokenizer.encode(test_record)])\n",
        "                        # token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in test_record.split()}\n",
        "                        # input_ids = initial_input_ids.to(device)\n",
        "                        # tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        tokenized_input= self.nerTokenizer(sentence)\n",
        "                        initial_input_ids = torch.tensor([self.nerTokenizer.encode(sentence)])\n",
        "                        # num_tokens = initial_input_ids.shape[1]\n",
        "\n",
        "                        initial_input_ids = initial_input_ids[:,:128]\n",
        "                        token_dict = {x : self.nerTokenizer.encode(x, add_special_tokens=False) for x in sentence.split()} #token, add_special_tokens=False, truncation=True\n",
        "                        input_ids = initial_input_ids.to(self.device)\n",
        "                        tokens = self.nerTokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        output = self.localNEREngine(input_ids)\n",
        "                        token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "                        prediction = (torch.argmax(output.logits, axis=2))\n",
        "                        prediction = prediction.cpu().numpy().reshape(-1)\n",
        "\n",
        "                        # prediction_labels=[self.label_list[l].split('-')[0] for l in prediction]\n",
        "                        prediction_labels=[self.label_list[l] for l in prediction] #Just BIO\n",
        "\n",
        "                        prediction_labels, entity_aware_embeddings=self.collate_token_label_embedding(tweetWordList, token_dict, prediction_labels[1:-1],token_embeddings)\n",
        "\n",
        "                        assert len(enumerated_tweetWordList)==len(entity_aware_embeddings)\n",
        "                        assert len(prediction_labels)==len(enumerated_tweetWordList)\n",
        "\n",
        "                        word_tag_tuples=list(zip(token_dict.keys(),prediction_labels))\n",
        "                        # print(list(word_tag_tuples))\n",
        "                        entities_from_sentence=self.get_entities(word_tag_tuples)\n",
        "                        # print('entities_from_sentence:',entities_from_sentence)\n",
        "\n",
        "                        if(self.f<5):\n",
        "                            print(len(tweetWordList),initial_input_ids.shape,len(prediction[1:-1]))\n",
        "                            print(tweetWordList)\n",
        "                            print(token_dict)\n",
        "                            print(initial_input_ids)\n",
        "                            print(input_ids)\n",
        "                            print(prediction)\n",
        "                            print('entities_from_sentence:',entities_from_sentence)\n",
        "                            print('======')\n",
        "                            self.f+=1\n",
        "\n",
        "                    just_candidates=[]\n",
        "\n",
        "                    # place some necessary filters\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.slang_remove(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(map(lambda element: self.start_end_token_check(element), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.all_slang(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: len(element[0])>1, entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: len(element[0].split())<=6, entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: element[0]!='', entities_from_sentence))\n",
        "\n",
        "                    \n",
        "\n",
        "                    for candidateTuple in entities_from_sentence:\n",
        "                        #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
        "                        candidateText, positions = candidateTuple\n",
        "                        candidateText=(((candidateText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
        "                        candidateText=(self.remAmpersand(candidateText).lstrip('“‘’”')).rstrip('“‘’”')\n",
        "                        candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "                        candidateText= candidateText.strip()\n",
        "                        self.set_stopword_exceptions(candidateText.split())\n",
        "                        just_candidates.append(candidateText)\n",
        "                        # if(index==9423):\n",
        "                        #     print(candidateText)\n",
        "                        position = '*'+'*'.join(str(v) for v in positions)\n",
        "                        position=position+'*'\n",
        "\n",
        "                        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
        "\n",
        "                        combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
        "                        if not ((candidateText in combined)|(len(candidateText)<=1)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
        "                            if(self.quickRegex.match(candidateText)):\n",
        "                                self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
        "                    \n",
        "                    print('entities_from_sentence:',just_candidates)\n",
        "                    #storing the outputs and token embeddings in dataframe tweetID,sen_index\n",
        "                    dict1 = {'tweetID':str(tweetID), 'sentID':str(sen_index), 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList,'phase1Candidates': just_candidates ,'phase1CandidatesWPositions':phase1Out,\n",
        "                             'contextual_embeddings':entity_aware_embeddings,\n",
        "                             'start_time':now,'entry_batch':batch_number}\n",
        "                    df_holder.append(dict1)\n",
        "                    # self.contextual_embeddings[(tweetID,sen_index)] = entity_aware_embeddings\n",
        "\n",
        "        time_out=time.time()\n",
        "        self.append_rows(df_holder)\n",
        "        self.phase2stopWordList=list(set(self.phase2stopWordList)|self.swSet)\n",
        "        self.counter=self.counter+1\n",
        "        \n",
        "        #return (copy.deepcopy(self.df_out),copy.deepcopy(freqs),time_in,time_out)\n",
        "        return (self.df_out,self.contextual_embeddings,self.CTrie,time_in,time_out,self.phase2stopWordList,self.tweet_to_sentences_w_annotation)\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        self.df_out=self.df_out.append(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncj3sC0FkBwA"
      },
      "source": [
        "## **Phase II: Global NER with Phrase Embedder to collect the Entity Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noxa5i5JPJi4"
      },
      "source": [
        "# coding: utf-8\n",
        "from nltk.corpus import stopwords\n",
        "import pandas  as pd\n",
        "# import NE_candidate_module as ne\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "import copy\n",
        "# import numpy\n",
        "import math\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from scipy import stats\n",
        "import emoji\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import trie as trie\n",
        "import re\n",
        "import ast\n",
        "import pickle\n",
        "import itertools\n",
        "from scipy import spatial\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.cluster import KMeans, MeanShift\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# import torch\n",
        "\n",
        "#CLASSIFIER CLASS\n",
        "# import SVM as svm\n",
        "# import entityClassifier as entityClassifier\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"since\",\"hence\",\"onto\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"us\")\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"&;\",\"v.\"] #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"httpurl\",\"n't\",\"&amp;\",\"gee\",\"€\",\"hmm\",\"bye\",\"pls\",\"please\",\"yrs\",\"4get\",\"ooh\",\"ouch\",\"am\",\"tv\",\"ima\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"qft\",\"ip\",\"icymi\",\"bdsm\",\"ah\",\"http\",\"https\",\"pm\",\"omw\",\"pts\",\"pt\",\"ive\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "\n",
        "class GlobalNERModule():\n",
        "\n",
        "\n",
        "    def executor(self,max_batch_value,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others,train_classifier):\n",
        "    # def executor(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others)\n",
        "\n",
        "\n",
        "        # SET CB\n",
        "        # print(phase2stopwordList)\n",
        "        if(train_classifier):\n",
        "            return self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,train_classifier)\n",
        "        candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag = self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,train_classifier)\n",
        "        candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        # print(self.good_candidates)\n",
        "\n",
        "        # SET TF \n",
        "        untrashed_tweets=self.set_tf(data_frame_holder, candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "        #mark incomplete tweets\n",
        "        self.set_column_for_candidates_in_incomplete_tweets(candidate_featureBase_DF,untrashed_tweets)\n",
        "\n",
        "        # SAVE INCOMING TWEETS FOR ANNOTATION FOR OTHERS\n",
        "        # self.raw_tweets_for_others=pd.concat([self.raw_tweets_for_others,raw_tweets_for_others ])\n",
        "\n",
        "        # DROP TF\n",
        "        just_converted_tweets=self.get_complete_tf(untrashed_tweets)\n",
        "\n",
        "        #incomplete tweets at the end of current batch\n",
        "        incomplete_tweets=self.get_incomplete_tf(untrashed_tweets)\n",
        "\n",
        "        #all incomplete_tweets---> incomplete_tweets at the end of current batch + incomplete_tweets not reintroduced\n",
        "        # self.incomplete_tweets=incomplete_tweets #without reintroduction--- when everything is reintroduced, just incomplete_tweets\n",
        "        # self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized','annotation','stanford_candidates'])\n",
        "        # self.incomplete_tweets=pd.concat([incomplete_tweets,self.not_reintroduced],ignore_index=True)\n",
        "        self.incomplete_tweets=pd.concat([incomplete_tweets],ignore_index=True)\n",
        "\n",
        "        print('completed tweets:',len(just_converted_tweets))\n",
        "        print('incomplete tweets:',len(incomplete_tweets))\n",
        "\n",
        "\n",
        "        # #recording tp, fp , f1\n",
        "        # #self.accuracy_tuples_prev_batch.append((just_converted_tweets.tp.sum(), just_converted_tweets.total_mention.sum(),just_converted_tweets.fp.sum(),just_converted_tweets.fn.sum()))\n",
        "\n",
        "\n",
        "        # #operations for getting ready for next batch.\n",
        "        # # self.incomplete_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "        # self.incomplete_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "        # self.counter=self.counter+1\n",
        "\n",
        "        self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets)\n",
        "\n",
        "        time_out=time.time()\n",
        "\n",
        "        self.calculate_tp_fp_f1(z_score_threshold,candidate_featureBase_DF,untrashed_tweets,raw_tweets_for_others)\n",
        "\n",
        "        if(self.counter==(max_batch_value+1)):\n",
        "            # self.just_converted_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "            self.just_converted_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "\n",
        "            print('completed tweets: ', len(self.just_converted_tweets),'incomplete tweets: ', len(self.incomplete_tweets))\n",
        "            \n",
        "            print(len(list(self.just_converted_tweets.columns.values)))\n",
        "            print(len(list(self.incomplete_tweets.columns.values)))\n",
        "\n",
        "            combined_frame_list=[self.just_converted_tweets, self.incomplete_tweets]\n",
        "            complete_tweet_dataframe = pd.concat(combined_frame_list)\n",
        "\n",
        "            print('final tally: ', (len(self.just_converted_tweets)+len(self.incomplete_tweets)), len(complete_tweet_dataframe))\n",
        "\n",
        "            #to groupby tweetID and get one tuple per tweetID\n",
        "            complete_tweet_dataframe_grouped_df= (complete_tweet_dataframe.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "            complete_tweet_dataframe_grouped_df['tweetID']=complete_tweet_dataframe_grouped_df['tweetID'].astype(int)\n",
        "            self.complete_tweet_dataframe_grouped_df_sorted=(complete_tweet_dataframe_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "            print(list(self.complete_tweet_dataframe_grouped_df_sorted.columns.values))\n",
        "\n",
        "\n",
        "        #self.aggregator_incomplete_tweets.to_csv(\"all_incompletes.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "\n",
        "        #self.just_converted_tweets.to_csv(\"all_converteds.csv\", sep=',', encoding='utf-8')\n",
        "        #self.incomplete_tweets.to_csv(\"incomplete_for_last_batch.csv\", sep=',', encoding='utf-8')\n",
        "        return candidate_featureBase_DF, self.complete_tweet_dataframe_grouped_df_sorted,time_out \n",
        "        # return candidate_featureBase_DF, untrashed_tweets,time_out\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,entity_phrase_embedder,device,filename):\n",
        "        self.counter=0\n",
        "        self.decay_factor=2**(-1/2)\n",
        "        self.decay_base_staggering=2\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "        self.device=device\n",
        "        self.save_file= filename\n",
        "\n",
        "        self.entity_phrase_embedder = entity_phrase_embedder\n",
        "\n",
        "        # context_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        context_feature_list=['cf_'+str(i) for i in range(300)]\n",
        "        self.candidateBaseHeaders=['candidate', 'batch', 'length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+context_feature_list+['cumulative']\n",
        "        self.candidateBaseHeaders_alt=['candidate', 'batch', 'length','cumulative']\n",
        "        \n",
        "        ## When not running on a notebook\n",
        "        # self.entity_classifier = entityClassifier.EntityClassifier('data/candidate_train_records.csv',True,self.device)\n",
        "\n",
        "        ################################### To do a fresh training\n",
        "\n",
        "        ## With one unified classifier\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large.csv',True,self.device)\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_300d.csv',True,self.device)\n",
        "\n",
        "        # # With two separate classifiers\n",
        "        # self.entity_classifierI= EntityClassifierI('data/training.csv',True,self.device)\n",
        "        # # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large.csv',True,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large_300d.csv',True,self.device)\n",
        "\n",
        "        ################################### To Load a pre-trained model\n",
        "\n",
        "        ## With one unified classifier\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large.csv',False,self.device)\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_300d.csv',False,self.device)\n",
        "\n",
        "        ## With two separate classifiers\n",
        "        # self.entity_classifierI= EntityClassifierI('data/training.csv',False,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large.csv',False,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large_300d.csv',False,self.device)\n",
        "\n",
        "\n",
        "        ################################### Older SVM classifier\n",
        "        # self.my_classifier= svm.SVM1('/home/satadisha/Desktop/GitProjects/TwiCSv2/production_code/training.csv')\n",
        "        \n",
        "        # self.my_classifier= svm.SVM1('/Users/satadisha/Documents/GitHub/TwiCSv2/production_code/training.csv')\n",
        "        # self.my_classifier= svm.SVM1('/Users/satadisha/Documents/GitHub/tweebo-parser/training.csv')\n",
        "        # self.my_classifier= svm.SVM1('training.csv')\n",
        "        self.complete_tweet_dataframe_grouped_df_sorted=pd.DataFrame([], columns=['tweetID', 'TweetSentence', 'ambiguous_candidates', 'annotation', 'candidates_with_label', 'completeness', 'current_minus_entry', 'entry_batch', 'hashtags', 'index', 'only_good_candidates', 'output_mentions', 'phase1Candidates', 'sentID', 'stanford_candidates', 'user'])\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1_generic(self,raw_tweets_for_others,state_of_art):\n",
        "\n",
        "        column_candidates_holder = raw_tweets_for_others[state_of_art].tolist()\n",
        "        \n",
        "\n",
        "        column_annot_holder= raw_tweets_for_others['mentions_other'].tolist()\n",
        "        # column_annot_holder= raw_tweets_for_others['annotation_limited types'].tolist()\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=column_annot_holder[idx].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=ast.literal_eval(column_candidates_holder[idx])\n",
        "            else:\n",
        "                output_mentions_list=column_candidates_holder[idx].split(',')\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            # print(annotated_mention_list,output_mentions_list)\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall))    \n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "        # tweet_ids_df[\"tp\"+state_of_art]=true_positive_holder\n",
        "        # tweet_ids_df[\"fn\"+state_of_art]=false_negative_holder\n",
        "        # tweet_ids_df['fp'+state_of_art]= false_positive_holder\n",
        "        \n",
        "        # if(state_of_art==\"ritter_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"ritter_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # if(state_of_art==\"stanford_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"stanford_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "    def calculate_tp_fp_f1_alternate(self,raw_tweets_for_others, state_of_art):\n",
        "        if(state_of_art=='neuroner'):\n",
        "            # fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/venezuela_input_2019-05-10_12-33-16-15380/mentions_output.txt\",\"r\")\n",
        "            fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/tweets_3K_input_2019-04-26_16-49-32-20455/mentions_output.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='stanford_candidates'):\n",
        "            fp= open(\"/home/satadisha/Desktop/stanford-ner-2016-10-31/stanford_venezuela_mentions.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='ritter_candidates'):\n",
        "            tweets_ritter=pd.read_csv(\"/home/satadisha/Desktop/GitProjects/twitter_nlp-master/ritter-venezuela-output.csv\",sep =',', keep_default_na=False)\n",
        "        if(state_of_art=='calai_candidates'):\n",
        "            tweets_calai=pd.read_csv(\"/home/satadisha/Desktop/opencalai_versions/venezuela_output.csv\",sep =',', keep_default_na=False)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        for index, row in raw_tweets_for_others.iterrows():\n",
        "            \n",
        "\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=row['mentions_other'].split(';')\n",
        "            # tweet_level_candidate_list=row['annotation_limited types'].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='ritter_candidates'):\n",
        "                # output_mentions_list=ast.literal_eval(mentions_list[idx])\n",
        "                output_mentions_list=tweets_ritter[tweets_ritter['ID']==row['ID']]['Output'].iloc[0].split(',')\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=tweets_calai[tweets_calai['ID']==row['ID']]['calai_candidates'].iloc[0].split(',')\n",
        "            if((state_of_art=='neuroner')|(state_of_art=='stanford_candidates')):\n",
        "                #for 3k Tweets:\n",
        "                idx=int(row['ID'])\n",
        "\n",
        "                # #for others:\n",
        "                # idx=int(row['ID']-1)\n",
        "\n",
        "                output_mentions_list=mentions_list[idx].split(',')\n",
        "\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            print(annotated_mention_list,output_mentions_list)\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall)) \n",
        "        if(state_of_art==\"neuroner\"):\n",
        "            self.accuracy_vals_neuroner.append((f_measure,precision,recall))\n",
        "\n",
        "\n",
        "        # output_mentions_list= mentions_list[output_index].split(',')\n",
        "#     # output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "#     # output_mentions_list=list(filter(lambda element: element !='', output_mentions_list))\n",
        "\n",
        "    def calculate_tp_fp_f1_for_others(self,raw_tweets_for_others):\n",
        "\n",
        "        opencalai=\"calai_candidates\"\n",
        "        stanford=\"stanford_candidates\"\n",
        "        ritter=\"ritter_candidates\"\n",
        "        neuroner=\"neuroner\"\n",
        "\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,opencalai)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,stanford)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,ritter)\n",
        "\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,opencalai)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,stanford)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,ritter)\n",
        "\n",
        "        self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,neuroner)\n",
        "\n",
        "    #################################\n",
        "    #input candidate_feature_Base\n",
        "    #output candidate_feature_Base with [\"Z_score\"], [\"probability\"],[\"class\"]\n",
        "    #################################\n",
        "    def classify_candidate_base(self,z_score_threshold,candidate_featureBase_DF):\n",
        "\n",
        "        # #filtering test set based on z_score\n",
        "        # mert1=candidate_featureBase_DF['cumulative'].as_matrix()\n",
        "        #frequency_array = np.array(list(map(lambda val: val[0], sortedCandidateDB.values())))\n",
        "        # zscore_array1=stats.zscore(mert1)\n",
        "\n",
        "        zscore_array1=stats.zscore(candidate_featureBase_DF['cumulative'])\n",
        "\n",
        "        candidate_featureBase_DF['Z_ScoreUnweighted']=zscore_array1\n",
        "        cumulative_threshold=1.0 #set threshold here\n",
        "        z_score_threshold=candidate_featureBase_DF[candidate_featureBase_DF['cumulative']==cumulative_threshold].Z_ScoreUnweighted.tolist()[0]\n",
        "        print(cumulative_threshold,z_score_threshold)\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_new_with_z_score.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        #multi-word infrequent candidates ---> to be used for recall correction\n",
        "        infrequent_candidates=candidate_featureBase_DF[(candidate_featureBase_DF['Z_ScoreUnweighted'] < z_score_threshold) & (candidate_featureBase_DF.length>1)].candidate.tolist()\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF[candidate_featureBase_DF['Z_ScoreUnweighted'] >= z_score_threshold]\n",
        "\n",
        "        # # #######################with one unified classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # return (self.entity_classifier.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # # #######################with only semantic classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # return (self.entity_classifierII.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with alt semantic classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        return (self.entity_classifier_alt.run(candidate_featureBase_DF,self.CandidateBase_dict_alt),infrequent_candidates)\n",
        "\n",
        "        # #######################with two separate classifiers--- requires some additional lines of code\n",
        "        # candidateList = candidate_featureBase_DF.candidate.tolist()\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # candidate_featureBase_DF.set_index(\"candidate\", inplace=True)\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF.index.name)\n",
        "\n",
        "        # # returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # candidate_featureBase_DF_classifierI = self.entity_classifierI.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "        # candidate_featureBase_DF_classifierII = self.entity_classifierII.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "\n",
        "        # # candidate_featureBase_DF_classifierI.to_csv('classifierI.csv', sep=',', encoding='utf-8')\n",
        "        # # candidate_featureBase_DF_classifierII.to_csv('classifierII.csv', sep=',', encoding='utf-8') #candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # final_probability_dict = {}\n",
        "        # print5=0\n",
        "        # for candidate in candidateList:\n",
        "        #     prob1 = candidate_featureBase_DF_classifierI.loc[candidate]['probability']\n",
        "        #     prob2 = candidate_featureBase_DF_classifierII.loc[candidate]['probability']\n",
        "        #     final_probability = max(prob1,prob2)\n",
        "        #     if(print5<5):\n",
        "        #         print(candidate,prob1,prob2,final_probability)\n",
        "        #         print5+=1\n",
        "        #     final_probability_dict[candidate] = final_probability\n",
        "\n",
        "        # candidate_featureBase_DF[\"probability\"] = pd.Series(final_probability_dict)\n",
        "\n",
        "        # candidate_featureBase_DF.reset_index(drop=False,inplace=True)\n",
        "        # print('after columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF[['candidate','probability']].head(5))\n",
        "\n",
        "        # return (candidate_featureBase_DF,infrequent_candidates)\n",
        "\n",
        "    # recall_correction\n",
        "    def set_partition_dict(self,candidate_featureBase_DF,infrequent_candidates):\n",
        "\n",
        "        #print(list(self.partition_dict.keys()))\n",
        "        ambiguous_bad_candidates=candidate_featureBase_DF[(((candidate_featureBase_DF.status==\"a\")|(candidate_featureBase_DF.status==\"b\"))&(candidate_featureBase_DF.length.astype(int)>1))]\n",
        "        good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "        flag1=False\n",
        "        flag2=False\n",
        "        if(len(ambiguous_bad_candidates)>0):\n",
        "            # ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1) \n",
        "            # ambiguous_bad_candidates_wFilter= ambiguous_bad_candidates[ambiguous_bad_candidates.max_column=='substring-cap']\n",
        "\n",
        "            #good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "            #print(ambiguous_bad_candidates_wFilter.candidate.tolist())\n",
        "\n",
        "            # for candidate in ambiguous_bad_candidates_wFilter.candidate.tolist():\n",
        "            for candidate in ambiguous_bad_candidates.candidate.tolist():\n",
        "                \n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "\n",
        "            flag1= True\n",
        "        if(len(infrequent_candidates)>0):\n",
        "            #print(len(ambiguous_bad_candidates_wFilter.candidate.tolist()))\n",
        "\n",
        "            for candidate in infrequent_candidates:\n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "            flag2= True\n",
        "        print(list(self.partition_dict.keys()))\n",
        "        return (flag1|flag2)\n",
        "\n",
        "    def get_reintroduced_tweets(self, reintroduction_threshold):\n",
        "        #no reintroduction\n",
        "        #no preferential selection\n",
        "        print(\"incomplete tweets in batch: \",len(self.incomplete_tweets))\n",
        "        # print(list(self.incomplete_tweets.columns.values))\n",
        "\n",
        "        reintroduced_tweets=self.incomplete_tweets[(self.counter-self.incomplete_tweets['entry_batch'])<=reintroduction_threshold]\n",
        "        self.not_reintroduced=self.incomplete_tweets[~self.incomplete_tweets.index.isin(reintroduced_tweets.index)]\n",
        "\n",
        "        print(\"reintroduced tweets: \",len(reintroduced_tweets))\n",
        "        # for i in range(self.counter):\n",
        "        #     print('i:',len(self.incomplete_tweets[self.incomplete_tweets['entry_batch']==i]))\n",
        "        return reintroduced_tweets\n",
        "\n",
        "    def set_cb(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,train_classifier):\n",
        "\n",
        "        #input new_tweets, z_score, Updated candidatebase of phase1\n",
        "        #output candidate_featureBase_DF, Incomplete_tweets\n",
        "        data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "\n",
        "        if(train_classifier):\n",
        "            return self.extract(TweetBase,CTrie,phase2stopwordList,0,train_classifier)\n",
        "\n",
        "        candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted= self.extract(TweetBase,CTrie,phase2stopwordList,0,train_classifier)\n",
        "        phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "        phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "        df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        if((self.counter>0)&(len(self.incomplete_tweets)>0)):\n",
        "\n",
        "            #tweet candidates for Reintroduction\n",
        "            reintroduced_tweets=self.get_reintroduced_tweets(reintroduction_threshold)\n",
        "            candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted = self.extract(reintroduced_tweets,CTrie,phase2stopwordList,1,train_classifier)\n",
        "            phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "            phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "            df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        #print(len(df_holder))\n",
        "        data_frame_holder = pd.DataFrame(df_holder)\n",
        "        #print(len(self.incomplete_tweets),len(data_frame_holder),len(candidate_featureBase_DF))\n",
        "        \n",
        "\n",
        "        #set ['probabilities'] for candidate_featureBase_DF\n",
        "        candidate_featureBase_DF,self.infrequent_candidates= self.classify_candidate_base(z_score_threshold,candidate_featureBase_DF)\n",
        "\n",
        "        # set readable labels (a,g,b) for candidate_featureBase_DF based on ['probabilities.']\n",
        "        candidate_featureBase_DF=self.set_readable_labels(candidate_featureBase_DF)\n",
        "\n",
        "        self.good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        self.ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "        self.bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        print('good_candidates:',self.good_candidates)\n",
        "        print('bad_candidates:',self.bad_candidates)\n",
        "        print('ambiguous_candidates:',self.ambiguous_candidates)\n",
        "\n",
        "        # entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.good_candidates)]\n",
        "        # non_entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.bad_candidates)]\n",
        "        # ambiguous_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.ambiguous_candidates)]\n",
        "\n",
        "        correction_flag=self.set_partition_dict(candidate_featureBase_DF,self.infrequent_candidates)\n",
        "\n",
        "        ambiguous_turned_good=[]\n",
        "        ambiguous_turned_bad=[]\n",
        "        ambiguous_remaining_ambiguous=[]\n",
        "        converted_candidates=[]\n",
        "\n",
        "        #['probability'],['a,g,b']\n",
        "        return candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag\n",
        "\n",
        "\n",
        "        #flush out completed tweets\n",
        "        # input candidate base, looped over tweets (incomplete tweets+ new tweets)\n",
        "        # output: incomplete tweets (a tags in it.), incomplete_tweets[\"Complete\"]\n",
        "    def set_tf(self,data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        return self.set_completeness_in_tweet_frame(data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "    def get_incomplete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==False]\n",
        "\n",
        "    def get_complete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==True]\n",
        "\n",
        "    def compute_seen_tweets_so_far(self,start_batch,end_batch):\n",
        "        if(start_batch==end_batch):\n",
        "            sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch]\n",
        "\n",
        "\n",
        "        sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch:]\n",
        "\n",
        "\n",
        "        counter=0\n",
        "        for elem in sliced_seen_tweets:\n",
        "            counter=counter+elem\n",
        "\n",
        "        return counter\n",
        "\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "    #ME_EXTR=Mention.Mention_Extraction()\n",
        "\n",
        "\n",
        "    # experiment function\n",
        "    def set_x_axis(self,just_converted_tweets_for_current_batch):\n",
        "\n",
        "        self.incomplete_tweets.to_csv(\"set_x_axis_debug.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        self.incomplete_tweets['number_of_seen_tweets'] = self.incomplete_tweets['entry_batch'].apply(lambda x: self.compute_seen_tweets_so_far(x,self.counter))\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"entry_vs_tweet_seen_ratio\"]=self.incomplete_tweets['entry_batch']/self.incomplete_tweets['number_of_seen_tweets']\n",
        "\n",
        "\n",
        "        #counter_list= \n",
        "        self.incomplete_tweets[\"ratio_entry_vs_current\"]=self.incomplete_tweets['entry_batch']/self.counter\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"current_minus_entry\"]=self.counter-self.incomplete_tweets['entry_batch']\n",
        "\n",
        "        just_converted_tweets_for_current_batch[\"current_minus_entry\"]=self.counter-just_converted_tweets_for_current_batch['entry_batch']\n",
        "\n",
        "        return just_converted_tweets_for_current_batch\n",
        "\n",
        "\n",
        "\n",
        "    def set_column_for_candidates_in_incomplete_tweets(self,candidate_featureBase_DF,input_to_eval):\n",
        "\n",
        "        incomplete_candidates= input_to_eval['2nd Iteration Candidates'].tolist()\n",
        "\n",
        "        candidate_featureBase_DF= candidate_featureBase_DF.set_index('candidate')\n",
        "\n",
        "        candidate_with_label_holder=[]\n",
        "        one_level=[]\n",
        "        \n",
        "\n",
        "        for sentence_level_candidates in incomplete_candidates:\n",
        "\n",
        "            one_level.clear()\n",
        "\n",
        "            for candidate in sentence_level_candidates:\n",
        "                if candidate.lower() in candidate_featureBase_DF.index:\n",
        "                    # label=candidate_featureBase_DF.get_value(candidate.lower(),'status')\n",
        "                    label=candidate_featureBase_DF.at[candidate.lower(),'status']\n",
        "                    one_level.append((candidate,label))\n",
        "                else:\n",
        "                    one_level.append((candidate,\"na\"))\n",
        "\n",
        "            candidate_with_label_holder.append(copy.deepcopy(one_level))\n",
        "\n",
        "\n",
        "        input_to_eval[\"candidates_with_label\"]=candidate_with_label_holder\n",
        "        debug_candidates_label_list= input_to_eval['candidates_with_label'].tolist()\n",
        "        candidates_filtered_g_labeled=[]\n",
        "        row_level_candidates=[]\n",
        "        index_outer=0\n",
        "\n",
        "        candidates_filtered_a_labeled=[]\n",
        "        row_level_a_candidates=[]\n",
        "\n",
        "        for sentence_level in debug_candidates_label_list:\n",
        "\n",
        "            # sentence_level_candidates_unnormalized= incomplete_candidates_unnormalized[index_outer]\n",
        "            row_level_candidates.clear()\n",
        "            row_level_a_candidates.clear()\n",
        "            for candidate in sentence_level:\n",
        "                if(candidate[1]==\"g\"):\n",
        "                    candidate_str = self.erode_article(candidate[0])\n",
        "                    row_level_candidates.append(candidate_str)\n",
        "                if(((candidate[1]==\"b\")|(candidate[1]==\"a\"))&(candidate[0]==\"US\")):\n",
        "                    # print('here')\n",
        "                    row_level_candidates.append(candidate[0])\n",
        "                if(candidate[1]==\"a\"):\n",
        "                    row_level_a_candidates.append(candidate[0])\n",
        "\n",
        "            candidates_filtered_g_labeled.append(copy.deepcopy(row_level_candidates))\n",
        "            candidates_filtered_a_labeled.append(copy.deepcopy(row_level_a_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "\n",
        "        input_to_eval[\"only_good_candidates\"]=candidates_filtered_g_labeled\n",
        "        input_to_eval[\"ambiguous_candidates\"]=candidates_filtered_a_labeled\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1(self,z_score_threshold,candidate_featureBase_DF,input_to_eval,raw_tweets_for_others):\n",
        "\n",
        "        input_to_eval_grouped_df= (input_to_eval.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "        input_to_eval_grouped_df['tweetID']=input_to_eval_grouped_df['tweetID'].astype(int)\n",
        "        input_to_eval_df_sorted=(input_to_eval_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "        print(len(input_to_eval_df_sorted),len(raw_tweets_for_others))\n",
        "        \n",
        "        print(set(input_to_eval_df_sorted['tweetID'].values.tolist())-set(raw_tweets_for_others['ID'].values.tolist()))\n",
        "\n",
        "        input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_other'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_limited_types'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['annotation_limited types'].iloc[0])\n",
        "\n",
        "        column_candidates_holder = input_to_eval_df_sorted['only_good_candidates'].tolist()\n",
        "        column_phase1Candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "        # column_candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "\n",
        "        file1 = open(self.save_file+\".txt\", \"w\")\n",
        "        \n",
        "        phase1Candidates=candidate_featureBase_DF.candidate.tolist()\n",
        "\n",
        "        column_annot_holder= input_to_eval_df_sorted['annotation'].tolist()\n",
        "        \n",
        "        output_str=''\n",
        "        # print(column_candidates_holder)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "        total_annotation=0\n",
        "        phase1NotInphase2_count=0\n",
        "        mislabelledCandidateMentions=[]\n",
        "\n",
        "        all_annotations=[]\n",
        "        annotation_dict={}\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        print('=========================BERTNER_candidates')\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "            phase1NotInphase2_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            output_mentions_list=[]\n",
        "            phase1_mentions_list=[]\n",
        "\n",
        "            tweet_level_candidate_list=str(column_annot_holder[idx]).split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            \n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: quickRegex.match(element), annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: ((element !='')&(element !='nan')), annotated_mention_list))\n",
        "\n",
        "            for lst in column_candidates_holder[idx]:\n",
        "                output_mentions_list.extend(lst)\n",
        "\n",
        "            for lst in column_phase1Candidates_holder[idx]:\n",
        "                phase1_mentions_list.extend(lst)\n",
        "\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "            output_mentions_list=list(map(lambda element: element.lower(), output_mentions_list))\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            for annotated_entity in annotated_mention_list:\n",
        "                try:\n",
        "                    annotation_dict[annotated_entity]+=1\n",
        "                except KeyError:\n",
        "                    annotation_dict[annotated_entity]=1\n",
        "\n",
        "            print(idx, annotated_mention_list,output_mentions_list)\n",
        "            print(phase1_mentions_list)\n",
        "\n",
        "            output_str+=','.join(output_mentions_list)+'\\n'\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            # candidate in Candidate but missed here\n",
        "            # for elem in annotated_mention_list:\n",
        "            #     elem = self.normalize(elem)\n",
        "            #     if((elem in phase1Candidates)&(elem not in output_mentions_list)):\n",
        "            #         mislabelledCandidates.append(elem)\n",
        "            #         mislabelledCandidateMentionCount+=1\n",
        "\n",
        "            # add phase1NotInphase2 code!!!!!!!!            \n",
        "            for elem in annotated_mention_list:\n",
        "                elem = self.normalize(elem)\n",
        "                if(phase1_mentions_list):\n",
        "                    if ((elem in phase1_mentions_list)&(elem not in output_mentions_list)):\n",
        "                        phase1_mentions_list.pop(phase1_mentions_list.index(elem))\n",
        "                        phase1NotInphase2_inner+=1\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= self.normalize(annotated_mention_list.pop())\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        if(annotated_candidate in phase1Candidates):\n",
        "                            mislabelledCandidateMentions.append(annotated_candidate)\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    for elem in annotated_mention_list:\n",
        "                        elem = self.normalize(elem)\n",
        "                        if(elem in phase1Candidates):\n",
        "                            mislabelledCandidateMentions.append(elem)\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            # print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "            self.true_positive_count+=tp_counter_inner\n",
        "            self.false_positive_count+=fp_counter_inner\n",
        "            self.false_negative_count+=fn_counter_inner\n",
        "            phase1NotInphase2_count+=phase1NotInphase2_inner\n",
        "\n",
        "        print('TP||||FP||||FN')\n",
        "        print(self.true_positive_count,self.false_positive_count,self.false_negative_count,total_mentions,total_annotation)\n",
        "        print('Phase1 but not in Phase2:',phase1NotInphase2_count)\n",
        "\n",
        "        print('mislabelled candidates:',len(list(set(mislabelledCandidateMentions))))\n",
        "        print('mentions missed by mislabeling:',len(mislabelledCandidateMentions))\n",
        "\n",
        "        precision=(self.true_positive_count)/(self.true_positive_count+self.false_positive_count)\n",
        "        recall=(self.true_positive_count)/(self.true_positive_count+self.false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        all_annotations=set(all_annotations)\n",
        "        all_mentions=set(all_mentions)\n",
        "        \n",
        "        true_positives = all_annotations.intersection(all_mentions)\n",
        "        true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        false_positive_count=len(all_mentions-all_annotations)\n",
        "        false_negative_count=len(all_annotations-all_mentions)\n",
        "        total_mentions=len(all_mentions)\n",
        "        total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        freq_bucket = {} \n",
        "        for candidate in all_annotations:\n",
        "            candidate_freq = annotation_dict[candidate]\n",
        "            flag = False\n",
        "            if(candidate in true_positives):\n",
        "                flag = True\n",
        "            try:\n",
        "                old_tup = freq_bucket[candidate_freq]\n",
        "                if flag:\n",
        "                    freq_bucket[candidate_freq] = (old_tup[0]+1,old_tup[1])\n",
        "                else:\n",
        "                    freq_bucket[candidate_freq] = (old_tup[0],old_tup[1]+1)\n",
        "            except KeyError:\n",
        "                if flag:\n",
        "                    freq_bucket[candidate_freq] = (1,0)\n",
        "                else:\n",
        "                    freq_bucket[candidate_freq] = (0,1)\n",
        "        \n",
        "        x_axis=[]\n",
        "        y_axis =[]\n",
        "        cumulative_tp=0\n",
        "        cumulative_annotated=0\n",
        "        freq_bucket_sorted = {k : freq_bucket[k] for k in sorted(freq_bucket)}\n",
        "\n",
        "        maxKey = list(freq_bucket_sorted.keys())[-1]\n",
        "        print(freq_bucket_sorted.keys(),maxKey)\n",
        "\n",
        "        step=0\n",
        "        for key in range(maxKey):\n",
        "            try:\n",
        "                tup = freq_bucket_sorted[key]\n",
        "            except KeyError:\n",
        "                tup = (0,0)\n",
        "\n",
        "            # freq_recall= tup[0]/(tup[0]+tup[1])\n",
        "\n",
        "            if(step==5):\n",
        "                if((cumulative_tp==0)&(cumulative_annotated==0)):\n",
        "                    freq_recall= y_axis[-1]\n",
        "                else:\n",
        "                    freq_recall= cumulative_tp/cumulative_annotated\n",
        "                # print(key, freq_recall)\n",
        "                x_axis.append(key)\n",
        "                y_axis.append(freq_recall)\n",
        "                step=0\n",
        "                cumulative_tp=0\n",
        "                cumulative_annotated=0\n",
        "\n",
        "\n",
        "            cumulative_tp+=tup[0]\n",
        "            cumulative_annotated+=(tup[0]+tup[1])\n",
        "            # freq_recall= cumulative_tp/cumulative_annotated\n",
        "            step+=1\n",
        "\n",
        "        print(x_axis)\n",
        "        print(y_axis)\n",
        "\n",
        "        # print20=20\n",
        "        for ind, elem in enumerate(x_axis):\n",
        "            # if(print20>0):\n",
        "            print(x_axis[ind],':',y_axis[ind])\n",
        "            #     print20=-1\n",
        "            # else:\n",
        "            #     print(x_axis[ind],':',y_axis[ind])\n",
        "            #     print20=20\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "\n",
        "        # true_positive_count_IPQ=true_positive_count\n",
        "        # false_positive_count_IPQ = false_positive_count\n",
        "        # false_negative_count_IPQ= false_negative_count\n",
        "        # total_mention_count_IPQ=total_mentions\n",
        "\n",
        "\n",
        "        # tp_count=0\n",
        "        # tm_count=0\n",
        "        # fp_count=0\n",
        "        # fn_count=0\n",
        "\n",
        "        # for idx,tup in enumerate(self.accuracy_tuples_prev_batch):\n",
        "        #     # print(idx,tup)\n",
        "        #     tp_count+=tup[0]\n",
        "        #     tm_count+=tup[1]\n",
        "        #     fp_count+=tup[2]\n",
        "        #     fn_count+=tup[3]\n",
        "\n",
        "\n",
        "\n",
        "        # tp_count+=true_positive_count_IPQ\n",
        "        # tm_count+=total_mention_count_IPQ\n",
        "        # fp_count+=false_positive_count_IPQ\n",
        "        # fn_count+=false_negative_count_IPQ\n",
        "\n",
        "        # precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        # recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        # f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        file1.write(output_str)\n",
        "        file1.close()\n",
        "\n",
        "\n",
        "\n",
        "        self.accuracy_vals=(z_score_threshold,f_measure,precision,recall)\n",
        "\n",
        "        print('Precision:',precision)\n",
        "        print('Recall:',recall)\n",
        "        print('F1:',f_measure)\n",
        "\n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "\n",
        "        # input_to_eval[\"tp\"]=true_positive_holder\n",
        "        # input_to_eval[\"fn\"]=false_negative_holder\n",
        "        # input_to_eval['fp']= false_positive_holder\n",
        "        # input_to_eval[\"total_mention\"]=total_mention_holder\n",
        "\n",
        "        # input_to_eval[\"ambigious_not_in_annot\"]=ambigious_not_in_annotation_holder\n",
        "        # input_to_eval[\"inverted_loss\"]=input_to_eval[\"tp\"]/( input_to_eval[\"fn\"]+input_to_eval[\"ambigious_not_in_annot\"])\n",
        "\n",
        "        return input_to_eval\n",
        "\n",
        "\n",
        "    def recall_correction(self,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder):\n",
        "\n",
        "        corrected_phase2_candidates_holder=[]\n",
        "        index_outer=0\n",
        "        for candidates in phase2_candidates_holder:\n",
        "            unnormalized_candidates=phase2_unnormalized_candidates_holder[index_outer]\n",
        "            corrected_phase2_candidates=[]\n",
        "            for idx, candidate in enumerate(candidates):\n",
        "                unnormalized_candidate=unnormalized_candidates[idx]\n",
        "                # if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates))):\n",
        "                if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))):   #do this only for 3K tweets\n",
        "                    #print(candidate, self.partition_dict[candidate])\n",
        "                    corrected_phase2_candidates.extend(self.partition_dict[candidate])\n",
        "                else:\n",
        "                    if(((candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))&(candidate=='us')&(unnormalized_candidate=='US')):\n",
        "                        # print(index_outer)\n",
        "                        candidate=unnormalized_candidate\n",
        "                    # if((len(candidate.strip().strip(string.punctuation).split())>1)&(candidate.strip().strip(string.punctuation).split()[0].lower() in ['a','an','the'])):\n",
        "                    #     candidate = (' '.join(candidate.strip().strip(string.punctuation).split()[1:])).strip()\n",
        "                    corrected_phase2_candidates.append(candidate)\n",
        "            corrected_phase2_candidates_holder.append(copy.deepcopy(corrected_phase2_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "        \n",
        "        #print(corrected_phase2_candidates_holder)\n",
        "        data_frame_holder['2nd Iteration Candidates']=corrected_phase2_candidates_holder\n",
        "\n",
        "        return corrected_phase2_candidates_holder,data_frame_holder                  \n",
        "\n",
        "\n",
        "    def erode_article(self, entity_string):\n",
        "        if((len(entity_string.strip().strip(string.punctuation).split())>1)&(entity_string.lower().strip().strip(string.punctuation).split()[0] in ['a','an','the'])):\n",
        "            # print(entity_string)\n",
        "            entity_string = ' '.join(entity_string.strip().strip(string.punctuation).split()[1:])\n",
        "            # print(entity_string)\n",
        "        return entity_string.strip()\n",
        "\n",
        "\n",
        "    def set_completeness_in_tweet_frame(self,data_frame_holder,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        #print(candidate_featureBase_DF.head())\n",
        "        good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        merged_g_b= bad_candidates+good_candidates\n",
        "\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_before_labeling_comp.csv\", sep=',', encoding='utf-8')\n",
        "        ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "\n",
        "        if(correction_flag):\n",
        "            phase2_candidates_holder,data_frame_holder=self.recall_correction(candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder)\n",
        "\n",
        "         \n",
        "\n",
        "        \n",
        "        truth_vals=[False if any(x not in merged_g_b for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        intermediate_output_mentions=[list(filter(lambda candidate: ((candidate in good_candidates))|(candidate=='US'), list1)) for list1 in phase2_candidates_holder]\n",
        "\n",
        "        output_mentions=[list(map(lambda candidate: self.erode_article(candidate), list1)) for list1 in intermediate_output_mentions]\n",
        "\n",
        "        # truth_vals=[False if any(x in ambiguous_candidates for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        # for list1 in phase2_candidates_holder:\n",
        "        #     if any(x in ambiguous_candidates  for x in list1):\n",
        "        #         truth_vals.append(False)\n",
        "        #     else:\n",
        "        #         truth_vals.append(True)\n",
        " \n",
        "\n",
        "\n",
        "        #print(truth_vals)\n",
        "        completeness_series = pd.Series( (v for v in truth_vals) )\n",
        "        output_mentions_series = pd.Series( (v for v in output_mentions) )\n",
        "\n",
        "\n",
        "        data_frame_holder['output_mentions']=output_mentions_series\n",
        "        data_frame_holder['completeness']=completeness_series\n",
        "        data_frame_holder[\"current_minus_entry\"]=self.counter-data_frame_holder['entry_batch']\n",
        "\n",
        "        return data_frame_holder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_readable_labels(self,candidate_featureBase_DF):\n",
        "\n",
        "        #candidate_featureBase_DF['status'] = candidate_featureBase_DF['probability'].apply(lambda x: set(x).issubset(good_candidates))\n",
        "        candidate_featureBase_DF['status']='ne'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.55]='g'\n",
        "        candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.55)] = 'a'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
        "\n",
        "        return candidate_featureBase_DF\n",
        "\n",
        "\n",
        "\n",
        "    def normalize(self,word):\n",
        "        strip_op=word\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        if strip_op.endswith(\"'s\"):\n",
        "            li = strip_op.rsplit(\"'s\", 1)\n",
        "            return ''.join(li)\n",
        "        elif strip_op.endswith(\"’s\"):\n",
        "            li = strip_op.rsplit(\"’s\", 1)\n",
        "            return ''.join(li)\n",
        "        else:\n",
        "            return strip_op\n",
        "        #return strip_op\n",
        "\n",
        "    \n",
        "    def isSubstring(self,to_increase_element,id_to_incr,comparison_holder,phase1_holder_holder_copy):\n",
        "        combined_list=comparison_holder[id_to_incr]+phase1_holder_holder_copy[id_to_incr]\n",
        "\n",
        "        for idx,val in enumerate(comparison_holder[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[5]) and to_increase_element[5] != val[5]):\n",
        "                    return True\n",
        "        for idx,val in enumerate(phase1_holder_holder_copy[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[2]) and to_increase_element[5] != val[2]):\n",
        "                    return True   \n",
        "                \n",
        "        return False\n",
        "\n",
        "\n",
        "    def calculate_pmi(self,big,x1,x2,total):\n",
        "        big__= float(big/total)\n",
        "        x1__=float(x1/total)\n",
        "        x2__=float(x2/total)\n",
        "        pmi= math.log(big__/(x1__*x2__),2.71828182845)\n",
        "        pklv=big__*pmi\n",
        "        #return (1/(1+math.exp(-1*pmi)))\n",
        "        npmi= pmi/(-1.0*(math.log(big__,2.71828182845)))\n",
        "        return npmi,pklv\n",
        "        #return pklv\n",
        "\n",
        "    def multiSlice(self,s,cutpoints,good_candidates):\n",
        "        k = len(cutpoints)\n",
        "        multislices=[]\n",
        "        if k == 0:\n",
        "            curr_candidate=self.normalize(' '.join(s))\n",
        "\n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]        \n",
        "        else:\n",
        "            \n",
        "            curr_candidate=self.normalize(' '.join(s[:cutpoints[0]]))\n",
        "            alt_list=[curr_candidate]\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]\n",
        "\n",
        "            alt_list.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1))\n",
        "            multislices.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1) if self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) in good_candidates)\n",
        "\n",
        "            curr_candidate=self.normalize(' '.join(s[cutpoints[k-1]:]))\n",
        "            alt_list.append(curr_candidate)\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices.append(curr_candidate)\n",
        "            # print('::',alt_list)\n",
        "        return multislices\n",
        "\n",
        "\n",
        "\n",
        "    def get_substring_candidates(self,candidate_words,good_candidates):\n",
        "        n = len(candidate_words)\n",
        "        all_partitions=[]\n",
        "        all_partitions_length=[]\n",
        "        cuts = list(range(1,n))\n",
        "        for k in range(n):\n",
        "            # all_partitions_inner=[]\n",
        "            partition_list=[]\n",
        "            partition_length_list=[]\n",
        "            for cutpoints in itertools.combinations(cuts,k):\n",
        "                ret_list=self.multiSlice(candidate_words,cutpoints,good_candidates)\n",
        "                if(ret_list):\n",
        "                    partition_length=sum([len(elem.split()) for elem in ret_list])\n",
        "                    # print('==',ret_list,partition_length)\n",
        "                    if(partition_length==len(candidate_words)):\n",
        "                        return ret_list\n",
        "                    partition_list.append(ret_list)\n",
        "                    partition_length_list.append(partition_length)\n",
        "                    # yield ret_list\n",
        "            # print('------')\n",
        "            if(partition_length_list):\n",
        "                max_index=partition_length_list.index(max(partition_length_list))\n",
        "                all_partitions.append(partition_list[max_index])\n",
        "                all_partitions_length.append(partition_length_list[max_index])\n",
        "        # print(all_partitions)\n",
        "        if(all_partitions_length):\n",
        "            max_index=all_partitions_length.index(max(all_partitions_length))\n",
        "            # print(all_partitions[max_index])\n",
        "            return all_partitions[max_index]\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "\n",
        "    def verify(self, subsequence, CTrie):\n",
        "        return CTrie.__contains__(subsequence)\n",
        "\n",
        "\n",
        "\n",
        "    def check_sequence(self, sequence, l, CTrie):\n",
        "        result=[]\n",
        "        subsequence_length=l\n",
        "        while(subsequence_length>0):\n",
        "            shift=len(sequence)-subsequence_length\n",
        "            verified_subsequence=[]\n",
        "            verified=False\n",
        "            for i in range(0,shift+1):\n",
        "                list1=sequence[i:(i+subsequence_length)]\n",
        "                text=' '.join(str(e[0]) for e in list1)\n",
        "                subsequence=(self.normalize(text)).split()\n",
        "                #print(\"search for\", subsequence)\n",
        "                if self.verify(subsequence, CTrie):\n",
        "                    verified_subsequence.append(i)\n",
        "                    verified_subsequence.append(i+subsequence_length)\n",
        "                    #print(subsequence)\n",
        "                    #print(subsequence,[(verified_subsequence[0]-0),(int(sequence[-1][1])-verified_subsequence[1])])\n",
        "                    verified=True\n",
        "                    break\n",
        "            if(verified):\n",
        "                result.append(sequence[verified_subsequence[0]:verified_subsequence[1]])\n",
        "                if(verified_subsequence[0]-0)>0:\n",
        "                    subequence_to_check=sequence[0:verified_subsequence[0]]\n",
        "                    #since tokens before the starting position of the verified subsequence have already been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length-1))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                if(int(sequence[-1][1])-verified_subsequence[1])>0:\n",
        "                    subequence_to_check=sequence[(verified_subsequence[1]):]\n",
        "                    #since tokens following the end position of the verified subsequence have not been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                return result\n",
        "            else:\n",
        "                subsequence_length-=1\n",
        "        return result\n",
        "\n",
        "    # def flatten(self,mylist, outlist,ignore_types=(str, bytes, int, ne.NE_candidate)):\n",
        "    def flatten(self,mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "        if mylist !=[]:\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.flatten(item, outlist)\n",
        "                else:\n",
        "                    # if isinstance(item,ne.NE_candidate):\n",
        "                    #     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
        "                    #     item.reset_length()\n",
        "                    # else:\n",
        "                    if type(item)!= int:\n",
        "                        item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "            \n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "            # elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "            #     temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "            #     if(temp1):\n",
        "            #         temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "            #     temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "                #print(index, temp)\n",
        "            # elif \"...\" in word:\n",
        "            #     #print(\"here\")\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"...\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"...\")):\n",
        "            #             temp=list(map(lambda elem: elem+'...', temp))\n",
        "            #         else:\n",
        "            #            temp=list(map(lambda elem: elem+'...', temp[:-1]))+[temp[-1]]\n",
        "            #     # temp.append(temp1[-1])\n",
        "            # elif \"..\" in word:\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"..\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"..\")):\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp))\n",
        "            #         else:\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp[:-1]))+[temp[-1]]\n",
        "            #     #temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def get_Candidates(self, sequence, CTrie,flag):\n",
        "        #flag: debug_flag\n",
        "        candidateList=[]\n",
        "        left=0\n",
        "        start_node=CTrie\n",
        "        last_cand=\"NAN\"\n",
        "        last_cand_substr=\"\"\n",
        "        reset=False\n",
        "        right=0\n",
        "        while (right < len(sequence)):\n",
        "            # if(flag):\n",
        "            #     print(right)\n",
        "            if(reset):\n",
        "                start_node=CTrie\n",
        "                last_cand_substr=\"\"\n",
        "                left=right\n",
        "            curr_text=sequence[right][0]\n",
        "            curr_pos=[sequence[right][1]]\n",
        "            #normalized curr_text\n",
        "            curr=self.normalize(sequence[right][0])\n",
        "            cand_str=self.normalize(last_cand_substr+\" \"+curr)\n",
        "            cand_str_wPunct=(last_cand_substr+\" \"+curr_text).lower()\n",
        "            last_cand_sequence=sequence[left:(right+1)]\n",
        "            last_cand_text=' '.join(str(e[0]) for e in last_cand_sequence)\n",
        "            last_cand_text_norm=self.normalize(' '.join(str(e[0]) for e in last_cand_sequence))\n",
        "            if(flag):\n",
        "                print(\"==>\",cand_str,last_cand_text_norm)\n",
        "            if((cand_str==last_cand_text_norm)&((curr in start_node.path.keys())|(curr_text.lower() in start_node.path.keys()))):\n",
        "            #if (((curr in start_node.path.keys())&(cand_str==last_cand_text_norm))|(curr_text.lower() in start_node.path.keys())):\n",
        "                if flag:\n",
        "                    print(\"=>\",cand_str,last_cand_text)\n",
        "                reset=False\n",
        "                if (curr_text.lower() in start_node.path.keys()):\n",
        "                    if (start_node.path[curr_text.lower()].value_valid):\n",
        "                        last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                        last_cand_batch=start_node.path[curr_text.lower()].feature_list[-1]\n",
        "                        last_cand=last_cand_text\n",
        "                    elif(curr in start_node.path.keys()):\n",
        "                        if ((start_node.path[curr].value_valid)):\n",
        "                            last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                            last_cand=last_cand_text\n",
        "                            last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                        else:\n",
        "                            if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                                #print(\"hehe\",cand_str)\n",
        "                                right=left\n",
        "                                reset=True\n",
        "                    else:\n",
        "                        if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                            #print(\"hehe\",cand_str)\n",
        "                            right=left\n",
        "                            reset=True\n",
        "                elif ((start_node.path[curr].value_valid)&(cand_str==last_cand_text_norm)):\n",
        "                    # if flag:\n",
        "                    #     print(\"==\",last_cand_text)\n",
        "                    last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                    last_cand=last_cand_text\n",
        "                    last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                else:\n",
        "                    if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                        #print(\"hehe\",cand_str)\n",
        "                        right=left\n",
        "                        reset=True\n",
        "                if((curr_text.lower() in start_node.path.keys())&(cand_str==last_cand_text_norm)):\n",
        "                    start_node=start_node.path[curr_text.lower()]\n",
        "                    last_cand_substr=cand_str_wPunct\n",
        "                else:\n",
        "                    start_node=start_node.path[curr]\n",
        "                    last_cand_substr=cand_str\n",
        "            else:\n",
        "                #print(\"=>\",cand_str,last_cand_text)\n",
        "                if(last_cand!=\"NAN\"):\n",
        "                    candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "                    last_cand=\"NAN\"\n",
        "                    if(start_node!=CTrie):\n",
        "                        start_node=CTrie\n",
        "                        last_cand_substr=\"\"\n",
        "                        if curr in start_node.path.keys():\n",
        "                            # if(flag):\n",
        "                            #     print(\"here\",curr)\n",
        "                            reset=False\n",
        "                            if start_node.path[curr].value_valid:\n",
        "                                last_cand_text=curr_text\n",
        "                                last_cand_pos=curr_pos\n",
        "                                last_cand=last_cand_text\n",
        "                                last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                            left=right\n",
        "                            start_node=start_node.path[curr]\n",
        "                            last_cand_substr=curr\n",
        "                        else:\n",
        "                            reset=True\n",
        "                    else:\n",
        "                        reset=True\n",
        "                else:\n",
        "                    if(left<right):\n",
        "                        # if(flag):\n",
        "                        #     print(sequence[(left+1):(right+1)])\n",
        "                        #candidateList.extend(self.get_Candidates(sequence[(left+1):(right+1)], CTrie, flag))\n",
        "                        right=left\n",
        "                        # if(flag):\n",
        "                        #     print(\"++\",right)\n",
        "                    reset=True\n",
        "            right+=1\n",
        "        # if(flag):\n",
        "        #     print(last_cand)\n",
        "        if(last_cand!=\"NAN\"):\n",
        "            candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "        return candidateList\n",
        "\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        #self.data_frame_holder=self.data_frame_holder.append(df,ignore_index=True)\n",
        "        #self.data_frame_holder=self.data_frame_holder.reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def join_token_tuples(self,list_of_tuples):\n",
        "        #print(string.punctuation)\n",
        "        combined_str=(' '.join(tuple[0] for tuple in list_of_tuples)).lstrip(string.punctuation).rstrip(string.punctuation).strip()\n",
        "        combined_pos='*'.join(str(tuple[1]) for tuple in list_of_tuples)\n",
        "        combined_tuple=(combined_str,combined_pos,list_of_tuples[0][2],list_of_tuples[0][3],list_of_tuples[0][4],list_of_tuples[0][5],list_of_tuples[0][6])\n",
        "        return combined_tuple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def all_capitalized(self,candidate):\n",
        "        strip_op=candidate\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip())\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        prep_article_list=prep_list+article_list+self.phase2stopwordList\n",
        "        word_list=strip_op.split()\n",
        "        for i in range(len(word_list)):\n",
        "            word=word_list[i]\n",
        "            if((word[0].isupper())|(word[0].isdigit())):\n",
        "                continue\n",
        "            else:\n",
        "                if(word in prep_article_list):\n",
        "                    if (i!=0):\n",
        "                        continue\n",
        "                    else:\n",
        "                        return False\n",
        "                elif(word in conjoiner):\n",
        "                    continue\n",
        "                else:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def check_feature_update(self, candidate_tuple,non_discriminative_flag):\n",
        "        #print(candidate_tuple)\n",
        "        if(non_discriminative_flag):\n",
        "            return 7\n",
        "        candidateText=candidate_tuple[0]\n",
        "        position=candidate_tuple[1]\n",
        "        word_list=candidateText.split()\n",
        "        if candidateText.islower():\n",
        "            return 6\n",
        "        elif candidateText.isupper():\n",
        "            return 5\n",
        "        elif (len(word_list)==1):\n",
        "            #start-of-sentence-check\n",
        "            if self.all_capitalized(candidateText):\n",
        "                if(int(position[0])==0):\n",
        "                    return 4\n",
        "                else:\n",
        "                    return 2\n",
        "            else:\n",
        "                return 3\n",
        "        else:\n",
        "            if(self.all_capitalized(candidateText)):\n",
        "                return 2\n",
        "            else:\n",
        "                return 3\n",
        "\n",
        "\n",
        "    def update_Candidatedict(self,candidate_tuple,non_discriminative_flag,contextual_embedding_vector):\n",
        "        candidateText=candidate_tuple[0]\n",
        "        normalized_candidate=self.normalize(candidateText)\n",
        "        # print('adding:',normalized_candidate)\n",
        "\n",
        "        feature_list=[]\n",
        "        alt_feature_list=[] # this is for the reduced CandidateBase for the Alt_Classifier\n",
        "        if(normalized_candidate in self.CandidateBase_dict.keys()):\n",
        "            feature_list=self.CandidateBase_dict[normalized_candidate]\n",
        "            alt_feature_list=self.CandidateBase_dict_alt[normalized_candidate]\n",
        "        else:\n",
        "            # feature_list=[0]*9 # only syntax\n",
        "            # feature_list=[0]*777 # context embedding: 768\n",
        "\n",
        "            feature_list=[0]*309 # context embedding: 300\n",
        "            feature_list[0]=self.counter\n",
        "            feature_list[1]=len(normalized_candidate.split())\n",
        "\n",
        "            alt_feature_list=[0]*3\n",
        "            alt_feature_list[0]=self.counter\n",
        "            alt_feature_list[1]=len(normalized_candidate.split())\n",
        "\n",
        "        #syntax_feature to update\n",
        "        feature_to_update=self.check_feature_update(candidate_tuple,non_discriminative_flag)\n",
        "        feature_list[feature_to_update]+=1\n",
        "\n",
        "        # add up the context embedding features\n",
        "        # print(len(contextual_embedding_vector))\n",
        "        feature_list[8:-1]= np.add(feature_list[8:-1],contextual_embedding_vector).tolist()\n",
        "\n",
        "        #increment cumulative frequency\n",
        "        feature_list[-1]+=1\n",
        "        self.CandidateBase_dict[normalized_candidate]=feature_list\n",
        "\n",
        "        alt_feature_list[-1]+=1\n",
        "        self.CandidateBase_dict_alt[normalized_candidate]=alt_feature_list\n",
        "\n",
        "        # alt_points=[]\n",
        "        if(normalized_candidate in self.candidateEmbeddingDict.keys()):\n",
        "            self.candidateEmbeddingDict[normalized_candidate].append(contextual_embedding_vector)\n",
        "        else:\n",
        "            self.candidateEmbeddingDict[normalized_candidate]=[contextual_embedding_vector]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def extract(self,tweetBaseInput,CTrie,phase2stopwordList,new_or_old,train_classifier):\n",
        "\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #output_queue\n",
        "            self.data_frame_holder_OQ=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.not_reintroduced=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.CandidateBase_dict= {}\n",
        "\n",
        "            self.CandidateBase_dict_alt={}\n",
        "            self.candidateEmbeddingDict={}\n",
        "\n",
        "            self.ambiguous_candidate_distanceDict_prev={}\n",
        "            self.partition_dict={}\n",
        "            self.good_candidates=[]\n",
        "            self.bad_candidates=[]\n",
        "            self.ambiguous_candidates=[]\n",
        "\n",
        "            self.aggregator_incomplete_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.just_converted_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            #self.data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates'])\n",
        "            self.raw_tweets_for_others=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "\n",
        "            self.accuracy_tuples_prev_batch=[]\n",
        "            self.accuracy_vals=[]\n",
        "            \n",
        "            #frequency_w_decay related information\n",
        "            self.ambiguous_candidates_reintroduction_dict={}\n",
        "\n",
        "            #### other systems\n",
        "            self.accuracy_vals_stanford=[]\n",
        "            self.accuracy_vals_opencalai=[]\n",
        "            self.accuracy_vals_ritter=[]\n",
        "            self.accuracy_vals_neuroner=[]\n",
        "\n",
        "            self.number_of_seen_tweets_per_batch=[]\n",
        "        self.phase2stopwordList=phase2stopwordList\n",
        "        self.number_of_seen_tweets_per_batch.append(len(tweetBaseInput))\n",
        "\n",
        "\n",
        "        #data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase1_holder_holder=[]\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "        \n",
        "        #candidateBase_holder=[]\n",
        "\n",
        "        #this has to be changed to an append function since IPQ already has incomplete tweets from prev batch  \n",
        "        #print(len(tweetBaseInput))\n",
        "        #immediate_processingQueue = pd.concat([self.incomplete_tweets,TweetBase ])\n",
        "        #immediate_processingQueue.to_csv(\"impq.csv\", sep=',', encoding='utf-8')\n",
        "        \n",
        "\n",
        "\n",
        "        #print('In Phase 2',len(immediate_processingQueue))\n",
        "        #immediate_processingQueue=immediate_processingQueue.reset_index(drop=True)\n",
        "        combined_list_here=([]+list(cachedStopWords)+chat_word_list+day_list+month_list+article_list+prep_list)\n",
        "        combined_list_filtered=list(filter(lambda word: word not in (prep_list+article_list+month_list+self.phase2stopwordList), combined_list_here))\n",
        "        #--------------------------------------PHASE II---------------------------------------------------\n",
        "        for index, row in tweetBaseInput.iterrows():\n",
        "\n",
        "            #phase 1 candidates for one sentence\n",
        "            phase1_holder=[]\n",
        "\n",
        "            tweetText=str(row['TweetSentence'])\n",
        "            tweetWordList = row['tweetwordList']\n",
        "            sentID=str(row['sentID'])\n",
        "            tweetID=str(row['tweetID'])\n",
        "            phase1Candidates=str(row['phase1CandidatesWPositions'])\n",
        "            batch=int(row['entry_batch'])\n",
        "            contextual_embeddings_dict= {ind: embedding for ind, embedding in enumerate(row['contextual_embeddings'])}\n",
        "            \n",
        "            # print('====',tweetID,sentID)\n",
        "            # print('tweetWordList:',tweetWordList)\n",
        "            \n",
        "            non_discriminative_flag=False\n",
        "            phase1CandidatesList=[]\n",
        "\n",
        "            # print('phase1Candidates:',phase1Candidates)\n",
        "\n",
        "            if (phase1Candidates !='nan'):\n",
        "                phase1Raw=phase1Candidates.split(\"||\")\n",
        "                phase1Raw = list(filter(None, phase1Raw))\n",
        "\n",
        "\n",
        "                for entities_with_loc in phase1Raw:\n",
        "                    entity_to_store=entities_with_loc.split(\"::\")[0]\n",
        "                    #print(entity_to_store)\n",
        "                    position=entities_with_loc.split(\"::\")[1]\n",
        "                    #print(position)\n",
        "                    phase1_holder.append((entity_to_store,position))\n",
        "                    phase1_holder.clear()\n",
        "                    phase1CandidatesList.append(entity_to_store.lower())\n",
        "\n",
        "                phase1_holder_holder.append(copy.deepcopy(phase1_holder))\n",
        "                \n",
        "\n",
        "            else:\n",
        "                non_discriminative_flag=True\n",
        "                phase1_holder_holder.append([])\n",
        "\n",
        "            \n",
        "            # tweetWordList=self.getWords(tweetText)\n",
        "            # tweetWordList= [(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "            \n",
        "            tweetWordList_stopWords=list(filter (lambda word: ((((word[0].strip()).strip(string.punctuation)).lower() in combined_list_filtered)|(word[0].strip() in string.punctuation)|(word[0].startswith('@'))|(word[0].startswith('#'))), tweetWordList))\n",
        "\n",
        "            # phase 2 candidate tuples without stopwords for a sentence\n",
        "            c=[(y[0],str(y[1]),tweetID,sentID,'ne',batch,time) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "            #c=[(y[0],str(y[1])) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "\n",
        "            \n",
        "            sequences=[]\n",
        "            for k, g in groupby(enumerate(c), lambda element: element[0]-int(element[1][1])):\n",
        "                sequences.append(list(map(itemgetter(1), g)))\n",
        "\n",
        "            # print('phaseII candidates:')\n",
        "            ne_candidate_list=[]\n",
        "            for sequence in sequences:\n",
        "                seq_candidate_list=self.get_Candidates(sequence, CTrie,False)\n",
        "                if(seq_candidate_list):\n",
        "                    for candidate_tuple in seq_candidate_list:\n",
        "                        # print(candidate_tuple)\n",
        "\n",
        "                        # extract candidate token embeddings\n",
        "                        candidate_token_embeddings = torch.stack([contextual_embeddings_dict[int(position)] for position in candidate_tuple[1]])\n",
        "                        # print('candidate with token_embeddings:',candidate_tuple[0],candidate_token_embeddings.shape,len(candidate_tuple[1]))\n",
        "\n",
        "                        # !!necessary because this function during training receives [1,n,768] tensors that it squeezes; so might screw up 1-token sentences\n",
        "                        candidate_embedding = self.entity_phrase_embedder.getEmbedding(candidate_token_embeddings.unsqueeze(0))\n",
        "                        # print('candidate_embedding:',candidate_embedding.shape)\n",
        "\n",
        "                        #inserts into CandidateBase: Syntax and Context Feature Setting\n",
        "                        if not ((float(batch)<self.counter)&(candidate_tuple[-1]<self.counter)):\n",
        "                            self.update_Candidatedict(candidate_tuple,non_discriminative_flag,candidate_embedding.tolist())\n",
        "\n",
        "                    ne_candidate_list.extend(seq_candidate_list)\n",
        "\n",
        "            phase2_candidates=[self.normalize(e[0]) for e in ne_candidate_list]\n",
        "            phase2_candidates_unnormalized=[e[0] for e in ne_candidate_list]\n",
        "\n",
        "            phase2_candidates_holder.append(phase2_candidates)\n",
        "            phase2_unnormalized_candidates_holder.append(phase2_candidates_unnormalized)\n",
        "\n",
        "            dict1 = {'entry_batch':batch, 'tweetID':tweetID, 'sentID':sentID, 'TweetSentence':tweetText, 'phase1Candidates':phase1CandidatesList,'2nd Iteration Candidates':phase2_candidates,'2nd Iteration Candidates Unnormalized':phase2_candidates_unnormalized}\n",
        "\n",
        "            df_holder.append(dict1)\n",
        "            \n",
        "            #-------------------------------------------------------------------END of 1st iteration: RESCAN+CANDIDATE_UPDATION-----------------------------------------------------------\n",
        "\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        # # candidate_records=pd.read_csv('data/training.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_large.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidateBase_dict_filtered={}\n",
        "        # for candidate in self.CandidateBase_dict:\n",
        "        #     if(candidate in candidate_list):\n",
        "        #         candidateBase_dict_filtered[candidate] = self.CandidateBase_dict[candidate]\n",
        "\n",
        "        # # candidateBase_dict_filtered = self.CandidateBase_dict\n",
        "\n",
        "        # candidate_featureBase_DF_filtered=pd.DataFrame.from_dict(candidateBase_dict_filtered, orient='index')\n",
        "        # candidate_featureBase_DF_filtered.columns=self.candidateBaseHeaders[1:]\n",
        "        # candidate_featureBase_DF_filtered.index.name=self.candidateBaseHeaders[0]\n",
        "        # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered.reset_index(drop=False)\n",
        "        # # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered[candidate_featureBase_DF_filtered['cumulative'] >= 5.0]\n",
        "\n",
        "        # print('before:', len(self.CandidateBase_dict),'after:',len(candidateBase_dict_filtered))\n",
        "        \n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "        # candidate_featureBase_DF_filtered['class'] = candidate_featureBase_DF_filtered.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # # candidate_featureBase_DF_filtered['class'] = 0\n",
        "\n",
        "        # candidate_featureBase_DF_filtered.to_csv(\"data/candidate_train_records_large_300d.csv\", sep=',', encoding='utf-8')\n",
        "        \n",
        "        # return #comment out if not collecting records for classifier training\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        # #===============comment out if not training alt_classifier===============\n",
        "\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_large.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidateBase_dict_filtered={}\n",
        "        # for candidate in self.CandidateBase_dict:\n",
        "        #     if(candidate in candidate_list):\n",
        "        #         candidateBase_dict_filtered[candidate] = self.CandidateBase_dict_alt[candidate]\n",
        "\n",
        "        # candidate_featureBase_DF_filtered=pd.DataFrame.from_dict(candidateBase_dict_filtered, orient='index')\n",
        "        # candidate_featureBase_DF_filtered.columns=self.candidateBaseHeaders_alt[1:]\n",
        "        # candidate_featureBase_DF_filtered.index.name=self.candidateBaseHeaders_alt[0]\n",
        "        # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered.reset_index(drop=False)\n",
        "        # # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered[candidate_featureBase_DF_filtered['cumulative'] >= 5.0]\n",
        "\n",
        "        # print('before:', len(self.CandidateBase_dict_alt),'after:',len(candidateBase_dict_filtered))\n",
        "        \n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "        # candidate_featureBase_DF_filtered['class'] = candidate_featureBase_DF_filtered.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # candidate_featureBase_DF_filtered.to_csv(\"data/candidate_train_records_altClassifier.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "\n",
        "        # # # #=============== to do a fresh labeling with all generated candidates\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_altClassifier.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "\n",
        "        # candidate_featureBase_DF_alt = pd.DataFrame.from_dict(self.CandidateBase_dict_alt, orient='index')\n",
        "\n",
        "        # candidate_featureBase_DF_alt.columns = self.candidateBaseHeaders_alt[1:]\n",
        "        # candidate_featureBase_DF_alt.index.name = self.candidateBaseHeaders_alt[0]\n",
        "        # candidate_featureBase_DF_alt  = candidate_featureBase_DF_alt.reset_index(drop=False)\n",
        "\n",
        "        # # candidate_featureBase_DF_alt['class']=-1\n",
        "        # val_list=[]\n",
        "        # for index, row in candidate_featureBase_DF_alt.iterrows():\n",
        "        #     try:\n",
        "        #         val_list.append(candidate_records.loc[row.candidate]['class'])\n",
        "        #     except KeyError:\n",
        "        #         val_list.append(-1)\n",
        "        # candidate_featureBase_DF_alt['class']=val_list\n",
        "        # # candidate_featureBase_DF_alt['class'] = candidate_featureBase_DF_alt.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # candidate_featureBase_DF_alt.to_csv(\"data/candidate_train_records_altClassifier_new.csv\", sep=',', encoding='utf-8')\n",
        "        # return\n",
        "        # #=============== to do a fresh labeling with all generated candidates\n",
        "\n",
        "        # # #=============== to train the alt classifier\n",
        "        if(train_classifier):\n",
        "            self.entity_classifier_alt = EntityClassifierAlt('data/candidate_train_records_altClassifier_new.csv',True,self.device,self.candidateEmbeddingDict,self.CandidateBase_dict_alt)\n",
        "            return\n",
        "\n",
        "        # # #===============comment out if not training alt_classifier===============\n",
        "\n",
        "        # # #=============== for test run with regular Classifier ===============\n",
        "\n",
        "        # candidate_featureBase_DF = pd.DataFrame.from_dict(self.CandidateBase_dict, orient='index')\n",
        "        # candidate_featureBase_DF.columns = self.candidateBaseHeaders[1:]\n",
        "        # candidate_featureBase_DF.index.name = self.candidateBaseHeaders[0]\n",
        "        # candidate_featureBase_DF = candidate_featureBase_DF.reset_index(drop=False)\n",
        "        # return candidate_featureBase_DF,df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "        # #=============== for test run with alt Classifier ===============\n",
        "\n",
        "        candidate_featureBase_DF_alt = pd.DataFrame.from_dict(self.CandidateBase_dict_alt, orient='index')\n",
        "        candidate_featureBase_DF_alt.columns = self.candidateBaseHeaders_alt[1:]\n",
        "        candidate_featureBase_DF_alt.index.name = self.candidateBaseHeaders_alt[0]\n",
        "        candidate_featureBase_DF_alt  = candidate_featureBase_DF_alt.reset_index(drop=False)\n",
        "\n",
        "        self.entity_classifier_alt = EntityClassifierAlt('data/candidate_train_records_altClassifier_new.csv',False,self.device,self.candidateEmbeddingDict,self.CandidateBase_dict_alt)\n",
        "        return candidate_featureBase_DF_alt,df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "\n",
        "        ## self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        ## self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets_for_current_batch)\n",
        "\n",
        "\n",
        "    def finish(self):\n",
        "        return self.accuracy_vals\n",
        "\n",
        "    def finish_other_systems(self):\n",
        "        stanford_f1=[]\n",
        "        stanford_precision=[]\n",
        "        stanford_recall=[]\n",
        "        print(\"*****************************************STANFORD***********************\")\n",
        "        for i in self.accuracy_vals_stanford:\n",
        "            stanford_f1.append(i[0])\n",
        "            stanford_precision.append(i[1])\n",
        "            stanford_recall.append(i[2])\n",
        "            # print(i)\n",
        "        print('stanford_f1:', stanford_f1)\n",
        "        print('stanford_precision:', stanford_precision)\n",
        "        print('stanford_recall:', stanford_recall)\n",
        "\n",
        "        print(sum(stanford_f1)/len(stanford_f1))\n",
        "        print(sum(stanford_precision)/len(stanford_precision))\n",
        "        print(sum(stanford_recall)/len(stanford_recall))\n",
        "\n",
        "        print(\"*****************************************Opencalai***********************\")\n",
        "        opencalai_f1=[]\n",
        "        opencalai_precision=[]\n",
        "        opencalai_recall=[]\n",
        "        for i in self.accuracy_vals_opencalai:\n",
        "            opencalai_f1.append(i[0])\n",
        "            opencalai_precision.append(i[1])\n",
        "            opencalai_recall.append(i[2])\n",
        "        print('opencalai_f1:', opencalai_f1)\n",
        "        print('opencalai_precision:', opencalai_precision)\n",
        "        print('opencalai_recall:', opencalai_recall)\n",
        "\n",
        "        print(sum(opencalai_f1)/len(opencalai_f1))\n",
        "        print(sum(opencalai_precision)/len(opencalai_precision))\n",
        "        print(sum(opencalai_recall)/len(opencalai_recall))\n",
        "        print(\"*****************************************Ritter***********************\")\n",
        "        ritter_f1=[]\n",
        "        ritter_precision=[]\n",
        "        ritter_recall=[]\n",
        "        for i in self.accuracy_vals_ritter:\n",
        "            ritter_f1.append(i[0])\n",
        "            ritter_precision.append(i[1])\n",
        "            ritter_recall.append(i[2])\n",
        "        print('ritter_f1:', ritter_f1)\n",
        "        print('ritter_precision:', ritter_precision)\n",
        "        print('ritter_recall:', ritter_recall)\n",
        "\n",
        "        print(sum(ritter_f1)/len(ritter_f1))\n",
        "        print(sum(ritter_precision)/len(ritter_precision))\n",
        "        print(sum(ritter_recall)/len(ritter_recall))\n",
        "        print(\"*****************************************Neuroner***********************\")\n",
        "        neuroner_f1=[]\n",
        "        neuroner_precision=[]\n",
        "        neuroner_recall=[]\n",
        "        for i in self.accuracy_vals_neuroner:\n",
        "            neuroner_f1.append(i[0])\n",
        "            neuroner_precision.append(i[1])\n",
        "            neuroner_recall.append(i[2])\n",
        "        print('neuroner_f1:', neuroner_f1)\n",
        "        print('neuroner_precision:', neuroner_precision)\n",
        "        print('neuroner_recall:',neuroner_recall)\n",
        "\n",
        "        print(sum(neuroner_f1)/len(neuroner_f1))\n",
        "        print(sum(neuroner_precision)/len(neuroner_precision))\n",
        "        print(sum(neuroner_recall)/len(neuroner_recall))\n",
        "\n",
        "        return (self.accuracy_vals_stanford,self.accuracy_vals_opencalai,self.accuracy_vals_ritter,self.accuracy_vals_neuroner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY3zs23wC6_3"
      },
      "source": [
        "## **External Run of BERTweet Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5tTI3fIl9yC"
      },
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"vinai/bertweet-base\"\n",
        "batch_size = 32\n",
        "# set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTip5Uxs29P8"
      },
      "source": [
        "# # reading the training set\n",
        "# f = open(\"wnut17_train.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_train = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_train))\n",
        "# df_train.to_csv(\"wnut17train.csv\", sep=',', encoding='utf-8',index=False)\n",
        "\n",
        "# # reading the validation set\n",
        "# f = open(\"wnut17_validation.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_validation = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_validation))\n",
        "# df_validation.to_csv(\"wnut17validation.csv\", sep=',', encoding='utf-8',index=False)\n",
        "\n",
        "# # reading the test set\n",
        "# f = open(\"wnut17_test.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_test = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_test))\n",
        "# df_test.to_csv(\"wnut17test.csv\", sep=',', encoding='utf-8',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlQEqoIJqo4l"
      },
      "source": [
        "# class TweetNERDataset(torch.utils.data.Dataset):\n",
        "\n",
        "#     def __init__(self, input, output):\n",
        "\n",
        "#         # print(data[0])\n",
        "#         # self.data = np.asarray(data)\n",
        "#         # self.output = np.asarray(output)\n",
        "\n",
        "#         datasets = load_dataset(\"wnut_17\")\n",
        "\n",
        "#         self.input = input\n",
        "#         self.output = output\n",
        "\n",
        "#         print(type(self.input),type(self.output))\n",
        "\n",
        "#     def prepare_input():\n",
        "        \n",
        "\n",
        "#     def tokenize_and_align_labels(self, example):\n",
        "        \n",
        "#         tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "#         inputId_to_token_dict={}\n",
        "#         for index, token in enumerate(example[\"tokens\"]):\n",
        "#             values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "#             for value in values:\n",
        "#                 try:\n",
        "#                     inputId_to_token_dict[value].append(index)\n",
        "#                 except KeyError:\n",
        "#                     inputId_to_token_dict[value]=[index]\n",
        "#         labels=[]\n",
        "#         for inputID in tokenized_input['input_ids']:\n",
        "#             try:\n",
        "#                 index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "#                 index_to_address=index_list.pop(0)\n",
        "\n",
        "#                 label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "#                 # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "#                 labels.append(label)\n",
        "#                 inputId_to_token_dict[inputID]=index_list\n",
        "#             except KeyError:\n",
        "#                 labels.append(-100)\n",
        "\n",
        "#         assert (len(tokenized_input['input_ids']) == len(labels))\n",
        "#         tokenized_input['labels']=labels\n",
        "        \n",
        "#         return tokenized_input\n",
        "\n",
        "#     def __len__(self):\n",
        "#         assert len(self.input) == len(self.output)\n",
        "#         return len(self.input)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         X = self.input[idx]\n",
        "#         y = self.output[idx]\n",
        "#         return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDmC8lnI-99G"
      },
      "source": [
        "\n",
        "# train_dataset = load_dataset('csv', data_files=\"wnut17train.csv\")\n",
        "# validation_dataset = load_dataset('csv', data_files='wnut17validation.csv')\n",
        "# test_dataset = load_dataset('csv', data_files=\"wnut17test.csv\")\n",
        "\n",
        "datasets = load_dataset(\"wnut_17\")\n",
        "\n",
        "# datasets = load_dataset(\"conll_2003\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaoyxjMX8bTP"
      },
      "source": [
        "# training_set = TweetNERDataset(datasets[\"train\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z17jPrtnKuu",
        "outputId": "45d7b7ff-6899-4068-cd37-1de70fe54b0f"
      },
      "source": [
        "# train_dataset\n",
        "datasets\n",
        "# train_dataset[\"train\"].features[f\"ner_tags\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 3394\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1009\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1287\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkvoRJbVbIeX"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "label_all_tokens = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csG7slfdkVwg"
      },
      "source": [
        "print(datasets['train'][-1])\n",
        "print(type(datasets['train']))\n",
        "print(type(datasets['train'][-1]))\n",
        "\n",
        "# last_record_dict = {'id': '3394', 'ner_tags': [1, 2, 0, 1, 2, 0, 0], 'tokens': ['Bill', 'Nye', 'explains', 'Global', 'Warming', 'to', 'adults']}\n",
        "# datasets['train'].append(last_record_dict)\n",
        "\n",
        "# last_id = '3394'\n",
        "# datasets['train']['id'].append(last_id)\n",
        "\n",
        "# last_ner_tags = [1, 2, 0, 1, 2, 0, 0]\n",
        "# datasets['train']['ner_tags'].append(last_ner_tags)\n",
        "\n",
        "# last_tokens = ['Bill', 'Nye', 'explains', 'Global', 'Warming', 'to', 'adults'] #Bill Nye explains Global Warming to adults\n",
        "# datasets['train']['tokens'].append(last_tokens)\n",
        "\n",
        "# example = datasets[\"train\"][4]\n",
        "# print(example[\"tokens\"])\n",
        "# tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "# # input_ids = tokenizer.encode(example[\"tokens\"])\n",
        "# # tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "# print(tokenized_input)\n",
        "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "# print(tokens)\n",
        "# # len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])\n",
        "# print({x : tokenizer.encode(x, add_special_tokens=False) for x in example[\"tokens\"]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5n31Mh1_l04"
      },
      "source": [
        "# print(datasets['train'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyth64PVbIht"
      },
      "source": [
        "expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location',\n",
        "                     8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "BIO_dict={'O':0,'B':1,'I':2}\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "        \n",
        "    tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "    inputId_to_token_dict={}\n",
        "    for index, token in enumerate(example[\"tokens\"]):\n",
        "        values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "        for value in values:\n",
        "            try:\n",
        "                inputId_to_token_dict[value].append(index)\n",
        "            except KeyError:\n",
        "                inputId_to_token_dict[value]=[index]\n",
        "    labels=[]\n",
        "    for inputID in tokenized_input['input_ids']:\n",
        "        try:\n",
        "            index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "            index_to_address=index_list.pop(0)\n",
        "\n",
        "            label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "            # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "            labels.append(label)\n",
        "            inputId_to_token_dict[inputID]=index_list\n",
        "        except KeyError:\n",
        "            labels.append(-100)\n",
        "\n",
        "    assert (len(tokenized_input['input_ids']) == len(labels))\n",
        "    tokenized_input['labels']=labels\n",
        "    \n",
        "    return tokenized_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q5SzJpD4cId"
      },
      "source": [
        "# tokenize_and_align_labels(datasets['train'][2])\n",
        "tokenized_datasets = datasets.map(tokenize_and_align_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XthRN5iUh_Kq"
      },
      "source": [
        "tokenized_datasets['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz4Y6EFnvB_e"
      },
      "source": [
        "print(tokenized_datasets['train'][-1])\n",
        "print(type(tokenized_datasets['train']))\n",
        "print(type(tokenized_datasets['train'][-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLWFWwewCBjL"
      },
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhh4V16GoHKx"
      },
      "source": [
        "# tokenized_datasets['train'].features[\"labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNzwMflzkDi3"
      },
      "source": [
        "# label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "\n",
        "label_list = ['O','B','I']\n",
        "\n",
        "print(label_list)\n",
        "print(len(label_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV4eL-hsIafT"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # print(p.shape)\n",
        "    output, labels = p\n",
        "\n",
        "    # print(len(predictions))\n",
        "    # print(predictions[0].shape)\n",
        "    # for elem in predictions[1]:\n",
        "    #   print(elem.shape)\n",
        "\n",
        "    predictions, _ = output\n",
        "    \n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNcDvqvUkgkc"
      },
      "source": [
        "# config = AutoConfig.from_pretrained(model_checkpoint, output_hidden_states=True)\n",
        "# config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9akEbBY2ZZl"
      },
      "source": [
        "alt_model = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(label_list))\n",
        "alt_model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRnD-mNZUaia"
      },
      "source": [
        "batch_size=32\n",
        "alt_training_args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "alt_training_args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aACODJ6AWVDG"
      },
      "source": [
        "alt_trainer = Trainer(\n",
        "    alt_model,\n",
        "    alt_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxw-OLa6mitu"
      },
      "source": [
        "**Model output is a tuple, when all hidden states are returned, i.e. output_hidden_states =True in config: (output, hidden-layers)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uryn4hHX06d"
      },
      "source": [
        "alt_trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU3-mGQJICbx"
      },
      "source": [
        "# tokenizer.save_pretrained('test-ner/')\n",
        "# alt_model.save_pretrained('test-ner/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRvhsCdn-NxN"
      },
      "source": [
        "## **Extracting Embeddings to train the Phrase Embedder on STS Dataset (NOT REQD TO RUN)-- unless training the phrase Embedder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfxtWYA50eDp"
      },
      "source": [
        "# sys.path.insert(0,'/content/gdrive/My Drive/BERTweet-ner')\n",
        "# print(sys.path)\n",
        "\n",
        "# sys.path.append('/content/gdrive/My Drive/BERTweet-ner')\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRg-uD1iS5Mm"
      },
      "source": [
        "class PhraseEmbeddingDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, input, output):\n",
        "\n",
        "        # print(data[0])\n",
        "        # self.data = np.asarray(data)\n",
        "        # self.output = np.asarray(output)\n",
        "\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "\n",
        "        print(type(self.input),type(self.output))\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.input) == len(self.output)\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.input[idx]\n",
        "        y = self.output[idx]\n",
        "        return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cqFnxb0l-1J"
      },
      "source": [
        "def get_STS_Data_embeddings(dataset):\n",
        "    predictions=[]\n",
        "    tokenized_sentences=[]\n",
        "    count=0\n",
        "    entity_embeddings=[]\n",
        "    with torch.no_grad():\n",
        "        for record in dataset:\n",
        "            # print(record)\n",
        "            # record=record.lower()\n",
        "            tokenized_input=tokenizer(record)\n",
        "            initial_input_ids = torch.tensor([tokenizer.encode(record)])\n",
        "            token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in record.split()}\n",
        "            input_ids = initial_input_ids.to(device)\n",
        "            tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "            # output = model(input_ids)\n",
        "\n",
        "            output = alt_model(input_ids)\n",
        "\n",
        "            if(count<5):\n",
        "                print(len(output.hidden_states))\n",
        "                print(output.hidden_states[-1].shape)\n",
        "            \n",
        "            token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "\n",
        "            prediction = (torch.argmax(output.logits, axis=2))\n",
        "            prediction = prediction.cpu().numpy().reshape(-1)\n",
        "            prediction_labels=[label_list[l].split('-')[0] for l in prediction]\n",
        "            \n",
        "#             print(token_embeddings.shape)\n",
        "            prediction_labels, entity_aware_embeddings=collate_token_label_embedding(record.split(), token_dict, prediction_labels[1:-1],token_embeddings)\n",
        "            \n",
        "            assert (len(prediction_labels)==len(record.split()))\n",
        "            assert (len(entity_aware_embeddings)==len(record.split()))\n",
        "\n",
        "            predictions.append(prediction_labels)\n",
        "            entity_embeddings.append(torch.stack(entity_aware_embeddings))\n",
        "            tokenized_sentences.append(token_dict.keys())\n",
        "\n",
        "            if(count<5):\n",
        "                print(len(entity_aware_embeddings))\n",
        "\n",
        "            count+=1\n",
        "\n",
        "    print(len(predictions),len(tokenized_sentences),len(entity_embeddings))\n",
        "    return entity_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFKMsh5Arns8"
      },
      "source": [
        "def get_sts_data(filename):\n",
        "    stsDataDictList=[]\n",
        "    stsData_columns =['sentence1', 'sentence2', 'score']\n",
        "    f=open(\"data/stsbenchmark/\"+filename+\".csv\",'r')\n",
        "    file_text= f.read()\n",
        "    lines=file_text.split('\\n')\n",
        "    for line in lines:\n",
        "        if(line):\n",
        "            fields=line.split('\\t')\n",
        "    #         print(len(fields))\n",
        "            dataDict={'sentence1':fields[5],'sentence2':fields[6],'score':float(fields[4])/5.0}\n",
        "    #         print(dataDict)\n",
        "            stsDataDictList.append(dataDict)\n",
        "    stsData=pd.DataFrame(stsDataDictList)\n",
        "    return stsData\n",
        "# stsTrainData.columns =['genre', 'filename', 'year', 'unidentified', 'score', 'sentence1', 'sentence2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvMv5O0MmFVG"
      },
      "source": [
        "stsTrainData = get_sts_data('sts-train')\n",
        "print(len(stsTrainData))\n",
        "print(stsTrainData.columns)\n",
        "\n",
        "stsDevData = get_sts_data('sts-dev')\n",
        "print(len(stsDevData))\n",
        "print(stsDevData.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fSzSvqFmVEW"
      },
      "source": [
        "# first pass it through NER Engine and get the contextual embeddings\n",
        "\n",
        "#Training Data\n",
        "#For Source Sentences:\n",
        "source_sentence_embeddings_train = get_STS_Data_embeddings(stsTrainData['sentence1'].tolist())\n",
        "#For Target Sentences:\n",
        "target_sentence_embeddings_train = get_STS_Data_embeddings(stsTrainData['sentence2'].tolist())\n",
        "assert len(source_sentence_embeddings_train)==len(target_sentence_embeddings_train)\n",
        "\n",
        "#Validation Data\n",
        "#For Source Sentences:\n",
        "source_sentence_embeddings_dev = get_STS_Data_embeddings(stsDevData['sentence1'].tolist())\n",
        "#For Target Sentences:\n",
        "target_sentence_embeddings_dev = get_STS_Data_embeddings(stsDevData['sentence2'].tolist())\n",
        "assert len(source_sentence_embeddings_dev)==len(target_sentence_embeddings_dev)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TXrkbXVIXp3"
      },
      "source": [
        "#Printing some shapes\n",
        "print(len(source_sentence_embeddings_train), len(target_sentence_embeddings_train))\n",
        "print(source_sentence_embeddings_train[0].shape)\n",
        "embeddingSize=list(source_sentence_embeddings_train[0][0].shape)[0]\n",
        "# print(embeddingSize)\n",
        "# print(type(list(embeddingSize)[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2WxtYySGMHA"
      },
      "source": [
        "# Datasets and DataLoaders\n",
        "training_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_train, target_sentence_embeddings_train))),stsTrainData['score'].tolist())\n",
        "training_generator = torch.utils.data.DataLoader(training_set, shuffle=True)\n",
        "\n",
        "validation_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_dev, target_sentence_embeddings_dev))),stsDevData['score'].tolist())\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfFolw3IhrA"
      },
      "source": [
        "x,y =training_set.__getitem__(0)\n",
        "print(type(x))\n",
        "print(x[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dje0rlO66WDC"
      },
      "source": [
        "def save_ckp(state, is_best, checkpoint_dir):\n",
        "    # f_path = checkpoint_dir + '/checkpoint.pt' \n",
        "    f_path = checkpoint_dir + '/checkpoint_model300.pt' #_model300\n",
        "    torch.save(state, f_path)\n",
        "    # if is_best:\n",
        "    #     best_fpath = best_model_dir +'/best_model.pt'\n",
        "    #     shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGbFUV5PcX8Q"
      },
      "source": [
        "# !pip3 install pytorchtools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7rLtDdOREIH"
      },
      "source": [
        "# import EarlyStopping\n",
        "# from pytorchtools import EarlyStopping\n",
        "\n",
        "# Initialize network\n",
        "phraseEmbeddingModel = PhraseEmbedding(embeddingSize, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# initialize the early_stopping object\n",
        "# early_stopping = EarlyStopping(patience=patience, verbose=True, path='entityEmbedding/model_checkpoints/checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM4tsJ6l0BxX"
      },
      "source": [
        "checkpoint_dir='entityEmbedding/model_checkpoints'\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 200\n",
        "patience = 5\n",
        "\n",
        "# Train Network\n",
        "history_training= []\n",
        "history_validation = []\n",
        "best_loss = np.float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_batch_loss=[]\n",
        "    for batch_idx, (data, target) in enumerate(training_generator):\n",
        "        target = torch.tensor(float(target)).to(device=device)\n",
        "        out = phraseEmbeddingModel(data)\n",
        "        # print(out.item())\n",
        "        # if(not math.isnan(out.item())):\n",
        "            # print(data)\n",
        "        loss = criterion(out, target)\n",
        "        # print(loss.item())\n",
        "        training_batch_loss.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(phraseEmbeddingModel.parameters(), 1.0)\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "    combined_training_loss = np.mean(training_batch_loss)\n",
        "    print('combined epoch training loss:', combined_training_loss)\n",
        "    history_training.append(combined_training_loss)\n",
        "\n",
        "    #Validation: DO NOT BACKPROPAGATE HERE\n",
        "    validation_batch_loss = []\n",
        "    print_only_one=True\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(validation_generator):\n",
        "            target = torch.tensor(float(target)).to(device=device)\n",
        "            if(print_only_one):\n",
        "                # print(len(data[0]),len(data[1]))\n",
        "                # print(type(data))\n",
        "                print_only_one=False\n",
        "            out = phraseEmbeddingModel(data)\n",
        "            loss = criterion(out, target)\n",
        "            validation_batch_loss.append(loss.item())\n",
        "            # print(validation_batch_loss)\n",
        "    combined_validation_loss= np.mean(validation_batch_loss)\n",
        "    history_validation.append(combined_validation_loss)\n",
        "    # if(((epoch+1)%10==0)|(epoch == (num_epochs-1))):\n",
        "    print('\\nEpoch',str(epoch+1),' Validation Loss:',combined_validation_loss)\n",
        "    print('=====================================================================\\n')\n",
        "    \n",
        "    if(combined_validation_loss<best_loss):\n",
        "        best_loss = combined_validation_loss\n",
        "        print('saving this checkpoint')\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        save_ckp(checkpoint, True, checkpoint_dir)\n",
        "        no_improvement_counter=0\n",
        "    else:\n",
        "        no_improvement_counter+=1\n",
        "        if(no_improvement_counter>patience):\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhKf3SBXPjI"
      },
      "source": [
        "# checkpoint = {\n",
        "#     'epoch': epoch + 1,\n",
        "#     'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "#     'optimizer': optimizer.state_dict()\n",
        "# }\n",
        "# checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint.pt' #768\n",
        "checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint_model300.pt' #300\n",
        "# model_dir = 'best-model'\n",
        "# save_ckp(checkpoint, True, checkpoint_dir, model_dir)\n",
        "\n",
        "# load the last checkpoint with the best model\n",
        "entityPhraseEmbedder, optimizer, start_epoch = load_ckp(checkpoint_path, phraseEmbeddingModel, optimizer)\n",
        "print('Loading model from epoch:', start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhVLIVUy57ZX"
      },
      "source": [
        "## **Training the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rck4IiG65lkt"
      },
      "source": [
        "#to train the Entity Classifiers\n",
        "# tweets_unpartitoned=pd.read_csv('data/deduplicated_test.csv',sep =';',keep_default_na=False)\n",
        "tweets_unpartitoned=pd.read_csv('data/large_training_file.csv',sep =',',keep_default_na=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PCjtUKR6IOl"
      },
      "source": [
        "# import localNER as localNER (self, sentenceTokenizer, nerTokenizer, nerEngine, label_list, device):\n",
        "# local_NER_Module= localNER.LocalNERModule(sentenceTokenizer, tokenizer, alt_model, label_list, device)\n",
        "\n",
        "#tokenizer here is the BERT model's tokenizer\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, tokenizer, alt_model, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I9OVbA6Q1sR"
      },
      "source": [
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'classifier-train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Qa9DQDpWIG"
      },
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThM_gzA8CqPd"
      },
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH3mGcUI9p-Q"
      },
      "source": [
        "tweet_to_sentences_w_annotation={}\n",
        "# df_out_holder_Phase1=[]\n",
        "total_time=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1\n",
        "\n",
        "# for g, tweet_batch in tweets_unpartitoned.groupby(np.arange(length) //batch_size):\n",
        "phaseI_timein=time.time()\n",
        "g=0\n",
        "tuple_of= local_NER_Module.extract(tweet_batch,g)\n",
        "phaseI_timeout=time.time()\n",
        "\n",
        "print('local emd time',(phaseI_timeout-phaseI_timein))\n",
        "\n",
        "tweet_base=tuple_of[0]\n",
        "contextual_embeddings=tuple_of[1]\n",
        "candidate_base=tuple_of[2]\n",
        "elapsedTime= tuple_of[4] - tuple_of[3]\n",
        "phase2stopwordList=tuple_of[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=tuple_of[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n",
        "\n",
        "# candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5RnF9Es20I1"
      },
      "source": [
        "# # # #PHASE I CHECKING:\n",
        "# # processed_tweets=pd.concat(df_out_holder_Phase1,ignore_index=True)\n",
        "# print(len(tweet_base.phase1Candidates.values),local_NER_Module.sentenceID)\n",
        "# calculate_f1(tweet_to_sentences_w_annotation, tweet_base.phase1Candidates.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSH_Ae_LDXWB"
      },
      "source": [
        "# candidates=candidate_base.displayTrie(\"\",[])\n",
        "# print(len(candidates))\n",
        "# print(candidates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzk71F1f-d1J"
      },
      "source": [
        "phaseII_timein=time.time()\n",
        "train_classifier = True\n",
        "\n",
        "#training does not return anything\n",
        "global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch,train_classifier)\n",
        "phaseII_timeout=time.time()\n",
        "\n",
        "print('global emd time',(phaseII_timeout-phaseII_timein))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfxr_dcV4SpM"
      },
      "source": [
        "## **Running the engine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxetyyli5YbW"
      },
      "source": [
        "# tweets_unpartitoned=pd.read_csv('data/wnut17test.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_6k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/covid_2K.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity.csv',sep =',',keep_default_na=False)\n",
        "tweets_unpartitoned=pd.read_csv('data/billnye.csv',sep =',',keep_default_na=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mr6dMDO4eFa",
        "outputId": "5917b083-9d8c-4b32-8e6e-7c83d8332412"
      },
      "source": [
        "#tokenizer here is the BERT model's tokenizer\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, tokenizer, alt_model, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Local NER Engine!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqmfgLtn4mZP"
      },
      "source": [
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'wnut17')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'btc')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_3k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_6k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'venezuela')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'covid_2K')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billdeblasio')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'ripcity')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'pikapika')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'roevwade')\n",
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billnye')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvPo33SW4vmz"
      },
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoUZuvJQ42IQ"
      },
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpFVByzb49CG"
      },
      "source": [
        "tweet_to_sentences_w_annotation={}\n",
        "# df_out_holder_Phase1=[]\n",
        "total_time=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1\n",
        "\n",
        "# for g, tweet_batch in tweets_unpartitoned.groupby(np.arange(length) //batch_size):\n",
        "phaseI_timein=time.time()\n",
        "g=0\n",
        "tuple_of= local_NER_Module.extract(tweet_batch,g)\n",
        "phaseI_timeout=time.time()\n",
        "\n",
        "print('local emd time',(phaseI_timeout-phaseI_timein))\n",
        "\n",
        "tweet_base=tuple_of[0]\n",
        "contextual_embeddings=tuple_of[1]\n",
        "candidate_base=tuple_of[2]\n",
        "elapsedTime= tuple_of[4] - tuple_of[3]\n",
        "phase2stopwordList=tuple_of[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=tuple_of[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n",
        "\n",
        "# candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n6sBR1O5DVd"
      },
      "source": [
        "# # # #PHASE I CHECKING:\n",
        "# # processed_tweets=pd.concat(df_out_holder_Phase1,ignore_index=True)\n",
        "# print(len(tweet_base.phase1Candidates.values),local_NER_Module.sentenceID)\n",
        "calculate_f1(tweet_to_sentences_w_annotation, tweet_base.phase1Candidates.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FforYsL5Irr"
      },
      "source": [
        "phaseII_timein=time.time()\n",
        "train_classifier = False\n",
        "candidate_base_post_Phase2, complete_tweet_dataframe,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch,train_classifier)\n",
        "phaseII_timeout=time_out\n",
        "\n",
        "print('global emd time',(phaseII_timeout-phaseII_timein))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}