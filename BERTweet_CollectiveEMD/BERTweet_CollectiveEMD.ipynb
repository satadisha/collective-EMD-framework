{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTweet-CollectiveEMD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wae23yf9PDRb",
        "PvEaaQhsX8aG",
        "GCq9EEmjU1li",
        "-a_6h3vG8UrG",
        "sm1P-HYgU6kb",
        "89dhqxmg5m25",
        "FOGT-F39-YBx",
        "DhUJiCRR-iFG",
        "M4NIDPJU5NOQ",
        "p0eE9I3zuArg",
        "ncj3sC0FkBwA",
        "XY3zs23wC6_3",
        "uRvhsCdn-NxN"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b8cb9e711bd48d09ce31071513bd026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f9d6b0f59314f53b9e3eab07c587635",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6756fac5b0a7459a95bdfa2ad5ee5de0",
              "IPY_MODEL_8ddf77cd445742989eb2494dbec6f25a"
            ]
          }
        },
        "7f9d6b0f59314f53b9e3eab07c587635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6756fac5b0a7459a95bdfa2ad5ee5de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aba23de6a075432786da2c3780442eca",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2543,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2543,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_716f0bc95dca4a6883e7e997b8e4ccda"
          }
        },
        "8ddf77cd445742989eb2494dbec6f25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b8531a137ffe4af9ac4d8b79953495ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7.46k/? [00:00&lt;00:00, 27.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15698eaf06104663aa41322c3cb92e20"
          }
        },
        "aba23de6a075432786da2c3780442eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "716f0bc95dca4a6883e7e997b8e4ccda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8531a137ffe4af9ac4d8b79953495ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15698eaf06104663aa41322c3cb92e20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e47edb34bed24cda9e9d67a8b6250347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e1678a37354f41eea770065f541abcbb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_39e06b8b546341feb241c1065023f8ac",
              "IPY_MODEL_fd148556c6da4df28fabee30f23bc1d3"
            ]
          }
        },
        "e1678a37354f41eea770065f541abcbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39e06b8b546341feb241c1065023f8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_427ad1e763994c589e2cdc1c8576de66",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee77e2d51e1449bda3ebd0a49245d0bd"
          }
        },
        "fd148556c6da4df28fabee30f23bc1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00c6dda96fb54ba2bb44151f78bd9d40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.28k/? [00:01&lt;00:00, 2.51kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c12f4b6f1bf40b4b40a4b46bbe0f952"
          }
        },
        "427ad1e763994c589e2cdc1c8576de66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee77e2d51e1449bda3ebd0a49245d0bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00c6dda96fb54ba2bb44151f78bd9d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c12f4b6f1bf40b4b40a4b46bbe0f952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66e8604a2a8948c0a13ddf6691a20710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_904164cd8c3d463f88e6f6575adbad5c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_56da14bbfa1d445a9684f4ced9ccdb2d",
              "IPY_MODEL_18e3c0410ce746dbb5ffee84fc63165f"
            ]
          }
        },
        "904164cd8c3d463f88e6f6575adbad5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56da14bbfa1d445a9684f4ced9ccdb2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_41ab613ef4ae49e89d921fed3c5aa720",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 185319,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 185319,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dfc0682fc9d449f5b61ae72afe6e036b"
          }
        },
        "18e3c0410ce746dbb5ffee84fc63165f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c2976a5238c94e66ad8abb0a6a003efc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 494k/? [00:00&lt;00:00, 640kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5ccc4874df34900b92037142860ecda"
          }
        },
        "41ab613ef4ae49e89d921fed3c5aa720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dfc0682fc9d449f5b61ae72afe6e036b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2976a5238c94e66ad8abb0a6a003efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5ccc4874df34900b92037142860ecda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8e5f8d7426e4592b21e0b3f11340708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_74dcf7b1d7684d5b8009da28143dd696",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_54879176b0ac436db6bbcf622c043f90",
              "IPY_MODEL_84beb2d526684d3fa422d99bdd33be48"
            ]
          }
        },
        "74dcf7b1d7684d5b8009da28143dd696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54879176b0ac436db6bbcf622c043f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_59397b14c09e43d2983befcf94f0d45e",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 39129,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 39129,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dbb777d438494a8f8bb945bfe04f2d6f"
          }
        },
        "84beb2d526684d3fa422d99bdd33be48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a53a2e21b42400297672d6ad555e3aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 115k/? [00:00&lt;00:00, 292kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_144722fde9814011976faca369e81052"
          }
        },
        "59397b14c09e43d2983befcf94f0d45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dbb777d438494a8f8bb945bfe04f2d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a53a2e21b42400297672d6ad555e3aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "144722fde9814011976faca369e81052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "245681fb5a93461bb259846ca060f3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_31b6bf87af454958a79ee0e32a9ace2a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a9b0b7cdd0cb493cafbc0e8c44061713",
              "IPY_MODEL_29a2c05c304c462aa43ad8fa58317878"
            ]
          }
        },
        "31b6bf87af454958a79ee0e32a9ace2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9b0b7cdd0cb493cafbc0e8c44061713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0f4090f40b14303b810879408b1349c",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 66855,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 66855,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7dd858ddcbd6496faa6ff6e784555d58"
          }
        },
        "29a2c05c304c462aa43ad8fa58317878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2abfdb9706e0432eb312af61dc2aaed4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 192k/? [00:00&lt;00:00, 2.59MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39f63baa36f14eaf943b45b1221cea4c"
          }
        },
        "d0f4090f40b14303b810879408b1349c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7dd858ddcbd6496faa6ff6e784555d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2abfdb9706e0432eb312af61dc2aaed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39f63baa36f14eaf943b45b1221cea4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b9cf11979d043fe8fdbd4033dd22237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ccb2c032758243a1ad8c6d79c2bc5b2d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61013e3b08fb4b1fbae264d4abc2f156",
              "IPY_MODEL_9e5b1f4091354b1585b9511e6961a0c2"
            ]
          }
        },
        "ccb2c032758243a1ad8c6d79c2bc5b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61013e3b08fb4b1fbae264d4abc2f156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50c52ef5dca94b3898167b8d5be51e22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bee4b963df734287bb540ed777ee0376"
          }
        },
        "9e5b1f4091354b1585b9511e6961a0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff04b42c9adc4c0584277db0cb81feae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3394/0 [00:00&lt;00:00, 5764.19 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8acb2b0cbb64306964cc774f4cf8d07"
          }
        },
        "50c52ef5dca94b3898167b8d5be51e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bee4b963df734287bb540ed777ee0376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff04b42c9adc4c0584277db0cb81feae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8acb2b0cbb64306964cc774f4cf8d07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcea5682df834375b18b5260bcd3fc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d2e57ee916e43d08aaa3450a3ca4a42",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4671ba268a7a48fc9c3d99c313466d84",
              "IPY_MODEL_59c741286211469bb34428c5e91e5011"
            ]
          }
        },
        "7d2e57ee916e43d08aaa3450a3ca4a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4671ba268a7a48fc9c3d99c313466d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7e1be016fc134c20b776f58e5fe9e644",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84c28d7fc3114038bb816bc6948fcc54"
          }
        },
        "59c741286211469bb34428c5e91e5011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_41905aa6d89c4f6f9cb3441c63f53586",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1009/0 [00:00&lt;00:00, 4772.69 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f4c31fa59f6d4f01bc9686acc3642266"
          }
        },
        "7e1be016fc134c20b776f58e5fe9e644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84c28d7fc3114038bb816bc6948fcc54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41905aa6d89c4f6f9cb3441c63f53586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f4c31fa59f6d4f01bc9686acc3642266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06bea0d4a0854bd6bd095797fd9896dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8367e6048da84f978f27f0675dd3f201",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9316206ee0e741d5a7fbb23807139f0a",
              "IPY_MODEL_9f1495eb852143f9be76de9062784681"
            ]
          }
        },
        "8367e6048da84f978f27f0675dd3f201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9316206ee0e741d5a7fbb23807139f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f490480f234446d5a1b0c0bcd407b3e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c03c4e9ef2f94c86b6d98c1e469778c5"
          }
        },
        "9f1495eb852143f9be76de9062784681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eba465c9b82643d2bc04c774c3e094c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1287/0 [00:00&lt;00:00, 4875.42 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9328c0fa6f1c4753a7e95a0a777f12f1"
          }
        },
        "f490480f234446d5a1b0c0bcd407b3e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c03c4e9ef2f94c86b6d98c1e469778c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eba465c9b82643d2bc04c774c3e094c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9328c0fa6f1c4753a7e95a0a777f12f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6134950fe99a4b2dac2dc657f3e772c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc477ff4f36d49f9925b2bf307a886d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1a1fb9a81d634e8bb5fd4c6735d6adf1",
              "IPY_MODEL_8b4e6355f7d246af938b6af1d68836ae"
            ]
          }
        },
        "fc477ff4f36d49f9925b2bf307a886d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a1fb9a81d634e8bb5fd4c6735d6adf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca18289603584f08a061e0224e33d39d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 558,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 558,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3309721815c74b418d2614c903a5ea81"
          }
        },
        "8b4e6355f7d246af938b6af1d68836ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_18e2c358cf094df586925298cd939118",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 558/558 [00:01&lt;00:00, 394B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_79fe201e00544514b53a087924e2746f"
          }
        },
        "ca18289603584f08a061e0224e33d39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3309721815c74b418d2614c903a5ea81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18e2c358cf094df586925298cd939118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "79fe201e00544514b53a087924e2746f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62e5a82a794649f98e7c059e3b167598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f48c0c4acd94da8ad9fde442a75360a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6274d0ddf954400693aad3335888e642",
              "IPY_MODEL_c44f9b16ba3c47f3afa13affc4c8c113"
            ]
          }
        },
        "7f48c0c4acd94da8ad9fde442a75360a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6274d0ddf954400693aad3335888e642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9adaf48f47614f39a61b8aea6f8b75c7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 843438,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 843438,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6fa83f95b5e744ae830a061a2757dbe8"
          }
        },
        "c44f9b16ba3c47f3afa13affc4c8c113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8d9c44753f4491cb4d40d883ed0fd9e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 843k/843k [00:00&lt;00:00, 1.06MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0d16af27de546be813d9f87209ab341"
          }
        },
        "9adaf48f47614f39a61b8aea6f8b75c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6fa83f95b5e744ae830a061a2757dbe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8d9c44753f4491cb4d40d883ed0fd9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0d16af27de546be813d9f87209ab341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a75deac1ef694602a7ffbdffcfc152aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_959bfdab0532404abad407acd2e21715",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7b4eb8522f3453d87b2c8e8ffe87a38",
              "IPY_MODEL_4a8527f4c84f48debf1362931f28ba25"
            ]
          }
        },
        "959bfdab0532404abad407acd2e21715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7b4eb8522f3453d87b2c8e8ffe87a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e29885c6fd048178bc68950ce6f10fc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1078931,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1078931,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a417da409e1c4eac8ca2be24877a8c5d"
          }
        },
        "4a8527f4c84f48debf1362931f28ba25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8db6e6532b3848909c713eb1a90d4ad6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.08M/1.08M [00:01&lt;00:00, 771kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a112cd0ae0bc4868a120cf8fb3dcfaa7"
          }
        },
        "0e29885c6fd048178bc68950ce6f10fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a417da409e1c4eac8ca2be24877a8c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8db6e6532b3848909c713eb1a90d4ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a112cd0ae0bc4868a120cf8fb3dcfaa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1d4e03f81da497aa24c340a2176b706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f79a375dae44a97a18ffef2837fcad7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b5548a69854845fba6780fae5b34d2a2",
              "IPY_MODEL_dabd6b1e60254121a47c8c7c2c8b69f3"
            ]
          }
        },
        "5f79a375dae44a97a18ffef2837fcad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5548a69854845fba6780fae5b34d2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1cafb9475984b028c08dea02507e255",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3394,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3394,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b9f3d6752b84d6d84ed3cc9782f7bc1"
          }
        },
        "dabd6b1e60254121a47c8c7c2c8b69f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b99043a51644581a63b445a9850b5fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3394/3394 [03:59&lt;00:00, 14.17ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1200e811a10141199e3f8f1aaf364907"
          }
        },
        "c1cafb9475984b028c08dea02507e255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b9f3d6752b84d6d84ed3cc9782f7bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b99043a51644581a63b445a9850b5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1200e811a10141199e3f8f1aaf364907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b9f4d6ad11b48faa5543002325c17eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_676ab4ba772542778586f6bb5d3f6c01",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49019872401d4f3f9a90890358d4d945",
              "IPY_MODEL_abb48207ce294a83a93c409c04d88d6e"
            ]
          }
        },
        "676ab4ba772542778586f6bb5d3f6c01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49019872401d4f3f9a90890358d4d945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e1239e2885cf4faf98854ef8ed160ff4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1009,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1009,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a3d2f9736084aa69316b5514c3e2262"
          }
        },
        "abb48207ce294a83a93c409c04d88d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8248ef241fb94a91bcc261702b2fee3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1009/1009 [03:46&lt;00:00,  4.46ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b955099508a4a5c89e2a411818fcbdc"
          }
        },
        "e1239e2885cf4faf98854ef8ed160ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a3d2f9736084aa69316b5514c3e2262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8248ef241fb94a91bcc261702b2fee3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b955099508a4a5c89e2a411818fcbdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e666a1f3ac9e49f88be2ef031ec84ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ca8d17e4379449d58b7ab7078d5ea309",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f7e541b5317d40d6b4d147ebd53275a8",
              "IPY_MODEL_839fb3fb0eb64a0baec651761f71db5e"
            ]
          }
        },
        "ca8d17e4379449d58b7ab7078d5ea309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7e541b5317d40d6b4d147ebd53275a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3be07279769b4633adf2400e985947fc",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1287,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1287,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a66bd5681f64cd38f55bbdb426b9ef9"
          }
        },
        "839fb3fb0eb64a0baec651761f71db5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03d3f9d608ef4a6cae62c0a5a320d670",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1287/1287 [03:41&lt;00:00,  5.80ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b64a5022a1be4913a80495c56750e294"
          }
        },
        "3be07279769b4633adf2400e985947fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a66bd5681f64cd38f55bbdb426b9ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03d3f9d608ef4a6cae62c0a5a320d670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b64a5022a1be4913a80495c56750e294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37d06c5e641046cca0966a07035386f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e7318249c6f414195721175167a1c31",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_628a27031f9e404ca85f4a12cf62bcac",
              "IPY_MODEL_d27ebdbbf25044d2b2584039878566cf"
            ]
          }
        },
        "1e7318249c6f414195721175167a1c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "628a27031f9e404ca85f4a12cf62bcac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3420f7e0e672402fb283072ce3235cad",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2482,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2482,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66d83a49356f4dbeb01e023e95730996"
          }
        },
        "d27ebdbbf25044d2b2584039878566cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a556959fc2b47c0a315968953adafaf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6.34k/? [00:00&lt;00:00, 26.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_807eb17c6896419a806e4473583a3ddf"
          }
        },
        "3420f7e0e672402fb283072ce3235cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66d83a49356f4dbeb01e023e95730996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a556959fc2b47c0a315968953adafaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "807eb17c6896419a806e4473583a3ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wae23yf9PDRb"
      },
      "source": [
        "## **Initial imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrLFVF0oiBlh",
        "outputId": "dba56856-6d81-4384-f2c8-f104b46aa87a"
      },
      "source": [
        "!python3 --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO30EXrb_pcx"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/BERTweet-ner')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3WDvVQhayIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed679de0-c9b1-41f9-9276-7f74a5743352"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "%cd gdrive/My Drive/BERTweet-ner"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/BERTweet-ner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eS5AgF0BMz2"
      },
      "source": [
        "\n",
        "# sys.path.insert(0,'/content/gdrive/My Drive/BERTweet-ner')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEvo5aXwYILN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332ee4da-cdb4-49b6-c12d-38c8377409e0"
      },
      "source": [
        "!pip3 install datasets\n",
        "!pip3 install transformers\n",
        "!pip3 install -U sentence-transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/27/9c91ddee87b06d2de12f134c5171a49890427e398389f07f6463485723c3/datasets-1.9.0-py3-none-any.whl (262kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec>=2021.05.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/e1/7111d8afc76ee3171f4f99592cd29bac9d233ae1aa34623011506f955434/fsspec-2021.7.0-py3-none-any.whl (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/d5/07894f2f047055576c3559a66d921c37d1280fda74b08fe574a8490e5999/huggingface_hub-0.0.14-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, xxhash, huggingface-hub, datasets\n",
            "Successfully installed datasets-1.9.0 fsspec-2021.7.0 huggingface-hub-0.0.14 xxhash-2.0.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 44.6MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "  Found existing installation: huggingface-hub 0.0.14\n",
            "    Uninstalling huggingface-hub-0.0.14:\n",
            "      Successfully uninstalled huggingface-hub-0.0.14\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/fd/8a81047bbd9fa134a3f27e12937d2a487bd49d353a038916a5d7ed4e5543/sentence-transformers-2.0.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-cp37-none-any.whl size=126711 sha256=f69cada23781434a4226f9ab35d375a5f4bee2f8c78b2617b0db8e7045454320\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/d2/98/d191289a877a34c68aa67e05179521e060f96394a3e9336be6\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.0.0 sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuR-p1-pMK2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa697517-ba9a-4807-8b8b-a4b39cc98a5e"
      },
      "source": [
        "!pip3 install emoji"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 102kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 112kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 122kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUz5eC7SfFqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a74aebd-9301-4ea1-fb6b-e49d3bcb2307"
      },
      "source": [
        "!pip3 install seqeval"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=8558222f0c3e27782a27dcbfbeb4bc6a6792555c1e15fea1fb75b821076ef61e\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhAmsA61gP6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d2e7ef-67e5-4ca6-d8f8-55907b3d999e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f1128ace150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvEaaQhsX8aG"
      },
      "source": [
        "## **Initial Trainer with Default Config (NOT REQUIRED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NDFUYBfCEwG"
      },
      "source": [
        "# labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
        "# metric.compute(predictions=[labels], references=[labels])\n",
        "\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=3) #Just BIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abGq0PXk040"
      },
      "source": [
        "# args = TrainingArguments(\n",
        "#     f\"test-{task}\",\n",
        "#     evaluation_strategy = \"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     per_device_eval_batch_size=batch_size,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p7ArnENIfTE"
      },
      "source": [
        "# trainer = Trainer(\n",
        "#     model,\n",
        "#     args,\n",
        "#     train_dataset=tokenized_datasets[\"train\"],\n",
        "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
        "#     data_collator=data_collator,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5YzgeKIlzj"
      },
      "source": [
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hscli_TYIq1c"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCq9EEmjU1li"
      },
      "source": [
        "## **Predict on validation/test (NOT REQUIRED TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTyESmmTIzzM"
      },
      "source": [
        "# predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "# predictions = np.argmax(predictions, axis=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYKXZlTesZAI"
      },
      "source": [
        "model_returns, labels, _ = alt_trainer.predict(tokenized_datasets[\"test\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqXzfKOmvT8Y"
      },
      "source": [
        "predictions, _ = model_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th4654zaJCNA"
      },
      "source": [
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1emxCA5lDcg"
      },
      "source": [
        "predictions\n",
        "print(len(predictions[0]))\n",
        "print(len(predictions[5]))\n",
        "print(len(predictions[50]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5CPzI0skwbH"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzbUNafHt3kj"
      },
      "source": [
        "# print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8aiYWi6B5X6"
      },
      "source": [
        "# # print(predictions[:5])\n",
        "# # Remove ignored index (special tokens)\n",
        "# true_predictions = [\n",
        "#     [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#     for prediction, label in zip(predictions, labels)\n",
        "# ]\n",
        "\n",
        "# true_labels = [\n",
        "#     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "#     for prediction, label in zip(predictions, labels)\n",
        "# ]\n",
        "\n",
        "# print(true_predictions[:5])\n",
        "# print(true_labels[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wktgUaW8zVl"
      },
      "source": [
        "# results = metric.compute(predictions=true_predictions, references=true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmCDlyQsoLJY"
      },
      "source": [
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a_6h3vG8UrG"
      },
      "source": [
        "## **Setting some global stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByJwnUdZ-YGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc38dfd-acfd-42c2-874a-76a5c8a0ada7"
      },
      "source": [
        "import torch\n",
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qob4MoKoXgs"
      },
      "source": [
        "def collate_token_labels(token_dict, prediction_labels):\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    for key in token_dict.keys():\n",
        "        vals=token_dict[key]\n",
        "        labels=prediction_labels[counter:counter+len(vals)]\n",
        "        if('I' in labels):\n",
        "            collated_labels.append('I')\n",
        "        elif('B' in labels):\n",
        "            collated_labels.append('B')\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "        counter+=len(vals)\n",
        "    return collated_labels"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg6Nz2W9lp5m"
      },
      "source": [
        "# def collate_token_label_embedding(token_dict, prediction_labels, entity_embeddings):\n",
        "#     counter=0\n",
        "#     collated_labels=[]\n",
        "#     collated_entity_embeddings=[]\n",
        "#     for key in token_dict.keys():\n",
        "#         vals=token_dict[key]\n",
        "#         labels=prediction_labels[counter:counter+len(vals)]\n",
        "#         token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "# #         print(token_entity_embeddings.shape)\n",
        "#         mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "#         mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "#         collated_entity_embeddings.append(mean_tensor)\n",
        "# #         print(collated_entity_embeddings)\n",
        "#         if('I' in labels):\n",
        "#             collated_labels.append('I')\n",
        "#         elif('B' in labels):\n",
        "#             collated_labels.append('B')\n",
        "#         else:\n",
        "#             collated_labels.append('O')\n",
        "#         counter+=len(vals)\n",
        "#     assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "#     return collated_labels,collated_entity_embeddings\n",
        "\n",
        "def collate_token_label_embedding(tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "    counter=0\n",
        "    collated_labels=[]\n",
        "    collated_entity_embeddings=[]\n",
        "    for word in tweetWordList:\n",
        "        vals=token_dict[word]\n",
        "        # print(word,vals)\n",
        "        if(counter<len(prediction_labels)):\n",
        "            labels=prediction_labels[counter:counter+len(vals)]\n",
        "            token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "    #         print(token_entity_embeddings.shape)\n",
        "            mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "            mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "            collated_entity_embeddings.append(mean_tensor)\n",
        "    #         print(collated_entity_embeddings)\n",
        "            if('I' in labels):\n",
        "                collated_labels.append('I')\n",
        "            elif('B' in labels):\n",
        "                collated_labels.append('B')\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "            counter+=len(vals)\n",
        "        else:\n",
        "            collated_labels.append('O')\n",
        "            collated_entity_embeddings.append(torch.zeros(768).to(device))\n",
        "    assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "    return collated_labels,collated_entity_embeddings"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC69pRJD87ly"
      },
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "class PhraseEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size,output_size,device):\n",
        "        super(PhraseEmbedding, self).__init__()\n",
        "        self.print_once=True\n",
        "        self.dense_layer = nn.Linear(input_size,output_size)\n",
        "        self.non_linear_layer = nn.Tanh()\n",
        "        self.cosine_layer = nn.CosineSimilarity(dim=0)\n",
        "        self.device = device\n",
        "        return\n",
        "\n",
        "    def encode(self, input_embedding):\n",
        "\n",
        "        # print(input_embedding.size())\n",
        "        input_sentence_embedding = input_embedding.squeeze(0)\n",
        "        # print(input_sentence_embedding.size())\n",
        "        # print('-----')\n",
        "\n",
        "        # Max Pool\n",
        "        # max_pooled_embedding = torch.max(input_sentence_embedding,dim=0)\n",
        "\n",
        "        #Average Pool\n",
        "        average_pooled_embedding = torch.mean(input_sentence_embedding,dim=0).to(device=self.device)\n",
        "        # print(average_pooled_embedding.size())\n",
        "\n",
        "        x = self.dense_layer(average_pooled_embedding)\n",
        "        # print(x.size())\n",
        "\n",
        "        out = self.non_linear_layer(x)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        # print(len(input_tuple))\n",
        "        input_source = input_tuple[0]\n",
        "        input_target = input_tuple[1]\n",
        "\n",
        "        output_source = self.encode(input_source)\n",
        "        output_target = self.encode(input_target)\n",
        "\n",
        "        similarity = self.cosine_layer(output_source, output_target)\n",
        "        # print(similarity)\n",
        "        return similarity\n",
        "\n",
        "    def getEmbedding(self, input_embeddings):\n",
        "        with torch.no_grad():\n",
        "            return self.encode(input_embeddings)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm1P-HYgU6kb"
      },
      "source": [
        "## **Helper Functions: Pre-processing on custom test set (NOT REQD TO RUN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgRPt985oiDa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safU-gd3oaRx"
      },
      "source": [
        "import re\n",
        "import emoji\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "import string\n",
        "\n",
        "string.punctuation=string.punctuation+'…‘’'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgl7fxwtSnHO"
      },
      "source": [
        "def get_entities(word_tag_tuples):\n",
        "    \n",
        "    mentions=[]\n",
        "    candidateMention=''\n",
        "    #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "    for tup in word_tag_tuples:\n",
        "        candidate=tup[0]\n",
        "        tag=tup[1]\n",
        "        if(tag=='O'):\n",
        "            if(candidateMention):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append(mention_to_add)\n",
        "            candidateMention=''\n",
        "        else:\n",
        "            if (tag=='B'):\n",
        "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                    if mention_to_add.endswith(\"'s\"):\n",
        "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    elif mention_to_add.endswith(\"’s\"):\n",
        "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                        mention_to_add=''.join(li)\n",
        "                    else:\n",
        "                        mention_to_add=mention_to_add\n",
        "                    if(mention_to_add!=''):\n",
        "                        mentions.append(mention_to_add)\n",
        "                candidateMention=candidate\n",
        "            else:\n",
        "                candidateMention+=\" \"+candidate\n",
        "        # if (tag=='B'):\n",
        "        #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "        #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "        #         if(mention_to_add):\n",
        "        #             mentions.append(mention_to_add)\n",
        "        #     candidateMention=candidate\n",
        "        # else:\n",
        "        #     candidateMention+=\" \"+candidate\n",
        "    if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "        if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "            mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            if(mention_to_add!=''):\n",
        "                mentions.append(mention_to_add)\n",
        "        # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "    # print('extracted mentions:', mentions)\n",
        "    return mentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YqhxNBPM67V"
      },
      "source": [
        "def get_encoding_seq(tweet_word_list, mentions):\n",
        "    print(tweet_word_list)\n",
        "    print(mentions)\n",
        "    tweet_word_index=0\n",
        "    encoded_tag_sequence=[]\n",
        "    while(mentions):\n",
        "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
        "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
        "            encoded_tag_sequence.append('O')\n",
        "            tweet_word_index+=1\n",
        "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
        "            for token_index, token in enumerate(current_mention):\n",
        "                if(token_index==0):\n",
        "                    encoded_tag_sequence.append('B')\n",
        "                else:\n",
        "                    encoded_tag_sequence.append('I')\n",
        "                tweet_word_index+=1\n",
        "    while(tweet_word_index<len(tweet_word_list)):\n",
        "        encoded_tag_sequence.append('O')\n",
        "        tweet_word_index+=1\n",
        "        \n",
        "    print(encoded_tag_sequence)\n",
        "    return encoded_tag_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4dCD4GDmgnM"
      },
      "source": [
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "def normalize_to_sentences(text):\n",
        "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "    tweetSentenceList_inter=custom_flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "    return tweetSentenceList\n",
        "\n",
        "def custom_flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "    if (mylist !=[]):\n",
        "        for item in mylist:\n",
        "            #print not isinstance(item, ne.NE_candidate)\n",
        "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                custom_flatten(item, outlist)\n",
        "            else:\n",
        "                item=item.strip(' \\t\\n\\r')\n",
        "                outlist.append(item)\n",
        "    return outlist\n",
        "\n",
        "def preprocess(filename):\n",
        "    \"\"\"save a file with token, label and prediction in each row\"\"\"\n",
        "    tweet_to_sentences_w_annotation={}\n",
        "    sentenceID=0\n",
        "    test=pd.read_csv(\"data/\"+filename,sep =',',keep_default_na=False)\n",
        "    # outputfilename=\"data/covid/covid_2K.txt\"\n",
        "    \n",
        "    all_annotated_ne=[]\n",
        "    tweetsentences=[]\n",
        "    tokenizedsentences=[]\n",
        "    \n",
        "    for row in test.itertuples():\n",
        "        tweetID=str(row.Index)\n",
        "        text=str(row.TweetText)\n",
        "        row_sentences = normalize_to_sentences(text)\n",
        "        tweetsentences += row_sentences\n",
        "        tokenizedsentences += [tokenizer(sentence, is_split_into_words=True) for sentence in row_sentences]\n",
        "        # print(text)\n",
        "        \n",
        "        mentions=[]\n",
        "        for sentence_level in str(row.mentions_other).split(';'):\n",
        "            if(sentence_level):\n",
        "                for mention in sentence_level.split(','):\n",
        "                    if(mention):\n",
        "                        mentions.append(mention.lower().strip(string.punctuation).strip())\n",
        "        mentions=list(filter(lambda element: ((element !='')&(element !='nan')), mentions))\n",
        "        # all_annotated_ne.extend(mentions)\n",
        "        \n",
        "        # if(row_sentences):\n",
        "        tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+len(row_sentences)),mentions)\n",
        "        sentenceID+=len(row_sentences)\n",
        "        # else:\n",
        "        #     tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+1),mentions)\n",
        "        #     sentenceID+=1\n",
        "        # print(sentenceID,len(row_sentences))\n",
        "    return tweetsentences, tokenizedsentences, tweet_to_sentences_w_annotation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89dhqxmg5m25"
      },
      "source": [
        "## **Helper Functions: Evaluation on custom test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep9TRivf7R9-"
      },
      "source": [
        "def calculate_f1(tweet_to_sentences_w_annotation, ner_arrays):\n",
        "    \n",
        "    # dataset, i = [], 0\n",
        "    file_write_text=''\n",
        "    all_detected_ne=[]\n",
        "    all_annotated_ne=[]\n",
        "    \n",
        "    true_positive_count=0\n",
        "    false_positive_count=0\n",
        "    false_negative_count=0\n",
        "    total_mentions=0\n",
        "    total_annotation=0\n",
        "    \n",
        "    for tweetID in tweet_to_sentences_w_annotation.keys():\n",
        "        unrecovered_annotated_mention_list=[]\n",
        "        tp_counter_inner=0\n",
        "        fp_counter_inner=0\n",
        "        fn_counter_inner=0\n",
        "        \n",
        "        annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
        "        all_annotated_ne.extend(annotated_mention_list)\n",
        "        output_mentions_list=[]\n",
        "        idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
        "        for sentID in range(idRange[0],idRange[1]):\n",
        "            output_mentions_list+=ner_arrays[sentID]\n",
        "        all_detected_ne+=output_mentions_list\n",
        "        print(tweetID,annotated_mention_list,output_mentions_list)\n",
        "        all_postitive_counter_inner=len(output_mentions_list)\n",
        "        while(annotated_mention_list):\n",
        "            if(len(output_mentions_list)):\n",
        "                annotated_candidate= annotated_mention_list.pop()\n",
        "                if(annotated_candidate in output_mentions_list):\n",
        "                    output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                    tp_counter_inner+=1\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "            else:\n",
        "                unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                break\n",
        "        # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "        fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "        fp_counter_inner=all_postitive_counter_inner - tp_counter_inner\n",
        "        \n",
        "        print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "        \n",
        "        true_positive_count+=tp_counter_inner\n",
        "        false_positive_count+=fp_counter_inner\n",
        "        false_negative_count+=fn_counter_inner\n",
        "        \n",
        "    print('true_positive_count,false_positive_count,false_negative_count:')\n",
        "    print(true_positive_count,false_positive_count,false_negative_count)\n",
        "    \n",
        "    precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "    recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "    f_measure=2*(precision*recall)/(precision+recall)\n",
        "            \n",
        "    print('========Entity Mention Detection========')\n",
        "    print('precision: ',precision)\n",
        "    print('recall: ',recall)\n",
        "    print('f_measure: ',f_measure)\n",
        "\n",
        "    print('========Entity Detection========')\n",
        "    true_positive_entities =  len(list(set(all_detected_ne).intersection(set(all_annotated_ne))))\n",
        "    false_positive_entities = len(list(set(all_annotated_ne)-set(all_detected_ne)))\n",
        "    false_negative_entities = len(list(set(all_detected_ne)-set(all_annotated_ne)))\n",
        "\n",
        "    precision= (true_positive_entities)/(true_positive_entities+false_positive_entities)\n",
        "    recall= (true_positive_entities)/(true_positive_entities+false_negative_entities)\n",
        "    f_measure = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "    print('precision: ',precision)\n",
        "    print('recall: ',recall)\n",
        "    print('f_measure: ',f_measure)   \n",
        "\n",
        "    return"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN17vi2QJh9Q"
      },
      "source": [
        "## **Initialization of some components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5rokKC-JgOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6efa35b7-a432-4b57-b1d4-18d760cde6d8"
      },
      "source": [
        "from os import path\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "# Initialize network\n",
        "output_embedding_size = 300\n",
        "# output_embedding_size = 768\n",
        "phraseEmbeddingModel = PhraseEmbedding(768, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# define checkpoint saved path\n",
        "# ckp_path = \"entityEmbedding/model_checkpoints/checkpoint.pt\" #768\n",
        "ckp_path = \"entityEmbedding/model_checkpoints/checkpoint_model300.pt\" #300\n",
        "\n",
        "if(path.exists(ckp_path)):\n",
        "    # load the saved checkpoint\n",
        "    entityPhraseEmbedder, optimizer, start_epoch = load_ckp(ckp_path, phraseEmbeddingModel, optimizer)\n",
        "\n",
        "    print(\"starting with model at epoch:\", start_epoch)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting with model at epoch: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUMEUhlS2N47",
        "outputId": "89e17467-a967-4ebe-b223-653721d9940e"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhlviRvO7DDE"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "gutenberg_text = \"\"\n",
        "for file_id in gutenberg.fileids():\n",
        "    gutenberg_text += gutenberg.raw(file_id)\n",
        "tokenizer_trainer = PunktTrainer()\n",
        "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "sentence_tokenizer._params.abbrev_types.add('u.s.a')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbajzPkDC_nO"
      },
      "source": [
        "# tweets_unpartitoned=pd.read_csv('data/wnut17test.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_3k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/tweets_6k_annotated.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/venezuela.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/covid_2K.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "tweets_unpartitoned=pd.read_csv('data/broad_twitter_corpus.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "# tweets_unpartitoned=pd.read_csv('data/billdeblasio.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/roevwade.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/pikapika.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/ripcity.csv',sep =',',keep_default_na=False)\n",
        "# tweets_unpartitoned=pd.read_csv('data/billnye.csv',sep =',',keep_default_na=False)\n",
        "\n",
        "#to train the Entity Classifiers\n",
        "# tweets_unpartitoned=pd.read_csv('data/deduplicated_test.csv',sep =';',keep_default_na=False)\n",
        "\n",
        "\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('tweets_3k_annotated.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('venezuela.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billdeblasio.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('pikapika.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('ripcity.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billnye.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('roevwade.csv')\n",
        "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('wnut17test.csv')"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOGT-F39-YBx"
      },
      "source": [
        "## **Entity Classifier I**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-F1fd56KK7N"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,12)\n",
        "    self.linear2 = nn.Linear(12,input_size)\n",
        "    self.linear3 = nn.Linear(input_size,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifierI():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # separately using only syntactic features\n",
        "        self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']\n",
        "\n",
        "        self.relevant_columns = ['normalized_length','normalized_cap',\n",
        "            'normalized_substring-cap',\n",
        "            'normalized_s-o-sCap',\n",
        "            'normalized_all-cap',\n",
        "            'normalized_non-cap',\n",
        "            'normalized_non-discriminative']\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 200\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/checkpoint.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifierI_checkpoint_model300.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/checkpoint.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierI_checkpoint_model.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                # if(combined_validation_loss<best_loss):\n",
        "                if(f1>best_f1):\n",
        "                    # best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhUJiCRR-iFG"
      },
      "source": [
        "## **Entity Classifier II**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX66sYfCKMHT"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,500)\n",
        "    self.linear2 = nn.Linear(500,200)\n",
        "    self.linear3 = nn.Linear(200,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifierII():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # separately using only semantic features\n",
        "        # self.combined_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        self.combined_feature_list=['length']+['cf_'+str(i) for i in range(300)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length']+['normalized_cf_'+str(i) for i in range(300)]\n",
        "        # self.relevant_columns = ['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.00001, weight_decay=0.00001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 1000\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint768.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300.pt\" #300\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/classifierII_checkpoint_model300_3K.pt\" #300\n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/classifierII_checkpoint768.pt' #768\n",
        "        f_path = checkpoint_dir + '/classifierII_checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                if((tp+fp)==0):\n",
        "                    precision = 0\n",
        "                else:\n",
        "                    precision = tp/(tp+fp)\n",
        "\n",
        "                if((tp+fn)==0):\n",
        "                    recall = 0\n",
        "                else:\n",
        "                    recall = tp/(tp+fn)\n",
        "\n",
        "                if((precision + recall)==0):\n",
        "                    f1 = 0\n",
        "                else:\n",
        "                    f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                if(combined_validation_loss<best_loss):\n",
        "                # if(f1>best_f1):\n",
        "                    best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4NIDPJU5NOQ"
      },
      "source": [
        "## **Entity Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3oTVBI5PWL"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 2 output_classes: 'entity'/'non-entity'; so sigmoid transformation would suffice\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,500)\n",
        "    self.linear2 = nn.Linear(500,200)\n",
        "    self.linear3 = nn.Linear(200,1)\n",
        "    self.sigmoid_layer = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    out = self.sigmoid_layer(x)\n",
        "    return out\n",
        "\n",
        "class EntityClassifier():\n",
        "\n",
        "    def __init__(self,training_file, to_train, device):\n",
        "\n",
        "        # self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(768)]\n",
        "        self.combined_feature_list=['length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+['cf_'+str(i) for i in range(300)]\n",
        "\n",
        "        self.relevant_columns = ['normalized_length',\n",
        "            'normalized_cap',\n",
        "            'normalized_substring-cap',\n",
        "            'normalized_s-o-sCap',\n",
        "            'normalized_all-cap',\n",
        "            'normalized_non-cap',\n",
        "            'normalized_non-discriminative'\n",
        "            ]\n",
        "            +['normalized_cf_'+str(i) for i in range(300)]\n",
        "            # +['normalized_cf_'+str(i) for i in range(768)]\n",
        "        \n",
        "        #initialize the classifier model\n",
        "        self.classifier = NN(len(self.relevant_columns)).to(device)\n",
        "        #Loss and Optimizer\n",
        "        self.ec_criterion = nn.BCELoss(reduction='mean' )\n",
        "        self.ec_optimizer = optim.Adam(self.classifier.parameters(), lr = 0.00001, weight_decay=0.0001)\n",
        "        self.ec_batch_size = 32\n",
        "        self.ec_num_epochs = 200\n",
        "        self.patience = 20\n",
        "\n",
        "\n",
        "        if(to_train):\n",
        "\n",
        "            self.train = pd.read_csv(training_file,delimiter=\",\",sep='\\s*,\\s*')\n",
        "            #pre-processing : this completes the global average pooling\n",
        "            \n",
        "            max_length=self.train['length'].max()\n",
        "            self.train['normalized_length']= self.train['length']/max_length\n",
        "            for column in self.combined_feature_list[1:]:\n",
        "                self.train['normalized_'+column]=self.train[column]/self.train['cumulative']\n",
        "            \n",
        "            #Loading the data\n",
        "            training_inputs_array = self.train[self.relevant_columns].to_numpy()\n",
        "            training_targets_array = self.train['class'].astype(float).to_numpy()\n",
        "\n",
        "            training_inputs = torch.from_numpy(training_inputs_array).type(torch.float)\n",
        "            training_targets = torch.from_numpy(training_targets_array).type(torch.float)\n",
        "\n",
        "            print('Input Shape: ', training_inputs.shape)\n",
        "            print('Output Shape: ', training_targets.shape)\n",
        "\n",
        "            dataset = TensorDataset(training_inputs, training_targets)\n",
        "\n",
        "            train=int(math.ceil(len(training_inputs_array)*0.8))\n",
        "            val=len(training_inputs_array)-train\n",
        "\n",
        "            train_ds, val_ds = random_split(dataset, [train, val])\n",
        "\n",
        "            self.train_loader = DataLoader(train_ds, self.ec_batch_size, shuffle=True)\n",
        "            self.val_loader = DataLoader(val_ds, val) #will execute in 1 batch\n",
        "\n",
        "            #Training the model\n",
        "            end_epoch = self.fit()\n",
        "\n",
        "            # #Saving the model\n",
        "            # self.checkpoint = {\n",
        "            #             'epoch': end_epoch + 1,\n",
        "            #             'state_dict': self.classifier.state_dict(),\n",
        "            #             'optimizer': self.ec_optimizer.state_dict()\n",
        "            #         }\n",
        "\n",
        "            checkpoint_dir = \"entityClassifier/model_checkpoints\"\n",
        "            self.save_ckp(self.checkpoint, True, checkpoint_dir)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            # define checkpoint saved path\n",
        "            # ckp_path = \"entityClassifier/model_checkpoints/checkpoint.pt\" #768\n",
        "            ckp_path = \"entityClassifier/model_checkpoints/checkpoint_model300.pt\" #300\n",
        "            \n",
        "\n",
        "            # load the saved checkpoint\n",
        "            self.classifier, self.ec_optimizer, self.start_epoch = self.load_ckp(ckp_path, self.classifier, self.ec_optimizer)\n",
        "\n",
        "    def load_ckp(self, checkpoint_fpath, model, optimizer):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "    def save_ckp(self,state, is_best, checkpoint_dir):\n",
        "        # f_path = checkpoint_dir + '/checkpoint.pt' #768\n",
        "        f_path = checkpoint_dir + '/checkpoint_model300.pt' #300\n",
        "        torch.save(state, f_path)\n",
        "\n",
        "    def fit(self):\n",
        "        # Train Network\n",
        "        history_validation = []\n",
        "        history_training= []\n",
        "        no_improvement_counter=0\n",
        "        best_loss = np.float('inf')\n",
        "        best_f1 = np.float('-inf')\n",
        "        for epoch in range(self.ec_num_epochs):\n",
        "            training_batch_loss=[]\n",
        "            for batch_idx, (data, targets) in enumerate(self.train_loader):\n",
        "                # Get data to cuda if possible\n",
        "                data = data.to(device=device)\n",
        "                targets = targets.unsqueeze(1).to(device=device)\n",
        "\n",
        "                # forwards\n",
        "                out = self.classifier(data)\n",
        "\n",
        "                # print('checking shapes:')\n",
        "                # print(out.shape)\n",
        "                # print(targets.shape)\n",
        "\n",
        "                loss = self.ec_criterion(out, targets)\n",
        "                training_batch_loss.append(loss.item())\n",
        "                # print(loss.item())\n",
        "\n",
        "                # backward\n",
        "                self.ec_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), 1.0)\n",
        "                # gradient descent or adam step\n",
        "                self.ec_optimizer.step()\n",
        "            combined_training_loss = np.mean(training_batch_loss)\n",
        "            history_training.append(combined_training_loss)\n",
        "\n",
        "            #Validation: DO NOT BACKPROPAGATE HERE\n",
        "            validation_batch_loss = []\n",
        "            labels = []\n",
        "            prediction = []\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, (val_data, val_targets) in enumerate(self.val_loader):\n",
        "                    val_data = val_data.to(device=device)\n",
        "                    val_targets = val_targets.unsqueeze(1).to(device=device)\n",
        "                    out = self.classifier(val_data)\n",
        "\n",
        "                    # print('checking shapes:')\n",
        "                    # print(out.shape)\n",
        "                    # print(val_targets.shape)\n",
        "                    prediction+=out.reshape(-1).tolist()\n",
        "                    labels+=val_targets\n",
        "\n",
        "                    # loss = F.mse_loss(out, val_targets) round\n",
        "                    loss = self.ec_criterion(out, val_targets)\n",
        "                    validation_batch_loss.append(loss.item())\n",
        "                    # print(validation_batch_loss)\n",
        "                combined_validation_loss= np.mean(validation_batch_loss)\n",
        "\n",
        "                class_prediction = [round(elem) for elem in prediction]\n",
        "                assert len(class_prediction)==len(labels)\n",
        "                tp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==1)&(elem==1))])\n",
        "                fp = len([elem for idx, elem in enumerate(class_prediction) if((labels[idx]==0)&(elem==1))])\n",
        "                fn = len([elem for idx, elem in enumerate(labels) if((elem==1)&(class_prediction[idx]==0))])\n",
        "\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1 = 2*precision*recall/(precision + recall)\n",
        "\n",
        "                history_validation.append(combined_validation_loss)\n",
        "                \n",
        "                print('Epoch',str(epoch+1),':',combined_validation_loss)\n",
        "                print(len(class_prediction),len(labels))\n",
        "                print('precision:',precision,'recall:',recall,'f1:',f1)\n",
        "                if(((epoch+1)%10==0)|(epoch == (self.ec_num_epochs-1))):\n",
        "                    print('=========')\n",
        "                # if(combined_validation_loss<best_loss):\n",
        "                if(f1>best_f1):\n",
        "                    # best_loss = combined_validation_loss\n",
        "                    best_f1 = f1\n",
        "                    print('making this the checkpoint to save')\n",
        "                    #Saving the model\n",
        "                    self.checkpoint = {\n",
        "                                'epoch': epoch + 1,\n",
        "                                'state_dict': self.classifier.state_dict(),\n",
        "                                'optimizer': self.ec_optimizer.state_dict()\n",
        "                            }\n",
        "                    no_improvement_counter=0\n",
        "                else:\n",
        "                    no_improvement_counter+=1\n",
        "                    if(no_improvement_counter>self.patience):\n",
        "                        break\n",
        "\n",
        "        return epoch\n",
        "\n",
        "    def run(self,candidateBase):\n",
        "\n",
        "        candidateBase['probability']=-1\n",
        "        max_length=candidateBase['length'].max()\n",
        "        candidateBase['normalized_length']= candidateBase['length']/max_length\n",
        "        for column in self.combined_feature_list[1:]:\n",
        "            candidateBase['normalized_'+column]=candidateBase[column]/candidateBase['cumulative']\n",
        "\n",
        "        test_inputs_array = candidateBase[self.relevant_columns].to_numpy()\n",
        "        test_targets_array = candidateBase['probability'].to_numpy()\n",
        "\n",
        "        test_inputs = torch.from_numpy(test_inputs_array).type(torch.float)\n",
        "        test_targets = torch.from_numpy(test_targets_array).type(torch.float)\n",
        "\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, len(test_dataset)) #will execute in 1 batch\n",
        "\n",
        "        #Testing\n",
        "        prediction=[]\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                data = data.to(device=device)\n",
        "                # targets = targets.to(device=device)\n",
        "                out = self.classifier(data)\n",
        "                print(out.shape)\n",
        "                prediction=out.reshape(-1)\n",
        "                print(prediction.shape)\n",
        "\n",
        "        candidateBase['probability'] = prediction.tolist()\n",
        "        print(candidateBase['probability'].min(), candidateBase['probability'].max())\n",
        "        return candidateBase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0eE9I3zuArg"
      },
      "source": [
        "## **Phase I: Local NER to collect entity candidates and Token Contextual Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf9KEE5H5xAd"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import emoji\n",
        "from emoji import demojize\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas  as pd\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "#from datasketch import MinHash, MinHashLSH\n",
        "# import NE_candidate_module as ne\n",
        "# import Mention\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "import trie as trie\n",
        "import ast\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "import copy\n",
        "\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "# import matplotlib.pyplot as plt\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"other\",\"since\",\"hence\",\"onto\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"up\")\n",
        "\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"v.\"]  #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"httpurl\",\"n't\",\"pls\",\"bye\",\"€\",\"vs\",\"ouch\",\"am\",\"pm\",\"omw\",\"http\",\"https\",\"tv\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"ip\",\"qft\",\"ima\",\"icymi\",\"bdsm\",\"ah\",\"ive\",\"qt\",\"dj\",\"dm\",\"pts\",\"pt\",\"yrs\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "class LocalNERModule():\n",
        "    def __init__(self, sentenceTokenizer, nerTokenizer, nerEngine, device):\n",
        "        self.counter=0\n",
        "\n",
        "        if(sentenceTokenizer):\n",
        "            self.my_sentence_tokenizer = sentenceTokenizer\n",
        "        else:\n",
        "            nltk.download('gutenberg')\n",
        "            gutenberg_text = \"\"\n",
        "            for file_id in gutenberg.fileids():\n",
        "                gutenberg_text += gutenberg.raw(file_id)\n",
        "            tokenizer_trainer = PunktTrainer()\n",
        "            tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
        "            tokenizer_trainer.train(gutenberg_text)\n",
        "\n",
        "            self.my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
        "            self.my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
        "\n",
        "        self.quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        self.tweet_to_sentences_w_annotation = {}\n",
        "        self.device = device\n",
        "        self.apostrophe_list =[\"'s\",'’s','s']\n",
        "        self.label_list = ['O','B','I']\n",
        "\n",
        "        self.tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "        self.contextual_embeddings = {}\n",
        "\n",
        "        print('Starting Local NER Engine!')\n",
        "        self.expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location', 8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "        self.BIO_dict={'O':0,'B':1,'I':2}\n",
        "        if((nerTokenizer is not None)&(nerEngine is not None)):\n",
        "            self.nerTokenizer = nerTokenizer\n",
        "            self.localNEREngine = nerEngine\n",
        "        else:\n",
        "            self.train_engine()\n",
        "\n",
        "    def train_engine(self):\n",
        "        task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "        model_checkpoint = \"vinai/bertweet-base\"\n",
        "        batch_size = 16\n",
        "        # set_seed(42)\n",
        "        datasets = load_dataset(\"wnut_17\")\n",
        "        self.nerTokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "        label_all_tokens = True\n",
        "        tokenized_datasets = datasets.map(self.tokenize_and_align_labels)\n",
        "        data_collator = DataCollatorForTokenClassification(self.nerTokenizer)\n",
        "        self.metric = load_metric(\"seqeval\")\n",
        "        self.localNEREngine = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(self.label_list))\n",
        "        alt_training_args = TrainingArguments(\n",
        "            f\"test-{task}\",\n",
        "            evaluation_strategy = \"epoch\",\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "        )\n",
        "        alt_trainer = Trainer(\n",
        "        self.localNEREngine,\n",
        "        alt_training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=self.nerTokenizer,\n",
        "        compute_metrics=self.compute_metrics\n",
        "        )\n",
        "        alt_trainer.train()\n",
        "\n",
        "        # tokenizer.save_pretrained('test-ner/')\n",
        "        # alt_model.save_pretrained('test-ner/')\n",
        "\n",
        "    def tokenize_and_align_labels(self,example):\n",
        "        \n",
        "        tokenized_ds_input = self.nerTokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "        inputId_to_token_dict={}\n",
        "        for index, token in enumerate(example[\"tokens\"]):\n",
        "            values=self.nerTokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "            for value in values:\n",
        "                try:\n",
        "                    inputId_to_token_dict[value].append(index)\n",
        "                except KeyError:\n",
        "                    inputId_to_token_dict[value]=[index]\n",
        "        labels=[]\n",
        "        for inputID in tokenized_ds_input['input_ids']:\n",
        "            try:\n",
        "                index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "                index_to_address=index_list.pop(0)\n",
        "\n",
        "                label=self.BIO_dict[self.expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "                # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "                labels.append(label)\n",
        "                inputId_to_token_dict[inputID]=index_list\n",
        "            except KeyError:\n",
        "                labels.append(-100)\n",
        "\n",
        "        assert (len(tokenized_ds_input['input_ids']) == len(labels))\n",
        "        tokenized_ds_input['labels']=labels\n",
        "        \n",
        "        return tokenized_ds_input\n",
        "\n",
        "    def compute_metrics(self, p):\n",
        "        # print(p.shape)\n",
        "        output, labels = p\n",
        "\n",
        "        # print(len(predictions))\n",
        "        # print(predictions[0].shape)\n",
        "        # for elem in predictions[1]:\n",
        "        #   print(elem.shape)\n",
        "\n",
        "        predictions, _ = output\n",
        "        \n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # Remove ignored index (special tokens)\n",
        "        true_predictions = [\n",
        "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        results = self.metric.compute(predictions=true_predictions, references=true_labels)\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }\n",
        "\n",
        "    def is_float(self,string):\n",
        "        try:\n",
        "            f=float(string)\n",
        "            if(f==0.0):\n",
        "              return True\n",
        "            else:\n",
        "              return ((f) and (string.count(\".\")==1))\n",
        "      #return True# True if string is a number with a dot\n",
        "        except ValueError:  # if string is not a number\n",
        "          return False\n",
        "\n",
        "    def collate_token_label_embedding(self, tweetWordList, token_dict, prediction_labels, entity_embeddings):\n",
        "        counter=0\n",
        "        collated_labels=[]\n",
        "        collated_entity_embeddings=[]\n",
        "        for word in tweetWordList:\n",
        "            vals=token_dict[word]\n",
        "            # print(word,vals)\n",
        "            if(counter<len(prediction_labels)):\n",
        "                labels=prediction_labels[counter:counter+len(vals)]\n",
        "                token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
        "        #         print(token_entity_embeddings.shape)\n",
        "                mean_tensor = torch.mean(token_entity_embeddings,dim=0)\n",
        "                mean_tensor[torch.isnan(mean_tensor)] = 0\n",
        "                collated_entity_embeddings.append(mean_tensor)\n",
        "        #         print(collated_entity_embeddings)\n",
        "                if('I' in labels):\n",
        "                    collated_labels.append('I')\n",
        "                elif('B' in labels):\n",
        "                    collated_labels.append('B')\n",
        "                else:\n",
        "                    collated_labels.append('O')\n",
        "                counter+=len(vals)\n",
        "            else:\n",
        "                collated_labels.append('O')\n",
        "                collated_entity_embeddings.append(torch.zeros(768).to(self.device))\n",
        "        assert len(collated_labels)==len(collated_entity_embeddings)\n",
        "        return collated_labels,collated_entity_embeddings\n",
        "\n",
        "\n",
        "    def get_entities(self, word_tag_tuples):\n",
        "        mentions=[]\n",
        "        candidateMention=''\n",
        "        positions=[]\n",
        "        \n",
        "        #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
        "        for index, tup in enumerate(word_tag_tuples):\n",
        "            candidate=tup[0]\n",
        "            tag=tup[1]\n",
        "            if(tag=='O'):\n",
        "                if(candidateMention):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,positions)\n",
        "                                return\n",
        "                candidateMention=''\n",
        "                positions=[]\n",
        "            else:\n",
        "                if (tag=='B'):\n",
        "                    if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
        "                        mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                        if mention_to_add.endswith(\"'s\"):\n",
        "                            li = mention_to_add.rsplit(\"'s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        elif mention_to_add.endswith(\"’s\"):\n",
        "                            li = mention_to_add.rsplit(\"’s\", 1)\n",
        "                            mention_to_add=''.join(li)\n",
        "                        else:\n",
        "                            mention_to_add=mention_to_add\n",
        "                        if(mention_to_add!=''):\n",
        "                            try:\n",
        "                                assert len(mention_to_add.split()) == len(positions)\n",
        "                                mentions.append((mention_to_add,positions))\n",
        "                            except AssertionError:\n",
        "                                print(word_tag_tuples)\n",
        "                                print(mention_to_add,positions)\n",
        "                                return\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention=candidate\n",
        "                        positions=[index]\n",
        "                else:\n",
        "                    if((candidate.strip() not in string.punctuation)&(emoji.get_emoji_regexp().sub(u'', candidate).strip(string.punctuation).lower().strip()!='')&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                        candidateMention+=\" \"+candidate\n",
        "                        positions.append(index)\n",
        "            # if (tag=='B'):\n",
        "            #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
        "            #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "            #         if(mention_to_add):\n",
        "            #             mentions.append(mention_to_add)\n",
        "            #     candidateMention=candidate\n",
        "            # else:\n",
        "            #     candidateMention+=\" \"+candidate\n",
        "        if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
        "            if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))&(candidate.strip().strip(string.punctuation) not in self.apostrophe_list)):\n",
        "                mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
        "                if(mention_to_add!=''):\n",
        "                    try:\n",
        "                        assert len(mention_to_add.split()) == len(positions)\n",
        "                        mentions.append((mention_to_add,positions))\n",
        "                    except AssertionError:\n",
        "                        print(word_tag_tuples)\n",
        "                        print(mention_to_add,positions)\n",
        "                        return\n",
        "            # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
        "        # print('extracted mentions:', mentions)\n",
        "        return mentions\n",
        "\n",
        "    def normalize_to_sentences(self, text):\n",
        "        tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
        "        tweetSentenceList_inter=self.custom_flatten(list(map(lambda sentText: self.my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
        "        tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
        "        return tweetSentenceList\n",
        "\n",
        "    def custom_flatten(self, mylist, outlist, ignore_types=(str, bytes, int)):\n",
        "        \n",
        "        if (mylist !=[]):\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.custom_flatten(item, outlist)\n",
        "                else:\n",
        "                    item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "\n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "    #         elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "    #             temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "    #             if(temp1):\n",
        "    #                 temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "    #             temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.custom_flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "\n",
        "    def remAmpersand(self,candidateStr):\n",
        "        candidateStr=candidateStr.replace('&amp;','')\n",
        "        return candidateStr\n",
        "\n",
        "    def normalizeToken(self,token):\n",
        "        lowercased_token = token.lower()\n",
        "        if token.startswith(\"@\"):\n",
        "            return \"@USER\"\n",
        "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "            return \"HTTPURL\"\n",
        "        elif len(token) == 1:\n",
        "            return demojize(token)\n",
        "        else:\n",
        "            if token == \"’\":\n",
        "                return \"'\"\n",
        "            elif token == \"…\":\n",
        "                return \"...\"\n",
        "            else:\n",
        "                return token\n",
        "\n",
        "    def normalizeTweet(self, tweet):\n",
        "        tokens = self.tweetTokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "        normTweet = \" \".join([self.normalizeToken(token) for token in tokens])\n",
        "\n",
        "        normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "        normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "        normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
        "\n",
        "        normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "        normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "        \n",
        "        return normTweet\n",
        "\n",
        "    #removing commonly used expletives, enunciated chat words and other common words (like days of the week, common expressions)\n",
        "    def slang_remove(self,ne_phrase):\n",
        "        phrase=ne_phrase.strip().strip(string.punctuation).lower()\n",
        "        p1= re.compile(r'([A-Za-z]+)\\1\\1{1,}')\n",
        "        match_lst = p1.findall(phrase)\n",
        "        if phrase in article_list:\n",
        "            return True\n",
        "        elif phrase in day_list:\n",
        "            return True\n",
        "        elif phrase in month_list:\n",
        "            return True\n",
        "        elif match_lst:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def all_slang(self, ne_phrase):\n",
        "        ne_words=ne_phrase.split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list\n",
        "        is_invalid=0\n",
        "\n",
        "        for word in ne_words:\n",
        "            if(word.strip().strip(string.punctuation).lower() in combined):\n",
        "                is_invalid+=1\n",
        "        if(is_invalid==len(ne_words)):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def start_end_token_check(self, ne_candidate):\n",
        "        positions = ne_candidate[1]\n",
        "        ne_words=ne_candidate[0].split()\n",
        "        combined=[]+cachedStopWords+cachedTitles+chat_word_list+day_list+prep_list+article_list\n",
        "        # print(combined)\n",
        "        # print(ne_words)\n",
        "        start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "        end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        while((len(ne_words)>1)&((start_word in combined)|(end_word in combined))):\n",
        "            if(start_word in combined):\n",
        "                ne_words.pop(0)\n",
        "                positions.pop(0)\n",
        "            if(end_word in combined):\n",
        "                ne_words.pop()\n",
        "                positions.pop()\n",
        "            if(len(ne_words)>1):\n",
        "                start_word = ne_words[0].strip().strip(string.punctuation).lower()\n",
        "                end_word = ne_words[-1].strip().strip(string.punctuation).lower()\n",
        "        ne_phrase = (' '.join(ne_words)).strip()\n",
        "        # print(ne_phrase)\n",
        "        return (ne_phrase,positions)\n",
        "\n",
        "    def set_stopword_exceptions(self,words):\n",
        "        combined=cachedStopWords+prep_list+article_list+day_list\n",
        "        for word in words:\n",
        "            if word in combined:\n",
        "                self.swSet.add(word)\n",
        "\n",
        "    def extract(self, batch, batch_number):\n",
        "        print(\"Running Local NER now\")\n",
        "        time_in=time.time()\n",
        "        self.batch=batch\n",
        "        self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'TweetSentence','tweetwordList', 'phase1Candidates','start_time','entry_batch'))\n",
        "        df_holder=[]\n",
        "        self.swSet= set()\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #self.df_out= pd.DataFrame(columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates','correct_candidates_tweet'))\n",
        "            #dict1 = {'tweetID':0, 'sentID':0, 'hashtags':'first', 'user':'user', 'TweetSentence':'sentence', 'phase1Candidates':'phase1Out','start_time':'now','entry_batch':'batch_number'}\n",
        "            self.CTrie=trie.Trie(\"ROOT\")\n",
        "            self.phase2stopWordList=[]\n",
        "            self.sentenceID = 0\n",
        "            self.f=0\n",
        "\n",
        "        for row in self.batch.itertuples():\n",
        "\n",
        "            now = datetime.datetime.now()\n",
        "            tweetID=str(row.Index)\n",
        "            text=str(row.TweetText)\n",
        "            row_sentences = self.normalize_to_sentences(text)\n",
        "\n",
        "            annnotated_mentions=[]\n",
        "\n",
        "            #Comment out when no annotations available\n",
        "            for sentence_level in str(row.mentions_other).split(';'):\n",
        "                if(sentence_level):\n",
        "                    for mention in sentence_level.split(','):\n",
        "                        if(mention):\n",
        "                            annnotated_mentions.append(mention.lower().strip(string.punctuation).strip())\n",
        "            annnotated_mentions=list(filter(lambda element: ((element !='')&(element !='nan')), annnotated_mentions))\n",
        "\n",
        "            self.tweet_to_sentences_w_annotation[tweetID]=((self.sentenceID,self.sentenceID+len(row_sentences)),annnotated_mentions)\n",
        "            self.sentenceID+=len(row_sentences)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for sen_index, sentence in enumerate(row_sentences):\n",
        "\n",
        "                    # print('tuple index:',tweetID,sen_index)\n",
        "                    phase1Out=\"\"\n",
        "                    sentence = self.normalizeTweet(sentence)\n",
        "                    # tweetWordList=self.getWords(sentence)\n",
        "                    tweetWordList = sentence.split()\n",
        "                    enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "\n",
        "                    entities_from_sentence=[]\n",
        "                    entity_aware_embeddings=[]\n",
        "\n",
        "                    if(len(tweetWordList)>0):\n",
        "\n",
        "                        # print(test_record)\n",
        "                        # tokenized_input=tokenizer(test_record)\n",
        "                        # initial_input_ids = torch.tensor([tokenizer.encode(test_record)])\n",
        "                        # token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in test_record.split()}\n",
        "                        # input_ids = initial_input_ids.to(device)\n",
        "                        # tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        tokenized_input= self.nerTokenizer(sentence)\n",
        "                        initial_input_ids = torch.tensor([self.nerTokenizer.encode(sentence)])\n",
        "                        # num_tokens = initial_input_ids.shape[1]\n",
        "\n",
        "                        initial_input_ids = initial_input_ids[:,:128]\n",
        "                        token_dict = {x : self.nerTokenizer.encode(x, add_special_tokens=False) for x in sentence.split()} #token, add_special_tokens=False, truncation=True\n",
        "                        input_ids = initial_input_ids.to(self.device)\n",
        "                        tokens = self.nerTokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "                        output = self.localNEREngine(input_ids)\n",
        "                        token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "                        prediction = (torch.argmax(output.logits, axis=2))\n",
        "                        prediction = prediction.cpu().numpy().reshape(-1)\n",
        "\n",
        "                        # prediction_labels=[self.label_list[l].split('-')[0] for l in prediction]\n",
        "                        prediction_labels=[self.label_list[l] for l in prediction] #Just BIO\n",
        "\n",
        "                        prediction_labels, entity_aware_embeddings=self.collate_token_label_embedding(tweetWordList, token_dict, prediction_labels[1:-1],token_embeddings)\n",
        "\n",
        "                        assert len(enumerated_tweetWordList)==len(entity_aware_embeddings)\n",
        "                        assert len(prediction_labels)==len(enumerated_tweetWordList)\n",
        "\n",
        "                        word_tag_tuples=list(zip(token_dict.keys(),prediction_labels))\n",
        "                        # print(list(word_tag_tuples))\n",
        "                        entities_from_sentence=self.get_entities(word_tag_tuples)\n",
        "                        # print('entities_from_sentence:',entities_from_sentence)\n",
        "\n",
        "                        if(self.f<5):\n",
        "                            print(len(tweetWordList),initial_input_ids.shape,len(prediction[1:-1]))\n",
        "                            print(tweetWordList)\n",
        "                            print(token_dict)\n",
        "                            print(initial_input_ids)\n",
        "                            print(input_ids)\n",
        "                            print(prediction)\n",
        "                            print('entities_from_sentence:',entities_from_sentence)\n",
        "                            print('======')\n",
        "                            self.f+=1\n",
        "\n",
        "                    just_candidates=[]\n",
        "\n",
        "                    # place some necessary filters\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.slang_remove(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(map(lambda element: self.start_end_token_check(element), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: not self.all_slang(element[0]), entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: len(element[0])>1, entities_from_sentence))\n",
        "                    entities_from_sentence= list(filter(lambda element: element[0]!='', entities_from_sentence))\n",
        "\n",
        "                    \n",
        "\n",
        "                    for candidateTuple in entities_from_sentence:\n",
        "                        #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
        "                        candidateText, positions = candidateTuple\n",
        "                        candidateText=(((candidateText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
        "                        candidateText=(self.remAmpersand(candidateText).lstrip('“‘’”')).rstrip('“‘’”')\n",
        "                        candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "                        candidateText= candidateText.strip()\n",
        "                        self.set_stopword_exceptions(candidateText.split())\n",
        "                        just_candidates.append(candidateText)\n",
        "                        # if(index==9423):\n",
        "                        #     print(candidateText)\n",
        "                        position = '*'+'*'.join(str(v) for v in positions)\n",
        "                        position=position+'*'\n",
        "\n",
        "                        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
        "\n",
        "                        combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
        "                        if not ((candidateText in combined)|(len(candidateText)<=1)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
        "                            if(self.quickRegex.match(candidateText)):\n",
        "                                self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
        "                    \n",
        "                    print('entities_from_sentence:',just_candidates)\n",
        "                    #storing the outputs and token embeddings in dataframe tweetID,sen_index\n",
        "                    dict1 = {'tweetID':str(tweetID), 'sentID':str(sen_index), 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList,'phase1Candidates': just_candidates ,'phase1CandidatesWPositions':phase1Out,\n",
        "                             'contextual_embeddings':entity_aware_embeddings,\n",
        "                             'start_time':now,'entry_batch':batch_number}\n",
        "                    df_holder.append(dict1)\n",
        "                    # self.contextual_embeddings[(tweetID,sen_index)] = entity_aware_embeddings\n",
        "\n",
        "        time_out=time.time()\n",
        "        self.append_rows(df_holder)\n",
        "        self.phase2stopWordList=list(set(self.phase2stopWordList)|self.swSet)\n",
        "        self.counter=self.counter+1\n",
        "        \n",
        "        #return (copy.deepcopy(self.df_out),copy.deepcopy(freqs),time_in,time_out)\n",
        "        return (self.df_out,self.contextual_embeddings,self.CTrie,time_in,time_out,self.phase2stopWordList,self.tweet_to_sentences_w_annotation)\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        self.df_out=self.df_out.append(df)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncj3sC0FkBwA"
      },
      "source": [
        "## **Phase II: Global NER with Phrase Embedder to collect the Entity Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noxa5i5JPJi4"
      },
      "source": [
        "# coding: utf-8\n",
        "from nltk.corpus import stopwords\n",
        "import pandas  as pd\n",
        "# import NE_candidate_module as ne\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "import copy\n",
        "import numpy\n",
        "import math\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from collections import Iterable, OrderedDict\n",
        "from scipy import stats\n",
        "import emoji\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import trie as trie\n",
        "import re\n",
        "import ast\n",
        "import pickle\n",
        "import itertools\n",
        "from scipy import spatial\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.cluster import KMeans, MeanShift\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import torch\n",
        "\n",
        "#CLASSIFIER CLASS\n",
        "# import SVM as svm\n",
        "# import entityClassifier as entityClassifier\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "tempList=[\"i\",\"and\",\"or\",\"since\",\"hence\",\"onto\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"probably\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
        "for item in tempList:\n",
        "    if item not in cachedStopWords:\n",
        "        cachedStopWords.append(item)\n",
        "cachedStopWords.remove(\"don\")\n",
        "# cachedStopWords.remove(\"your\")\n",
        "# cachedStopWords.remove(\"us\")\n",
        "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
        "prep_list=[\"of\",\"&;\",\"v.\"] #includes common conjunction as well\n",
        "# prep_list=[]\n",
        "# article_list=[]\n",
        "article_list=[\"a\",\"an\",\"the\"]\n",
        "conjoiner=[\"de\"]\n",
        "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
        "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "chat_word_list=[\"nope\",\"httpurl\",\"n't\",\"&amp;\",\"gee\",\"€\",\"hmm\",\"bye\",\"pls\",\"please\",\"yrs\",\"4get\",\"ooh\",\"ouch\",\"am\",\"tv\",\"ima\",\"tmw\",\"og\",\"psst\",\"b.s\",\"thanku\",\"em\",\"qft\",\"ip\",\"icymi\",\"bdsm\",\"ah\",\"http\",\"https\",\"pm\",\"omw\",\"pts\",\"pt\",\"ive\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
        "string.punctuation=string.punctuation+'…‘’'\n",
        "\n",
        "\n",
        "\n",
        "class GlobalNERModule():\n",
        "\n",
        "\n",
        "    def executor(self,max_batch_value,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others):\n",
        "    # def executor(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold,raw_tweets_for_others)\n",
        "\n",
        "\n",
        "        # SET CB\n",
        "        # print(phase2stopwordList)\n",
        "        candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag = self.set_cb(TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold)\n",
        "        candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        # print(self.good_candidates)\n",
        "\n",
        "        # SET TF \n",
        "        untrashed_tweets=self.set_tf(data_frame_holder, candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "        #mark incomplete tweets\n",
        "        self.set_column_for_candidates_in_incomplete_tweets(candidate_featureBase_DF,untrashed_tweets)\n",
        "\n",
        "        # SAVE INCOMING TWEETS FOR ANNOTATION FOR OTHERS\n",
        "        # self.raw_tweets_for_others=pd.concat([self.raw_tweets_for_others,raw_tweets_for_others ])\n",
        "\n",
        "        # DROP TF\n",
        "        just_converted_tweets=self.get_complete_tf(untrashed_tweets)\n",
        "\n",
        "        #incomplete tweets at the end of current batch\n",
        "        incomplete_tweets=self.get_incomplete_tf(untrashed_tweets)\n",
        "\n",
        "        #all incomplete_tweets---> incomplete_tweets at the end of current batch + incomplete_tweets not reintroduced\n",
        "        # self.incomplete_tweets=incomplete_tweets #without reintroduction--- when everything is reintroduced, just incomplete_tweets\n",
        "        # self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized','annotation','stanford_candidates'])\n",
        "        # self.incomplete_tweets=pd.concat([incomplete_tweets,self.not_reintroduced],ignore_index=True)\n",
        "        self.incomplete_tweets=pd.concat([incomplete_tweets],ignore_index=True)\n",
        "\n",
        "        print('completed tweets:',len(just_converted_tweets))\n",
        "        print('incomplete tweets:',len(incomplete_tweets))\n",
        "\n",
        "\n",
        "        # #recording tp, fp , f1\n",
        "        # #self.accuracy_tuples_prev_batch.append((just_converted_tweets.tp.sum(), just_converted_tweets.total_mention.sum(),just_converted_tweets.fp.sum(),just_converted_tweets.fn.sum()))\n",
        "\n",
        "\n",
        "        # #operations for getting ready for next batch.\n",
        "        # # self.incomplete_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "        # self.incomplete_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "        # self.counter=self.counter+1\n",
        "\n",
        "        self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets)\n",
        "\n",
        "        time_out=time.time()\n",
        "\n",
        "        self.calculate_tp_fp_f1(z_score_threshold,untrashed_tweets,raw_tweets_for_others)\n",
        "\n",
        "        if(self.counter==(max_batch_value+1)):\n",
        "            # self.just_converted_tweets.drop('2nd Iteration Candidates', axis=1, inplace=True)\n",
        "            self.just_converted_tweets.drop(['2nd Iteration Candidates','2nd Iteration Candidates Unnormalized'], axis=1, inplace=True)\n",
        "\n",
        "            print('completed tweets: ', len(self.just_converted_tweets),'incomplete tweets: ', len(self.incomplete_tweets))\n",
        "            \n",
        "            print(len(list(self.just_converted_tweets.columns.values)))\n",
        "            print(len(list(self.incomplete_tweets.columns.values)))\n",
        "\n",
        "            combined_frame_list=[self.just_converted_tweets, self.incomplete_tweets]\n",
        "            complete_tweet_dataframe = pd.concat(combined_frame_list)\n",
        "\n",
        "            print('final tally: ', (len(self.just_converted_tweets)+len(self.incomplete_tweets)), len(complete_tweet_dataframe))\n",
        "\n",
        "            #to groupby tweetID and get one tuple per tweetID\n",
        "            complete_tweet_dataframe_grouped_df= (complete_tweet_dataframe.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "            complete_tweet_dataframe_grouped_df['tweetID']=complete_tweet_dataframe_grouped_df['tweetID'].astype(int)\n",
        "            self.complete_tweet_dataframe_grouped_df_sorted=(complete_tweet_dataframe_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "            print(list(self.complete_tweet_dataframe_grouped_df_sorted.columns.values))\n",
        "\n",
        "\n",
        "        #self.aggregator_incomplete_tweets.to_csv(\"all_incompletes.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "\n",
        "        #self.just_converted_tweets.to_csv(\"all_converteds.csv\", sep=',', encoding='utf-8')\n",
        "        #self.incomplete_tweets.to_csv(\"incomplete_for_last_batch.csv\", sep=',', encoding='utf-8')\n",
        "        return candidate_featureBase_DF, self.complete_tweet_dataframe_grouped_df_sorted,time_out \n",
        "        # return candidate_featureBase_DF, untrashed_tweets,time_out\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,entity_phrase_embedder,device,filename):\n",
        "        self.counter=0\n",
        "        self.decay_factor=2**(-1/2)\n",
        "        self.decay_base_staggering=2\n",
        "        self.true_positive_count=0\n",
        "        self.false_positive_count=0\n",
        "        self.false_negative_count=0\n",
        "        self.device=device\n",
        "        self.save_file= filename\n",
        "\n",
        "        self.entity_phrase_embedder = entity_phrase_embedder\n",
        "\n",
        "        # context_feature_list=['cf_'+str(i) for i in range(768)]\n",
        "        context_feature_list=['cf_'+str(i) for i in range(300)]\n",
        "        self.candidateBaseHeaders=['candidate', 'batch', 'length','cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']+context_feature_list+['cumulative']\n",
        "\n",
        "        ## When not running on a notebook\n",
        "        # self.entity_classifier = entityClassifier.EntityClassifier('data/candidate_train_records.csv',True,self.device)\n",
        "\n",
        "        ################################### To do a fresh training\n",
        "\n",
        "        ## With one unified classifier\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large.csv',True,self.device)\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_300d.csv',True,self.device)\n",
        "\n",
        "        # # With two separate classifiers\n",
        "        # self.entity_classifierI= EntityClassifierI('data/training.csv',True,self.device)\n",
        "        # # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large.csv',True,self.device)\n",
        "        self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large_300d.csv',True,self.device)\n",
        "\n",
        "        ################################### To Load a pre-trained model\n",
        "\n",
        "        ## With one unified classifier\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large.csv',False,self.device)\n",
        "        # self.entity_classifier = EntityClassifier('data/candidate_train_records_large_300d.csv',False,self.device)\n",
        "\n",
        "        ## With two separate classifiers\n",
        "        # self.entity_classifierI= EntityClassifierI('data/training.csv',False,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large.csv',False,self.device)\n",
        "        # self.entity_classifierII = EntityClassifierII('data/candidate_train_records_large_300d.csv',False,self.device)\n",
        "\n",
        "\n",
        "        ################################### Older SVM classifier\n",
        "        # self.my_classifier= svm.SVM1('/home/satadisha/Desktop/GitProjects/TwiCSv2/production_code/training.csv')\n",
        "        \n",
        "        # self.my_classifier= svm.SVM1('/Users/satadisha/Documents/GitHub/TwiCSv2/production_code/training.csv')\n",
        "        # self.my_classifier= svm.SVM1('/Users/satadisha/Documents/GitHub/tweebo-parser/training.csv')\n",
        "        # self.my_classifier= svm.SVM1('training.csv')\n",
        "        self.complete_tweet_dataframe_grouped_df_sorted=pd.DataFrame([], columns=['tweetID', 'TweetSentence', 'ambiguous_candidates', 'annotation', 'candidates_with_label', 'completeness', 'current_minus_entry', 'entry_batch', 'hashtags', 'index', 'only_good_candidates', 'output_mentions', 'phase1Candidates', 'sentID', 'stanford_candidates', 'user'])\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1_generic(self,raw_tweets_for_others,state_of_art):\n",
        "\n",
        "        column_candidates_holder = raw_tweets_for_others[state_of_art].tolist()\n",
        "        \n",
        "\n",
        "        column_annot_holder= raw_tweets_for_others['mentions_other'].tolist()\n",
        "        # column_annot_holder= raw_tweets_for_others['annotation_limited types'].tolist()\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=column_annot_holder[idx].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=ast.literal_eval(column_candidates_holder[idx])\n",
        "            else:\n",
        "                output_mentions_list=column_candidates_holder[idx].split(',')\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            # print(annotated_mention_list,output_mentions_list)\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall))    \n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "        # tweet_ids_df[\"tp\"+state_of_art]=true_positive_holder\n",
        "        # tweet_ids_df[\"fn\"+state_of_art]=false_negative_holder\n",
        "        # tweet_ids_df['fp'+state_of_art]= false_positive_holder\n",
        "        \n",
        "        # if(state_of_art==\"ritter_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"ritter_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # if(state_of_art==\"stanford_candidates\"):\n",
        "        #     tweet_ids_df.to_csv(\"stanford_results.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "    def calculate_tp_fp_f1_alternate(self,raw_tweets_for_others, state_of_art):\n",
        "        if(state_of_art=='neuroner'):\n",
        "            # fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/venezuela_input_2019-05-10_12-33-16-15380/mentions_output.txt\",\"r\")\n",
        "            fp= open(\"/home/satadisha/Desktop/GitProjects/NeuroNER-master/neuroner/output/tweets_3K_input_2019-04-26_16-49-32-20455/mentions_output.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='stanford_candidates'):\n",
        "            fp= open(\"/home/satadisha/Desktop/stanford-ner-2016-10-31/stanford_venezuela_mentions.txt\",\"r\")\n",
        "            mentions_list = fp.read().split(\"\\n\") # Create a list containing all lines\n",
        "            fp.close() # Close file\n",
        "        if(state_of_art=='ritter_candidates'):\n",
        "            tweets_ritter=pd.read_csv(\"/home/satadisha/Desktop/GitProjects/twitter_nlp-master/ritter-venezuela-output.csv\",sep =',', keep_default_na=False)\n",
        "        if(state_of_art=='calai_candidates'):\n",
        "            tweets_calai=pd.read_csv(\"/home/satadisha/Desktop/opencalai_versions/venezuela_output.csv\",sep =',', keep_default_na=False)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        print('========================='+state_of_art)\n",
        "\n",
        "        total_annotation=0\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        for index, row in raw_tweets_for_others.iterrows():\n",
        "            \n",
        "\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            tweet_level_candidate_list=row['mentions_other'].split(';')\n",
        "            # tweet_level_candidate_list=row['annotation_limited types'].split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: (element !=''), annotated_mention_list))\n",
        "\n",
        "            if(state_of_art=='ritter_candidates'):\n",
        "                # output_mentions_list=ast.literal_eval(mentions_list[idx])\n",
        "                output_mentions_list=tweets_ritter[tweets_ritter['ID']==row['ID']]['Output'].iloc[0].split(',')\n",
        "            if(state_of_art=='calai_candidates'):\n",
        "                output_mentions_list=tweets_calai[tweets_calai['ID']==row['ID']]['calai_candidates'].iloc[0].split(',')\n",
        "            if((state_of_art=='neuroner')|(state_of_art=='stanford_candidates')):\n",
        "                #for 3k Tweets:\n",
        "                idx=int(row['ID'])\n",
        "\n",
        "                # #for others:\n",
        "                # idx=int(row['ID']-1)\n",
        "\n",
        "                output_mentions_list=mentions_list[idx].split(',')\n",
        "\n",
        "            output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "\n",
        "            print(annotated_mention_list,output_mentions_list)\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= annotated_mention_list.pop()\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            true_positive_count+=tp_counter_inner\n",
        "            false_positive_count+=fp_counter_inner\n",
        "            false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "\n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        # print(false_positive_count)\n",
        "        # print(false_negative_count)\n",
        "        precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        if(state_of_art==\"ritter_candidates\"):\n",
        "            self.accuracy_vals_ritter.append((f_measure,precision,recall))    \n",
        "        if(state_of_art==\"stanford_candidates\"):\n",
        "            self.accuracy_vals_stanford.append((f_measure,precision,recall))\n",
        "        if(state_of_art==\"calai_candidates\"):\n",
        "            self.accuracy_vals_opencalai.append((f_measure,precision,recall)) \n",
        "        if(state_of_art==\"neuroner\"):\n",
        "            self.accuracy_vals_neuroner.append((f_measure,precision,recall))\n",
        "\n",
        "\n",
        "        # output_mentions_list= mentions_list[output_index].split(',')\n",
        "#     # output_mentions_list=list(map(lambda element: element.lower().strip(),output_mentions_list))\n",
        "#     # output_mentions_list=list(filter(lambda element: element !='', output_mentions_list))\n",
        "\n",
        "    def calculate_tp_fp_f1_for_others(self,raw_tweets_for_others):\n",
        "\n",
        "        opencalai=\"calai_candidates\"\n",
        "        stanford=\"stanford_candidates\"\n",
        "        ritter=\"ritter_candidates\"\n",
        "        neuroner=\"neuroner\"\n",
        "\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,opencalai)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,stanford)\n",
        "        self.calculate_tp_fp_f1_generic(raw_tweets_for_others,ritter)\n",
        "\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,opencalai)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,stanford)\n",
        "        # self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,ritter)\n",
        "\n",
        "        self.calculate_tp_fp_f1_alternate(raw_tweets_for_others,neuroner)\n",
        "\n",
        "    #################################\n",
        "    #input candidate_feature_Base\n",
        "    #output candidate_feature_Base with [\"Z_score\"], [\"probability\"],[\"class\"]\n",
        "    #################################\n",
        "    def classify_candidate_base(self,z_score_threshold,candidate_featureBase_DF):\n",
        "\n",
        "        # #filtering test set based on z_score\n",
        "        # mert1=candidate_featureBase_DF['cumulative'].as_matrix()\n",
        "        #frequency_array = np.array(list(map(lambda val: val[0], sortedCandidateDB.values())))\n",
        "        # zscore_array1=stats.zscore(mert1)\n",
        "\n",
        "        zscore_array1=stats.zscore(candidate_featureBase_DF['cumulative'])\n",
        "\n",
        "        candidate_featureBase_DF['Z_ScoreUnweighted']=zscore_array1\n",
        "        cumulative_threshold=1.0 #set threshold here\n",
        "        z_score_threshold=candidate_featureBase_DF[candidate_featureBase_DF['cumulative']==cumulative_threshold].Z_ScoreUnweighted.tolist()[0]\n",
        "        print(cumulative_threshold,z_score_threshold)\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_new_with_z_score.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        #multi-word infrequent candidates ---> to be used for recall correction\n",
        "        infrequent_candidates=candidate_featureBase_DF[(candidate_featureBase_DF['Z_ScoreUnweighted'] < z_score_threshold) & (candidate_featureBase_DF.length>1)].candidate.tolist()\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF[candidate_featureBase_DF['Z_ScoreUnweighted'] >= z_score_threshold]\n",
        "\n",
        "        # # #######################with one unified classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # return (self.entity_classifier.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with only semantic classifier--- returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        return (self.entity_classifierII.run(candidate_featureBase_DF),infrequent_candidates)\n",
        "\n",
        "        # #######################with two separate classifiers--- requires some additional lines of code\n",
        "        # candidateList = candidate_featureBase_DF.candidate.tolist()\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # candidate_featureBase_DF.set_index(\"candidate\", inplace=True)\n",
        "        # # print('before columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF.index.name)\n",
        "\n",
        "        # # returns updated candidate_featureBase_DF with [\"Z_score\"], [\"probability\"],[\"class\"] attributes.\n",
        "        # candidate_featureBase_DF_classifierI = self.entity_classifierI.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "        # candidate_featureBase_DF_classifierII = self.entity_classifierII.run(copy.deepcopy(candidate_featureBase_DF))\n",
        "\n",
        "        # # candidate_featureBase_DF_classifierI.to_csv('classifierI.csv', sep=',', encoding='utf-8')\n",
        "        # # candidate_featureBase_DF_classifierII.to_csv('classifierII.csv', sep=',', encoding='utf-8') #candidate_featureBase_DF.to_csv(\"candidate_base_new.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        # final_probability_dict = {}\n",
        "        # print5=0\n",
        "        # for candidate in candidateList:\n",
        "        #     prob1 = candidate_featureBase_DF_classifierI.loc[candidate]['probability']\n",
        "        #     prob2 = candidate_featureBase_DF_classifierII.loc[candidate]['probability']\n",
        "        #     final_probability = max(prob1,prob2)\n",
        "        #     if(print5<5):\n",
        "        #         print(candidate,prob1,prob2,final_probability)\n",
        "        #         print5+=1\n",
        "        #     final_probability_dict[candidate] = final_probability\n",
        "\n",
        "        # candidate_featureBase_DF[\"probability\"] = pd.Series(final_probability_dict)\n",
        "\n",
        "        # candidate_featureBase_DF.reset_index(drop=False,inplace=True)\n",
        "        # print('after columns:',candidate_featureBase_DF.columns)\n",
        "        # print(candidate_featureBase_DF[['candidate','probability']].head(5))\n",
        "\n",
        "        # return (candidate_featureBase_DF,infrequent_candidates)\n",
        "\n",
        "    # recall_correction\n",
        "    def set_partition_dict(self,candidate_featureBase_DF,infrequent_candidates):\n",
        "\n",
        "        #print(list(self.partition_dict.keys()))\n",
        "        ambiguous_bad_candidates=candidate_featureBase_DF[(((candidate_featureBase_DF.status==\"a\")|(candidate_featureBase_DF.status==\"b\"))&(candidate_featureBase_DF.length.astype(int)>1))]\n",
        "        good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "        flag1=False\n",
        "        flag2=False\n",
        "        if(len(ambiguous_bad_candidates)>0):\n",
        "            # ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1) \n",
        "            # ambiguous_bad_candidates_wFilter= ambiguous_bad_candidates[ambiguous_bad_candidates.max_column=='substring-cap']\n",
        "\n",
        "            #good_candidates=candidate_featureBase_DF[(candidate_featureBase_DF.status==\"g\")].candidate.tolist()\n",
        "            #print(ambiguous_bad_candidates_wFilter.candidate.tolist())\n",
        "\n",
        "            # for candidate in ambiguous_bad_candidates_wFilter.candidate.tolist():\n",
        "            for candidate in ambiguous_bad_candidates.candidate.tolist():\n",
        "                \n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "\n",
        "            flag1= True\n",
        "        if(len(infrequent_candidates)>0):\n",
        "            #print(len(ambiguous_bad_candidates_wFilter.candidate.tolist()))\n",
        "\n",
        "            for candidate in infrequent_candidates:\n",
        "                #print(candidate)\n",
        "                if candidate not in self.partition_dict.keys():\n",
        "                    substring_candidates=self.get_substring_candidates(candidate.split(),good_candidates)\n",
        "                    if(len(substring_candidates)>0):\n",
        "                        self.partition_dict[candidate]=substring_candidates\n",
        "            flag2= True\n",
        "        print(list(self.partition_dict.keys()))\n",
        "        return (flag1|flag2)\n",
        "\n",
        "    def get_reintroduced_tweets(self, reintroduction_threshold):\n",
        "        #no reintroduction\n",
        "        #no preferential selection\n",
        "        print(\"incomplete tweets in batch: \",len(self.incomplete_tweets))\n",
        "        # print(list(self.incomplete_tweets.columns.values))\n",
        "\n",
        "        reintroduced_tweets=self.incomplete_tweets[(self.counter-self.incomplete_tweets['entry_batch'])<=reintroduction_threshold]\n",
        "        self.not_reintroduced=self.incomplete_tweets[~self.incomplete_tweets.index.isin(reintroduced_tweets.index)]\n",
        "\n",
        "        print(\"reintroduced tweets: \",len(reintroduced_tweets))\n",
        "        # for i in range(self.counter):\n",
        "        #     print('i:',len(self.incomplete_tweets[self.incomplete_tweets['entry_batch']==i]))\n",
        "        return reintroduced_tweets\n",
        "\n",
        "    def set_cb(self,TweetBase,CTrie,phase2stopwordList,z_score_threshold,reintroduction_threshold):\n",
        "\n",
        "        #input new_tweets, z_score, Updated candidatebase of phase1\n",
        "        #output candidate_featureBase_DF, Incomplete_tweets\n",
        "        data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "\n",
        "        candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted= self.extract(TweetBase,CTrie,phase2stopwordList,0)\n",
        "        phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "        phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "        df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "\n",
        "        if((self.counter>0)&(len(self.incomplete_tweets)>0)):\n",
        "\n",
        "            #tweet candidates for Reintroduction\n",
        "            reintroduced_tweets=self.get_reintroduced_tweets(reintroduction_threshold)\n",
        "            candidate_featureBase_DF,df_holder_extracted,phase2_candidates_holder_extracted,phase2_unnormalized_candidates_holder_extracted = self.extract(reintroduced_tweets,CTrie,phase2stopwordList,1)\n",
        "            phase2_candidates_holder.extend(phase2_candidates_holder_extracted)\n",
        "            phase2_unnormalized_candidates_holder.extend(phase2_unnormalized_candidates_holder_extracted)\n",
        "            df_holder.extend(df_holder_extracted)\n",
        "\n",
        "        #print(len(df_holder))\n",
        "        data_frame_holder = pd.DataFrame(df_holder)\n",
        "        #print(len(self.incomplete_tweets),len(data_frame_holder),len(candidate_featureBase_DF))\n",
        "        \n",
        "\n",
        "        #set ['probabilities'] for candidate_featureBase_DF\n",
        "        candidate_featureBase_DF,self.infrequent_candidates= self.classify_candidate_base(z_score_threshold,candidate_featureBase_DF)\n",
        "\n",
        "        # set readable labels (a,g,b) for candidate_featureBase_DF based on ['probabilities.']\n",
        "        candidate_featureBase_DF=self.set_readable_labels(candidate_featureBase_DF)\n",
        "\n",
        "        self.good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        self.ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "        self.bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        print('good_candidates:',self.good_candidates)\n",
        "        print('bad_candidates:',self.bad_candidates)\n",
        "        print('ambiguous_candidates:',self.ambiguous_candidates)\n",
        "\n",
        "        # entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.good_candidates)]\n",
        "        # non_entity_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.bad_candidates)]\n",
        "        # ambiguous_candidate_records=candidate_featureBase_DF[candidate_featureBase_DF['candidate'].isin(self.ambiguous_candidates)]\n",
        "\n",
        "        correction_flag=self.set_partition_dict(candidate_featureBase_DF,self.infrequent_candidates)\n",
        "\n",
        "        ambiguous_turned_good=[]\n",
        "        ambiguous_turned_bad=[]\n",
        "        ambiguous_remaining_ambiguous=[]\n",
        "        converted_candidates=[]\n",
        "\n",
        "        #['probability'],['a,g,b']\n",
        "        return candidate_featureBase_DF,data_frame_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag\n",
        "\n",
        "\n",
        "        #flush out completed tweets\n",
        "        # input candidate base, looped over tweets (incomplete tweets+ new tweets)\n",
        "        # output: incomplete tweets (a tags in it.), incomplete_tweets[\"Complete\"]\n",
        "    def set_tf(self,data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        return self.set_completeness_in_tweet_frame(data_frame_holder,\n",
        "            candidate_featureBase_DF,\n",
        "            phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag)\n",
        "\n",
        "    def get_incomplete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==False]\n",
        "\n",
        "    def get_complete_tf(self,untrashed_tweets):\n",
        "        return untrashed_tweets[untrashed_tweets.completeness==True]\n",
        "\n",
        "    def compute_seen_tweets_so_far(self,start_batch,end_batch):\n",
        "        if(start_batch==end_batch):\n",
        "            sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch]\n",
        "\n",
        "\n",
        "        sliced_seen_tweets=self.number_of_seen_tweets_per_batch[start_batch:]\n",
        "\n",
        "\n",
        "        counter=0\n",
        "        for elem in sliced_seen_tweets:\n",
        "            counter=counter+elem\n",
        "\n",
        "        return counter\n",
        "\n",
        "\n",
        "    def rreplace(self,s, old, new, occurrence):\n",
        "        if s.endswith(old):\n",
        "            li = s.rsplit(old, occurrence)\n",
        "            return new.join(li)\n",
        "        else:\n",
        "            return s\n",
        "    #ME_EXTR=Mention.Mention_Extraction()\n",
        "\n",
        "\n",
        "    # experiment function\n",
        "    def set_x_axis(self,just_converted_tweets_for_current_batch):\n",
        "\n",
        "        self.incomplete_tweets.to_csv(\"set_x_axis_debug.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "        self.incomplete_tweets['number_of_seen_tweets'] = self.incomplete_tweets['entry_batch'].apply(lambda x: self.compute_seen_tweets_so_far(x,self.counter))\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"entry_vs_tweet_seen_ratio\"]=self.incomplete_tweets['entry_batch']/self.incomplete_tweets['number_of_seen_tweets']\n",
        "\n",
        "\n",
        "        #counter_list= \n",
        "        self.incomplete_tweets[\"ratio_entry_vs_current\"]=self.incomplete_tweets['entry_batch']/self.counter\n",
        "\n",
        "\n",
        "        self.incomplete_tweets[\"current_minus_entry\"]=self.counter-self.incomplete_tweets['entry_batch']\n",
        "\n",
        "        just_converted_tweets_for_current_batch[\"current_minus_entry\"]=self.counter-just_converted_tweets_for_current_batch['entry_batch']\n",
        "\n",
        "        return just_converted_tweets_for_current_batch\n",
        "\n",
        "\n",
        "\n",
        "    def set_column_for_candidates_in_incomplete_tweets(self,candidate_featureBase_DF,input_to_eval):\n",
        "\n",
        "        incomplete_candidates= input_to_eval['2nd Iteration Candidates'].tolist()\n",
        "\n",
        "        candidate_featureBase_DF= candidate_featureBase_DF.set_index('candidate')\n",
        "\n",
        "        candidate_with_label_holder=[]\n",
        "        one_level=[]\n",
        "        \n",
        "\n",
        "        for sentence_level_candidates in incomplete_candidates:\n",
        "\n",
        "            one_level.clear()\n",
        "\n",
        "            for candidate in sentence_level_candidates:\n",
        "                if candidate.lower() in candidate_featureBase_DF.index:\n",
        "                    # label=candidate_featureBase_DF.get_value(candidate.lower(),'status')\n",
        "                    label=candidate_featureBase_DF.at[candidate.lower(),'status']\n",
        "                    one_level.append((candidate,label))\n",
        "                else:\n",
        "                    one_level.append((candidate,\"na\"))\n",
        "\n",
        "            candidate_with_label_holder.append(copy.deepcopy(one_level))\n",
        "\n",
        "\n",
        "        input_to_eval[\"candidates_with_label\"]=candidate_with_label_holder\n",
        "        debug_candidates_label_list= input_to_eval['candidates_with_label'].tolist()\n",
        "        candidates_filtered_g_labeled=[]\n",
        "        row_level_candidates=[]\n",
        "        index_outer=0\n",
        "\n",
        "        candidates_filtered_a_labeled=[]\n",
        "        row_level_a_candidates=[]\n",
        "\n",
        "        for sentence_level in debug_candidates_label_list:\n",
        "\n",
        "            # sentence_level_candidates_unnormalized= incomplete_candidates_unnormalized[index_outer]\n",
        "            row_level_candidates.clear()\n",
        "            row_level_a_candidates.clear()\n",
        "            for candidate in sentence_level:\n",
        "                if(candidate[1]==\"g\"):\n",
        "                    candidate_str = self.erode_article(candidate[0])\n",
        "                    row_level_candidates.append(candidate_str)\n",
        "                if(((candidate[1]==\"b\")|(candidate[1]==\"a\"))&(candidate[0]==\"US\")):\n",
        "                    # print('here')\n",
        "                    row_level_candidates.append(candidate[0])\n",
        "                if(candidate[1]==\"a\"):\n",
        "                    row_level_a_candidates.append(candidate[0])\n",
        "\n",
        "            candidates_filtered_g_labeled.append(copy.deepcopy(row_level_candidates))\n",
        "            candidates_filtered_a_labeled.append(copy.deepcopy(row_level_a_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "\n",
        "        input_to_eval[\"only_good_candidates\"]=candidates_filtered_g_labeled\n",
        "        input_to_eval[\"ambiguous_candidates\"]=candidates_filtered_a_labeled\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_tp_fp_f1(self,z_score_threshold,input_to_eval,raw_tweets_for_others):\n",
        "\n",
        "        input_to_eval_grouped_df= (input_to_eval.groupby('tweetID', as_index=False).aggregate(lambda x: x.tolist()))\n",
        "        input_to_eval_grouped_df['tweetID']=input_to_eval_grouped_df['tweetID'].astype(int)\n",
        "        input_to_eval_df_sorted=(input_to_eval_grouped_df.sort_values(by='tweetID', ascending=True)).reset_index(drop=True)\n",
        "\n",
        "        print(len(input_to_eval_df_sorted),len(raw_tweets_for_others))\n",
        "        \n",
        "        print(set(input_to_eval_df_sorted['tweetID'].values.tolist())-set(raw_tweets_for_others['ID'].values.tolist()))\n",
        "\n",
        "        input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_other'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['mentions_limited_types'].iloc[0])\n",
        "        # input_to_eval_df_sorted['annotation']=input_to_eval_df_sorted['tweetID'].apply(lambda x: raw_tweets_for_others[raw_tweets_for_others['ID']==x]['annotation_limited types'].iloc[0])\n",
        "\n",
        "        column_candidates_holder = input_to_eval_df_sorted['only_good_candidates'].tolist()\n",
        "        # column_candidates_holder = input_to_eval_df_sorted['phase1Candidates'].tolist()\n",
        "\n",
        "        file1 = open(self.save_file+\".txt\", \"w\")\n",
        "        \n",
        "\n",
        "        column_annot_holder= input_to_eval_df_sorted['annotation'].tolist()\n",
        "        \n",
        "        output_str=''\n",
        "        # print(column_candidates_holder)\n",
        "\n",
        "        true_positive_count=0\n",
        "        false_positive_count=0\n",
        "        false_negative_count=0\n",
        "        ambigious_not_in_annotation=0\n",
        "        total_mentions=0\n",
        "        total_annotation=0\n",
        "\n",
        "        all_annotations=[]\n",
        "        all_mentions=[]\n",
        "\n",
        "        true_positive_holder = []\n",
        "        false_negative_holder=[]\n",
        "        false_positive_holder=[]\n",
        "        total_mention_holder=[]\n",
        "        ambigious_not_in_annotation_holder=[]\n",
        "        f_measure_holder=[]\n",
        "\n",
        "        quickRegex=re.compile(\"[a-z]+\")\n",
        "\n",
        "        print('=========================BERTNER_candidates')\n",
        "\n",
        "        for idx in range(len(column_annot_holder)):\n",
        "            unrecovered_annotated_mention_list=[]\n",
        "            tp_counter_inner=0\n",
        "            fp_counter_inner=0\n",
        "            fn_counter_inner=0\n",
        "\n",
        "            annotated_mention_list=[]\n",
        "            output_mentions_list=[]\n",
        "            tweet_level_candidate_list=str(column_annot_holder[idx]).split(';')\n",
        "            for tweet_level_candidates in tweet_level_candidate_list:\n",
        "                sentence_level_cand_list= tweet_level_candidates.split(',')\n",
        "                annotated_mention_list.extend(sentence_level_cand_list)\n",
        "            annotated_mention_list=list(map(lambda element: element.lower().strip(),annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: quickRegex.match(element), annotated_mention_list))\n",
        "            annotated_mention_list=list(filter(lambda element: ((element !='')&(element !='nan')), annotated_mention_list))\n",
        "\n",
        "            for lst in column_candidates_holder[idx]:\n",
        "                output_mentions_list.extend(lst)\n",
        "\n",
        "            output_mentions_list=list(filter(lambda element: (element !=''), output_mentions_list))\n",
        "            output_mentions_list=list(map(lambda element: element.lower(), output_mentions_list))\n",
        "            total_annotation+=len(annotated_mention_list)\n",
        "\n",
        "            print(idx, annotated_mention_list,output_mentions_list)\n",
        "            output_str+=','.join(output_mentions_list)+'\\n'\n",
        "\n",
        "            all_annotations.extend(annotated_mention_list)\n",
        "            all_mentions.extend(output_mentions_list)\n",
        "\n",
        "            total_mentions+=len(output_mentions_list)\n",
        "            all_postitive_counter_inner=len(output_mentions_list)\n",
        "\n",
        "            while(annotated_mention_list):\n",
        "                if(len(output_mentions_list)):\n",
        "                    annotated_candidate= self.normalize(annotated_mention_list.pop())\n",
        "                    if(annotated_candidate in output_mentions_list):\n",
        "                        output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
        "                        tp_counter_inner+=1\n",
        "                    else:\n",
        "                        unrecovered_annotated_mention_list.append(annotated_candidate)\n",
        "                else:\n",
        "                    unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
        "                    break\n",
        "\n",
        "            # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
        "            fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
        "            fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
        "\n",
        "            # print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
        "\n",
        "            self.true_positive_count+=tp_counter_inner\n",
        "            self.false_positive_count+=fp_counter_inner\n",
        "            self.false_negative_count+=fn_counter_inner\n",
        "\n",
        "        print('TP||||FP||||FN')\n",
        "        print(self.true_positive_count,self.false_positive_count,self.false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "        precision=(self.true_positive_count)/(self.true_positive_count+self.false_positive_count)\n",
        "        recall=(self.true_positive_count)/(self.true_positive_count+self.false_negative_count)\n",
        "        f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        # all_annotations=set(all_annotations)\n",
        "        # all_mentions=set(all_mentions)\n",
        "        \n",
        "        # true_positive_count= len(all_annotations.intersection(all_mentions))\n",
        "        # false_positive_count=len(all_mentions-all_annotations)\n",
        "        # false_negative_count=len(all_annotations-all_mentions)\n",
        "        # total_mentions=len(all_mentions)\n",
        "        # total_annotation=len(all_annotations)\n",
        "\n",
        "\n",
        "        # print(true_positive_count,false_positive_count,false_negative_count,total_mentions,total_annotation)\n",
        "\n",
        "\n",
        "        # true_positive_count_IPQ=true_positive_count\n",
        "        # false_positive_count_IPQ = false_positive_count\n",
        "        # false_negative_count_IPQ= false_negative_count\n",
        "        # total_mention_count_IPQ=total_mentions\n",
        "\n",
        "\n",
        "        # tp_count=0\n",
        "        # tm_count=0\n",
        "        # fp_count=0\n",
        "        # fn_count=0\n",
        "\n",
        "        # for idx,tup in enumerate(self.accuracy_tuples_prev_batch):\n",
        "        #     # print(idx,tup)\n",
        "        #     tp_count+=tup[0]\n",
        "        #     tm_count+=tup[1]\n",
        "        #     fp_count+=tup[2]\n",
        "        #     fn_count+=tup[3]\n",
        "\n",
        "\n",
        "\n",
        "        # tp_count+=true_positive_count_IPQ\n",
        "        # tm_count+=total_mention_count_IPQ\n",
        "        # fp_count+=false_positive_count_IPQ\n",
        "        # fn_count+=false_negative_count_IPQ\n",
        "\n",
        "        # precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
        "        # recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
        "        # f_measure=2*(precision*recall)/(precision+recall)\n",
        "\n",
        "        file1.write(output_str)\n",
        "        file1.close()\n",
        "\n",
        "\n",
        "\n",
        "        self.accuracy_vals=(z_score_threshold,f_measure,precision,recall)\n",
        "\n",
        "        print('Precision:',precision)\n",
        "        print('Recall:',recall)\n",
        "        print('F1:',f_measure)\n",
        "\n",
        "        # print('z_score:', z_score_threshold , 'precision: ',precision,'recall: ',recall,'f measure: ',f_measure)\n",
        "        # print('trupe positive: ',tp_count, 'false positive: ',fp_count,'false negative: ', fn_count,'total mentions: ', tm_count)\n",
        "\n",
        "\n",
        "        # input_to_eval[\"tp\"]=true_positive_holder\n",
        "        # input_to_eval[\"fn\"]=false_negative_holder\n",
        "        # input_to_eval['fp']= false_positive_holder\n",
        "        # input_to_eval[\"total_mention\"]=total_mention_holder\n",
        "\n",
        "        # input_to_eval[\"ambigious_not_in_annot\"]=ambigious_not_in_annotation_holder\n",
        "        # input_to_eval[\"inverted_loss\"]=input_to_eval[\"tp\"]/( input_to_eval[\"fn\"]+input_to_eval[\"ambigious_not_in_annot\"])\n",
        "\n",
        "        return input_to_eval\n",
        "\n",
        "\n",
        "    def recall_correction(self,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder):\n",
        "\n",
        "        corrected_phase2_candidates_holder=[]\n",
        "        index_outer=0\n",
        "        for candidates in phase2_candidates_holder:\n",
        "            unnormalized_candidates=phase2_unnormalized_candidates_holder[index_outer]\n",
        "            corrected_phase2_candidates=[]\n",
        "            for idx, candidate in enumerate(candidates):\n",
        "                unnormalized_candidate=unnormalized_candidates[idx]\n",
        "                # if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates))):\n",
        "                if((candidate in self.partition_dict.keys())&((candidate in self.infrequent_candidates)|(candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))):   #do this only for 3K tweets\n",
        "                    #print(candidate, self.partition_dict[candidate])\n",
        "                    corrected_phase2_candidates.extend(self.partition_dict[candidate])\n",
        "                else:\n",
        "                    if(((candidate in self.bad_candidates)|(candidate in self.ambiguous_candidates))&(candidate=='us')&(unnormalized_candidate=='US')):\n",
        "                        # print(index_outer)\n",
        "                        candidate=unnormalized_candidate\n",
        "                    # if((len(candidate.strip().strip(string.punctuation).split())>1)&(candidate.strip().strip(string.punctuation).split()[0].lower() in ['a','an','the'])):\n",
        "                    #     candidate = (' '.join(candidate.strip().strip(string.punctuation).split()[1:])).strip()\n",
        "                    corrected_phase2_candidates.append(candidate)\n",
        "            corrected_phase2_candidates_holder.append(copy.deepcopy(corrected_phase2_candidates))\n",
        "            index_outer+=1\n",
        "\n",
        "        \n",
        "        #print(corrected_phase2_candidates_holder)\n",
        "        data_frame_holder['2nd Iteration Candidates']=corrected_phase2_candidates_holder\n",
        "\n",
        "        return corrected_phase2_candidates_holder,data_frame_holder                  \n",
        "\n",
        "\n",
        "    def erode_article(self, entity_string):\n",
        "        if((len(entity_string.strip().strip(string.punctuation).split())>1)&(entity_string.lower().strip().strip(string.punctuation).split()[0] in ['a','an','the'])):\n",
        "            # print(entity_string)\n",
        "            entity_string = ' '.join(entity_string.strip().strip(string.punctuation).split()[1:])\n",
        "            # print(entity_string)\n",
        "        return entity_string.strip()\n",
        "\n",
        "\n",
        "    def set_completeness_in_tweet_frame(self,data_frame_holder,candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,correction_flag):\n",
        "        #print(candidate_featureBase_DF.head())\n",
        "        good_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"g\"].candidate.tolist()\n",
        "        bad_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"b\"].candidate.tolist()\n",
        "\n",
        "        merged_g_b= bad_candidates+good_candidates\n",
        "\n",
        "        #candidate_featureBase_DF.to_csv(\"cf_before_labeling_comp.csv\", sep=',', encoding='utf-8')\n",
        "        ambiguous_candidates=candidate_featureBase_DF[candidate_featureBase_DF.status==\"a\"].candidate.tolist()\n",
        "\n",
        "        if(correction_flag):\n",
        "            phase2_candidates_holder,data_frame_holder=self.recall_correction(candidate_featureBase_DF,phase2_candidates_holder,phase2_unnormalized_candidates_holder,data_frame_holder)\n",
        "\n",
        "         \n",
        "\n",
        "        \n",
        "        truth_vals=[False if any(x not in merged_g_b for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        intermediate_output_mentions=[list(filter(lambda candidate: ((candidate in good_candidates))|(candidate=='US'), list1)) for list1 in phase2_candidates_holder]\n",
        "\n",
        "        output_mentions=[list(map(lambda candidate: self.erode_article(candidate), list1)) for list1 in intermediate_output_mentions]\n",
        "\n",
        "        # truth_vals=[False if any(x in ambiguous_candidates for x in list1) else True for list1 in phase2_candidates_holder]\n",
        "\n",
        "        # for list1 in phase2_candidates_holder:\n",
        "        #     if any(x in ambiguous_candidates  for x in list1):\n",
        "        #         truth_vals.append(False)\n",
        "        #     else:\n",
        "        #         truth_vals.append(True)\n",
        " \n",
        "\n",
        "\n",
        "        #print(truth_vals)\n",
        "        completeness_series = pd.Series( (v for v in truth_vals) )\n",
        "        output_mentions_series = pd.Series( (v for v in output_mentions) )\n",
        "\n",
        "\n",
        "        data_frame_holder['output_mentions']=output_mentions_series\n",
        "        data_frame_holder['completeness']=completeness_series\n",
        "        data_frame_holder[\"current_minus_entry\"]=self.counter-data_frame_holder['entry_batch']\n",
        "\n",
        "        return data_frame_holder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_readable_labels(self,candidate_featureBase_DF):\n",
        "\n",
        "        #candidate_featureBase_DF['status'] = candidate_featureBase_DF['probability'].apply(lambda x: set(x).issubset(good_candidates))\n",
        "        candidate_featureBase_DF['status']='ne'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.65]='g'\n",
        "        candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.65)] = 'a'\n",
        "        candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
        "\n",
        "        return candidate_featureBase_DF\n",
        "\n",
        "\n",
        "\n",
        "    def normalize(self,word):\n",
        "        strip_op=word\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
        "        #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        if strip_op.endswith(\"'s\"):\n",
        "            li = strip_op.rsplit(\"'s\", 1)\n",
        "            return ''.join(li)\n",
        "        elif strip_op.endswith(\"’s\"):\n",
        "            li = strip_op.rsplit(\"’s\", 1)\n",
        "            return ''.join(li)\n",
        "        else:\n",
        "            return strip_op\n",
        "        #return strip_op\n",
        "\n",
        "    \n",
        "    def isSubstring(self,to_increase_element,id_to_incr,comparison_holder,phase1_holder_holder_copy):\n",
        "        combined_list=comparison_holder[id_to_incr]+phase1_holder_holder_copy[id_to_incr]\n",
        "\n",
        "        for idx,val in enumerate(comparison_holder[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[5]) and to_increase_element[5] != val[5]):\n",
        "                    return True\n",
        "        for idx,val in enumerate(phase1_holder_holder_copy[id_to_incr]):\n",
        "            if((to_increase_element[0] in val[0]) and to_increase_element[0] != val[0]):\n",
        "                if((to_increase_element[5] in val[2]) and to_increase_element[5] != val[2]):\n",
        "                    return True   \n",
        "                \n",
        "        return False\n",
        "\n",
        "\n",
        "    def calculate_pmi(self,big,x1,x2,total):\n",
        "        big__= float(big/total)\n",
        "        x1__=float(x1/total)\n",
        "        x2__=float(x2/total)\n",
        "        pmi= math.log(big__/(x1__*x2__),2.71828182845)\n",
        "        pklv=big__*pmi\n",
        "        #return (1/(1+math.exp(-1*pmi)))\n",
        "        npmi= pmi/(-1.0*(math.log(big__,2.71828182845)))\n",
        "        return npmi,pklv\n",
        "        #return pklv\n",
        "\n",
        "    def multiSlice(self,s,cutpoints,good_candidates):\n",
        "        k = len(cutpoints)\n",
        "        multislices=[]\n",
        "        if k == 0:\n",
        "            curr_candidate=self.normalize(' '.join(s))\n",
        "\n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]        \n",
        "        else:\n",
        "            \n",
        "            curr_candidate=self.normalize(' '.join(s[:cutpoints[0]]))\n",
        "            alt_list=[curr_candidate]\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices = [curr_candidate]\n",
        "\n",
        "            alt_list.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1))\n",
        "            multislices.extend(self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) for i in range(k-1) if self.normalize(' '.join(s[cutpoints[i]:cutpoints[i+1]])) in good_candidates)\n",
        "\n",
        "            curr_candidate=self.normalize(' '.join(s[cutpoints[k-1]:]))\n",
        "            alt_list.append(curr_candidate)\n",
        "            \n",
        "            if(curr_candidate in good_candidates):\n",
        "                multislices.append(curr_candidate)\n",
        "            # print('::',alt_list)\n",
        "        return multislices\n",
        "\n",
        "\n",
        "\n",
        "    def get_substring_candidates(self,candidate_words,good_candidates):\n",
        "        n = len(candidate_words)\n",
        "        all_partitions=[]\n",
        "        all_partitions_length=[]\n",
        "        cuts = list(range(1,n))\n",
        "        for k in range(n):\n",
        "            # all_partitions_inner=[]\n",
        "            partition_list=[]\n",
        "            partition_length_list=[]\n",
        "            for cutpoints in itertools.combinations(cuts,k):\n",
        "                ret_list=self.multiSlice(candidate_words,cutpoints,good_candidates)\n",
        "                if(ret_list):\n",
        "                    partition_length=sum([len(elem.split()) for elem in ret_list])\n",
        "                    # print('==',ret_list,partition_length)\n",
        "                    if(partition_length==len(candidate_words)):\n",
        "                        return ret_list\n",
        "                    partition_list.append(ret_list)\n",
        "                    partition_length_list.append(partition_length)\n",
        "                    # yield ret_list\n",
        "            # print('------')\n",
        "            if(partition_length_list):\n",
        "                max_index=partition_length_list.index(max(partition_length_list))\n",
        "                all_partitions.append(partition_list[max_index])\n",
        "                all_partitions_length.append(partition_length_list[max_index])\n",
        "        # print(all_partitions)\n",
        "        if(all_partitions_length):\n",
        "            max_index=all_partitions_length.index(max(all_partitions_length))\n",
        "            # print(all_partitions[max_index])\n",
        "            return all_partitions[max_index]\n",
        "        else:\n",
        "            return []\n",
        "    \n",
        "\n",
        "    def verify(self, subsequence, CTrie):\n",
        "        return CTrie.__contains__(subsequence)\n",
        "\n",
        "\n",
        "\n",
        "    def check_sequence(self, sequence, l, CTrie):\n",
        "        result=[]\n",
        "        subsequence_length=l\n",
        "        while(subsequence_length>0):\n",
        "            shift=len(sequence)-subsequence_length\n",
        "            verified_subsequence=[]\n",
        "            verified=False\n",
        "            for i in range(0,shift+1):\n",
        "                list1=sequence[i:(i+subsequence_length)]\n",
        "                text=' '.join(str(e[0]) for e in list1)\n",
        "                subsequence=(self.normalize(text)).split()\n",
        "                #print(\"search for\", subsequence)\n",
        "                if self.verify(subsequence, CTrie):\n",
        "                    verified_subsequence.append(i)\n",
        "                    verified_subsequence.append(i+subsequence_length)\n",
        "                    #print(subsequence)\n",
        "                    #print(subsequence,[(verified_subsequence[0]-0),(int(sequence[-1][1])-verified_subsequence[1])])\n",
        "                    verified=True\n",
        "                    break\n",
        "            if(verified):\n",
        "                result.append(sequence[verified_subsequence[0]:verified_subsequence[1]])\n",
        "                if(verified_subsequence[0]-0)>0:\n",
        "                    subequence_to_check=sequence[0:verified_subsequence[0]]\n",
        "                    #since tokens before the starting position of the verified subsequence have already been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length-1))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                if(int(sequence[-1][1])-verified_subsequence[1])>0:\n",
        "                    subequence_to_check=sequence[(verified_subsequence[1]):]\n",
        "                    #since tokens following the end position of the verified subsequence have not been checked for subsequences of this length\n",
        "                    partition_length=min(len(subequence_to_check),(subsequence_length))\n",
        "                    #print(subequence_to_check)\n",
        "                    lst=self.check_sequence(subequence_to_check,partition_length, CTrie)\n",
        "                    if(lst):\n",
        "                        result.extend(lst)\n",
        "                return result\n",
        "            else:\n",
        "                subsequence_length-=1\n",
        "        return result\n",
        "\n",
        "    # def flatten(self,mylist, outlist,ignore_types=(str, bytes, int, ne.NE_candidate)):\n",
        "    def flatten(self,mylist, outlist,ignore_types=(str, bytes, int)):\n",
        "    \n",
        "        if mylist !=[]:\n",
        "            for item in mylist:\n",
        "                #print not isinstance(item, ne.NE_candidate)\n",
        "                if isinstance(item, list) and not isinstance(item, ignore_types):\n",
        "                    self.flatten(item, outlist)\n",
        "                else:\n",
        "                    # if isinstance(item,ne.NE_candidate):\n",
        "                    #     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
        "                    #     item.reset_length()\n",
        "                    # else:\n",
        "                    if type(item)!= int:\n",
        "                        item=item.strip(' \\t\\n\\r')\n",
        "                    outlist.append(item)\n",
        "        return outlist\n",
        "\n",
        "\n",
        "    def getWords(self, sentence):\n",
        "        tempList=[]\n",
        "        tempWordList=sentence.split()\n",
        "        p_dots= re.compile(r'[.]{2,}')\n",
        "        #print(tempWordList)\n",
        "        for word in tempWordList:\n",
        "            temp=[]\n",
        "            \n",
        "            if \"(\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: '('+elem, temp))\n",
        "            elif \")\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
        "                if(temp):\n",
        "                    temp=list(map(lambda elem: elem+')', temp))\n",
        "                # temp.append(temp1[-1])\n",
        "            # elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
        "            #     temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
        "            #     if(temp1):\n",
        "            #         temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
        "            #     temp.append(temp1[-1])\n",
        "            elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
        "                #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "            elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
        "                temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
        "                if(temp1):\n",
        "                    temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
        "                temp.append(temp1[-1])\n",
        "                #print(index, temp)\n",
        "            # elif \"...\" in word:\n",
        "            #     #print(\"here\")\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"...\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"...\")):\n",
        "            #             temp=list(map(lambda elem: elem+'...', temp))\n",
        "            #         else:\n",
        "            #            temp=list(map(lambda elem: elem+'...', temp[:-1]))+[temp[-1]]\n",
        "            #     # temp.append(temp1[-1])\n",
        "            # elif \"..\" in word:\n",
        "            #     temp=list(filter(lambda elem: elem!='',word.split(\"..\")))\n",
        "            #     if(temp):\n",
        "            #         if(word.endswith(\"..\")):\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp))\n",
        "            #         else:\n",
        "            #             temp=list(map(lambda elem: elem+'..', temp[:-1]))+[temp[-1]]\n",
        "            #     #temp.append(temp1[-1])\n",
        "            elif (list(p_dots.finditer(word))):\n",
        "                matched_spans= list(p_dots.finditer(word)) \n",
        "                temp=[]\n",
        "                next_string_start=0\n",
        "                for matched_span in matched_spans:\n",
        "                    matched_start=matched_span.span()[0]\n",
        "                    this_excerpt=word[next_string_start:matched_start]\n",
        "                    if(this_excerpt):\n",
        "                        temp.append(this_excerpt)\n",
        "                    next_string_start=matched_span.span()[1]\n",
        "                if(next_string_start<len(word)):\n",
        "                    last_excerpt=word[next_string_start:]\n",
        "                    if(last_excerpt):\n",
        "                        temp.append(last_excerpt)\n",
        "            elif \"…\" in word:\n",
        "                temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
        "                if(temp):\n",
        "                    if(word.endswith(\"…\")):\n",
        "                        temp=list(map(lambda elem: elem+'…', temp))\n",
        "                    else:\n",
        "                        temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
        "            else:\n",
        "                #if word not in string.punctuation:\n",
        "                temp=[word]\n",
        "            if(temp):\n",
        "                tempList.append(temp)\n",
        "        tweetWordList=self.flatten(tempList,[])\n",
        "        return tweetWordList\n",
        "\n",
        "    def get_Candidates(self, sequence, CTrie,flag):\n",
        "        #flag: debug_flag\n",
        "        candidateList=[]\n",
        "        left=0\n",
        "        start_node=CTrie\n",
        "        last_cand=\"NAN\"\n",
        "        last_cand_substr=\"\"\n",
        "        reset=False\n",
        "        right=0\n",
        "        while (right < len(sequence)):\n",
        "            # if(flag):\n",
        "            #     print(right)\n",
        "            if(reset):\n",
        "                start_node=CTrie\n",
        "                last_cand_substr=\"\"\n",
        "                left=right\n",
        "            curr_text=sequence[right][0]\n",
        "            curr_pos=[sequence[right][1]]\n",
        "            #normalized curr_text\n",
        "            curr=self.normalize(sequence[right][0])\n",
        "            cand_str=self.normalize(last_cand_substr+\" \"+curr)\n",
        "            cand_str_wPunct=(last_cand_substr+\" \"+curr_text).lower()\n",
        "            last_cand_sequence=sequence[left:(right+1)]\n",
        "            last_cand_text=' '.join(str(e[0]) for e in last_cand_sequence)\n",
        "            last_cand_text_norm=self.normalize(' '.join(str(e[0]) for e in last_cand_sequence))\n",
        "            if(flag):\n",
        "                print(\"==>\",cand_str,last_cand_text_norm)\n",
        "            if((cand_str==last_cand_text_norm)&((curr in start_node.path.keys())|(curr_text.lower() in start_node.path.keys()))):\n",
        "            #if (((curr in start_node.path.keys())&(cand_str==last_cand_text_norm))|(curr_text.lower() in start_node.path.keys())):\n",
        "                if flag:\n",
        "                    print(\"=>\",cand_str,last_cand_text)\n",
        "                reset=False\n",
        "                if (curr_text.lower() in start_node.path.keys()):\n",
        "                    if (start_node.path[curr_text.lower()].value_valid):\n",
        "                        last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                        last_cand_batch=start_node.path[curr_text.lower()].feature_list[-1]\n",
        "                        last_cand=last_cand_text\n",
        "                    elif(curr in start_node.path.keys()):\n",
        "                        if ((start_node.path[curr].value_valid)):\n",
        "                            last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                            last_cand=last_cand_text\n",
        "                            last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                        else:\n",
        "                            if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                                #print(\"hehe\",cand_str)\n",
        "                                right=left\n",
        "                                reset=True\n",
        "                    else:\n",
        "                        if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                            #print(\"hehe\",cand_str)\n",
        "                            right=left\n",
        "                            reset=True\n",
        "                elif ((start_node.path[curr].value_valid)&(cand_str==last_cand_text_norm)):\n",
        "                    # if flag:\n",
        "                    #     print(\"==\",last_cand_text)\n",
        "                    last_cand_pos=[e[1] for e in last_cand_sequence]\n",
        "                    last_cand=last_cand_text\n",
        "                    last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                else:\n",
        "                    if((right==(len(sequence)-1))&(last_cand==\"NAN\")&(left<right)):\n",
        "                        #print(\"hehe\",cand_str)\n",
        "                        right=left\n",
        "                        reset=True\n",
        "                if((curr_text.lower() in start_node.path.keys())&(cand_str==last_cand_text_norm)):\n",
        "                    start_node=start_node.path[curr_text.lower()]\n",
        "                    last_cand_substr=cand_str_wPunct\n",
        "                else:\n",
        "                    start_node=start_node.path[curr]\n",
        "                    last_cand_substr=cand_str\n",
        "            else:\n",
        "                #print(\"=>\",cand_str,last_cand_text)\n",
        "                if(last_cand!=\"NAN\"):\n",
        "                    candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "                    last_cand=\"NAN\"\n",
        "                    if(start_node!=CTrie):\n",
        "                        start_node=CTrie\n",
        "                        last_cand_substr=\"\"\n",
        "                        if curr in start_node.path.keys():\n",
        "                            # if(flag):\n",
        "                            #     print(\"here\",curr)\n",
        "                            reset=False\n",
        "                            if start_node.path[curr].value_valid:\n",
        "                                last_cand_text=curr_text\n",
        "                                last_cand_pos=curr_pos\n",
        "                                last_cand=last_cand_text\n",
        "                                last_cand_batch=start_node.path[curr].feature_list[-1]\n",
        "                            left=right\n",
        "                            start_node=start_node.path[curr]\n",
        "                            last_cand_substr=curr\n",
        "                        else:\n",
        "                            reset=True\n",
        "                    else:\n",
        "                        reset=True\n",
        "                else:\n",
        "                    if(left<right):\n",
        "                        # if(flag):\n",
        "                        #     print(sequence[(left+1):(right+1)])\n",
        "                        #candidateList.extend(self.get_Candidates(sequence[(left+1):(right+1)], CTrie, flag))\n",
        "                        right=left\n",
        "                        # if(flag):\n",
        "                        #     print(\"++\",right)\n",
        "                    reset=True\n",
        "            right+=1\n",
        "        # if(flag):\n",
        "        #     print(last_cand)\n",
        "        if(last_cand!=\"NAN\"):\n",
        "            candidateList.append((last_cand,last_cand_pos,last_cand_batch))\n",
        "        return candidateList\n",
        "\n",
        "\n",
        "    def append_rows(self,df_holder):\n",
        "    \n",
        "        df = pd.DataFrame(df_holder)\n",
        "        #self.data_frame_holder=self.data_frame_holder.append(df,ignore_index=True)\n",
        "        #self.data_frame_holder=self.data_frame_holder.reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def join_token_tuples(self,list_of_tuples):\n",
        "        #print(string.punctuation)\n",
        "        combined_str=(' '.join(tuple[0] for tuple in list_of_tuples)).lstrip(string.punctuation).rstrip(string.punctuation).strip()\n",
        "        combined_pos='*'.join(str(tuple[1]) for tuple in list_of_tuples)\n",
        "        combined_tuple=(combined_str,combined_pos,list_of_tuples[0][2],list_of_tuples[0][3],list_of_tuples[0][4],list_of_tuples[0][5],list_of_tuples[0][6])\n",
        "        return combined_tuple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def all_capitalized(self,candidate):\n",
        "        strip_op=candidate\n",
        "        strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip())\n",
        "        strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
        "        strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
        "        prep_article_list=prep_list+article_list+self.phase2stopwordList\n",
        "        word_list=strip_op.split()\n",
        "        for i in range(len(word_list)):\n",
        "            word=word_list[i]\n",
        "            if((word[0].isupper())|(word[0].isdigit())):\n",
        "                continue\n",
        "            else:\n",
        "                if(word in prep_article_list):\n",
        "                    if (i!=0):\n",
        "                        continue\n",
        "                    else:\n",
        "                        return False\n",
        "                elif(word in conjoiner):\n",
        "                    continue\n",
        "                else:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def check_feature_update(self, candidate_tuple,non_discriminative_flag):\n",
        "        #print(candidate_tuple)\n",
        "        if(non_discriminative_flag):\n",
        "            return 7\n",
        "        candidateText=candidate_tuple[0]\n",
        "        position=candidate_tuple[1]\n",
        "        word_list=candidateText.split()\n",
        "        if candidateText.islower():\n",
        "            return 6\n",
        "        elif candidateText.isupper():\n",
        "            return 5\n",
        "        elif (len(word_list)==1):\n",
        "            #start-of-sentence-check\n",
        "            if self.all_capitalized(candidateText):\n",
        "                if(int(position[0])==0):\n",
        "                    return 4\n",
        "                else:\n",
        "                    return 2\n",
        "            else:\n",
        "                return 3\n",
        "        else:\n",
        "            if(self.all_capitalized(candidateText)):\n",
        "                return 2\n",
        "            else:\n",
        "                return 3\n",
        "\n",
        "\n",
        "    def update_Candidatedict(self,candidate_tuple,non_discriminative_flag,contextual_embedding_vector):\n",
        "        candidateText=candidate_tuple[0]\n",
        "        normalized_candidate=self.normalize(candidateText)\n",
        "        # print('adding:',normalized_candidate)\n",
        "\n",
        "        feature_list=[]\n",
        "        if(normalized_candidate in self.CandidateBase_dict.keys()):\n",
        "            feature_list=self.CandidateBase_dict[normalized_candidate]\n",
        "        else:\n",
        "            # feature_list=[0]*9 # only syntax\n",
        "            # feature_list=[0]*777 # context embedding: 768\n",
        "            feature_list=[0]*309 # context embedding: 300\n",
        "\n",
        "            feature_list[0]=self.counter\n",
        "            feature_list[1]=len(normalized_candidate.split())\n",
        "\n",
        "        #syntax_feature to update\n",
        "        feature_to_update=self.check_feature_update(candidate_tuple,non_discriminative_flag)\n",
        "        feature_list[feature_to_update]+=1\n",
        "\n",
        "        # add up the context embedding features\n",
        "        # print(len(contextual_embedding_vector))\n",
        "        feature_list[8:-1]= np.add(feature_list[8:-1],contextual_embedding_vector).tolist()\n",
        "\n",
        "        #increment cumulative frequency\n",
        "        feature_list[-1]+=1\n",
        "        self.CandidateBase_dict[normalized_candidate]=feature_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def extract(self,tweetBaseInput,CTrie,phase2stopwordList,new_or_old):\n",
        "\n",
        "\n",
        "        if(self.counter==0):\n",
        "            #output_queue\n",
        "            self.data_frame_holder_OQ=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.incomplete_tweets=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.not_reintroduced=pd.DataFrame([], columns=['index','entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.CandidateBase_dict= {}\n",
        "            self.ambiguous_candidate_distanceDict_prev={}\n",
        "            self.partition_dict={}\n",
        "            self.good_candidates=[]\n",
        "            self.bad_candidates=[]\n",
        "            self.ambiguous_candidates=[]\n",
        "\n",
        "            self.aggregator_incomplete_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            self.just_converted_tweets=pd.DataFrame([], columns=['index', 'entry_batch', 'tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "            #self.data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates'])\n",
        "            self.raw_tweets_for_others=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "\n",
        "            self.accuracy_tuples_prev_batch=[]\n",
        "            self.accuracy_vals=[]\n",
        "            \n",
        "            #frequency_w_decay related information\n",
        "            self.ambiguous_candidates_reintroduction_dict={}\n",
        "\n",
        "            #### other systems\n",
        "            self.accuracy_vals_stanford=[]\n",
        "            self.accuracy_vals_opencalai=[]\n",
        "            self.accuracy_vals_ritter=[]\n",
        "            self.accuracy_vals_neuroner=[]\n",
        "\n",
        "            self.number_of_seen_tweets_per_batch=[]\n",
        "        self.phase2stopwordList=phase2stopwordList\n",
        "        self.number_of_seen_tweets_per_batch.append(len(tweetBaseInput))\n",
        "\n",
        "\n",
        "        #data_frame_holder=pd.DataFrame([], columns=['index','entry_batch','tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','phase1Candidates', '2nd Iteration Candidates', '2nd Iteration Candidates Unnormalized'])\n",
        "        phase1_holder_holder=[]\n",
        "        phase2_candidates_holder=[]\n",
        "        phase2_unnormalized_candidates_holder=[]\n",
        "        df_holder=[]\n",
        "        \n",
        "        #candidateBase_holder=[]\n",
        "\n",
        "        #this has to be changed to an append function since IPQ already has incomplete tweets from prev batch  \n",
        "        #print(len(tweetBaseInput))\n",
        "        #immediate_processingQueue = pd.concat([self.incomplete_tweets,TweetBase ])\n",
        "        #immediate_processingQueue.to_csv(\"impq.csv\", sep=',', encoding='utf-8')\n",
        "        \n",
        "\n",
        "\n",
        "        #print('In Phase 2',len(immediate_processingQueue))\n",
        "        #immediate_processingQueue=immediate_processingQueue.reset_index(drop=True)\n",
        "        combined_list_here=([]+list(cachedStopWords)+chat_word_list+day_list+month_list+article_list+prep_list)\n",
        "        combined_list_filtered=list(filter(lambda word: word not in (prep_list+article_list+month_list+self.phase2stopwordList), combined_list_here))\n",
        "        #--------------------------------------PHASE II---------------------------------------------------\n",
        "        for index, row in tweetBaseInput.iterrows():\n",
        "\n",
        "            #phase 1 candidates for one sentence\n",
        "            phase1_holder=[]\n",
        "\n",
        "            tweetText=str(row['TweetSentence'])\n",
        "            tweetWordList = row['tweetwordList']\n",
        "            sentID=str(row['sentID'])\n",
        "            tweetID=str(row['tweetID'])\n",
        "            phase1Candidates=str(row['phase1CandidatesWPositions'])\n",
        "            batch=int(row['entry_batch'])\n",
        "            contextual_embeddings_dict= {ind: embedding for ind, embedding in enumerate(row['contextual_embeddings'])}\n",
        "            \n",
        "            # print('====',tweetID,sentID)\n",
        "            # print('tweetWordList:',tweetWordList)\n",
        "            \n",
        "            non_discriminative_flag=False\n",
        "            phase1CandidatesList=[]\n",
        "\n",
        "            # print('phase1Candidates:',phase1Candidates)\n",
        "\n",
        "            if (phase1Candidates !='nan'):\n",
        "                phase1Raw=phase1Candidates.split(\"||\")\n",
        "                phase1Raw = list(filter(None, phase1Raw))\n",
        "\n",
        "\n",
        "                for entities_with_loc in phase1Raw:\n",
        "                    entity_to_store=entities_with_loc.split(\"::\")[0]\n",
        "                    #print(entity_to_store)\n",
        "                    position=entities_with_loc.split(\"::\")[1]\n",
        "                    #print(position)\n",
        "                    phase1_holder.append((entity_to_store,position))\n",
        "                    phase1_holder.clear()\n",
        "                    phase1CandidatesList.append(entity_to_store.lower())\n",
        "\n",
        "                phase1_holder_holder.append(copy.deepcopy(phase1_holder))\n",
        "                \n",
        "\n",
        "            else:\n",
        "                non_discriminative_flag=True\n",
        "                phase1_holder_holder.append([])\n",
        "\n",
        "            \n",
        "            # tweetWordList=self.getWords(tweetText)\n",
        "            # tweetWordList= [(token,idx) for idx,token in enumerate(tweetWordList)]\n",
        "            \n",
        "            tweetWordList_stopWords=list(filter (lambda word: ((((word[0].strip()).strip(string.punctuation)).lower() in combined_list_filtered)|(word[0].strip() in string.punctuation)|(word[0].startswith('@'))|(word[0].startswith('#'))), tweetWordList))\n",
        "\n",
        "            # phase 2 candidate tuples without stopwords for a sentence\n",
        "            c=[(y[0],str(y[1]),tweetID,sentID,'ne',batch,time) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "            #c=[(y[0],str(y[1])) for y  in tweetWordList if y not in tweetWordList_stopWords ]\n",
        "\n",
        "            \n",
        "            sequences=[]\n",
        "            for k, g in groupby(enumerate(c), lambda element: element[0]-int(element[1][1])):\n",
        "                sequences.append(list(map(itemgetter(1), g)))\n",
        "\n",
        "            # print('phaseII candidates:')\n",
        "            ne_candidate_list=[]\n",
        "            for sequence in sequences:\n",
        "                seq_candidate_list=self.get_Candidates(sequence, CTrie,False)\n",
        "                if(seq_candidate_list):\n",
        "                    for candidate_tuple in seq_candidate_list:\n",
        "                        # print(candidate_tuple)\n",
        "\n",
        "                        # extract candidate token embeddings\n",
        "                        candidate_token_embeddings = torch.stack([contextual_embeddings_dict[int(position)] for position in candidate_tuple[1]])\n",
        "                        # print('candidate with token_embeddings:',candidate_tuple[0],candidate_token_embeddings.shape,len(candidate_tuple[1]))\n",
        "\n",
        "                        # !!necessary because this function during training receives [1,n,768] tensors that it squeezes; so might screw up 1-token sentences\n",
        "                        candidate_embedding = self.entity_phrase_embedder.getEmbedding(candidate_token_embeddings.unsqueeze(0))\n",
        "                        # print('candidate_embedding:',candidate_embedding.shape)\n",
        "\n",
        "                        #inserts into CandidateBase: Syntax and Context Feature Setting\n",
        "                        if not ((float(batch)<self.counter)&(candidate_tuple[-1]<self.counter)):\n",
        "                            self.update_Candidatedict(candidate_tuple,non_discriminative_flag,candidate_embedding.tolist())\n",
        "\n",
        "                    ne_candidate_list.extend(seq_candidate_list)\n",
        "\n",
        "            phase2_candidates=[self.normalize(e[0]) for e in ne_candidate_list]\n",
        "            phase2_candidates_unnormalized=[e[0] for e in ne_candidate_list]\n",
        "\n",
        "            phase2_candidates_holder.append(phase2_candidates)\n",
        "            phase2_unnormalized_candidates_holder.append(phase2_candidates_unnormalized)\n",
        "\n",
        "            dict1 = {'entry_batch':batch, 'tweetID':tweetID, 'sentID':sentID, 'TweetSentence':tweetText, 'phase1Candidates':phase1CandidatesList,'2nd Iteration Candidates':phase2_candidates,'2nd Iteration Candidates Unnormalized':phase2_candidates_unnormalized}\n",
        "\n",
        "            df_holder.append(dict1)\n",
        "            \n",
        "            #-------------------------------------------------------------------END of 1st iteration: RESCAN+CANDIDATE_UPDATION-----------------------------------------------------------\n",
        "\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        # # candidate_records=pd.read_csv('data/training.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records=pd.read_csv('data/candidate_train_records_large.csv',sep =',',keep_default_na=False)\n",
        "        # candidate_records = candidate_records[['candidate','class']]\n",
        "        # candidate_list = candidate_records.candidate.values.tolist()\n",
        "\n",
        "        # candidateBase_dict_filtered={}\n",
        "        # for candidate in self.CandidateBase_dict:\n",
        "        #     if(candidate in candidate_list):\n",
        "        #         candidateBase_dict_filtered[candidate] = self.CandidateBase_dict[candidate]\n",
        "\n",
        "        # # candidateBase_dict_filtered = self.CandidateBase_dict\n",
        "\n",
        "        # candidate_featureBase_DF_filtered=pd.DataFrame.from_dict(candidateBase_dict_filtered, orient='index')\n",
        "        # candidate_featureBase_DF_filtered.columns=self.candidateBaseHeaders[1:]\n",
        "        # candidate_featureBase_DF_filtered.index.name=self.candidateBaseHeaders[0]\n",
        "        # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered.reset_index(drop=False)\n",
        "        # # candidate_featureBase_DF_filtered = candidate_featureBase_DF_filtered[candidate_featureBase_DF_filtered['cumulative'] >= 5.0]\n",
        "\n",
        "        # print('before:', len(self.CandidateBase_dict),'after:',len(candidateBase_dict_filtered))\n",
        "        \n",
        "        # candidate_records.set_index('candidate', inplace=True)\n",
        "        # candidate_featureBase_DF_filtered['class'] = candidate_featureBase_DF_filtered.apply(lambda row:candidate_records.loc[row.candidate]['class'] , axis = 1)\n",
        "\n",
        "        # # candidate_featureBase_DF_filtered['class'] = 0\n",
        "\n",
        "        # candidate_featureBase_DF_filtered.to_csv(\"data/candidate_train_records_large_300d.csv\", sep=',', encoding='utf-8')\n",
        "        # return #comment out if not collecting records for classifier training\n",
        "        # #===============comment out if not gathering records for classifier training===============\n",
        "\n",
        "        candidate_featureBase_DF = pd.DataFrame.from_dict(self.CandidateBase_dict, orient='index')\n",
        "        candidate_featureBase_DF.columns = self.candidateBaseHeaders[1:]\n",
        "        candidate_featureBase_DF.index.name = self.candidateBaseHeaders[0]\n",
        "        candidate_featureBase_DF = candidate_featureBase_DF.reset_index(drop=False)\n",
        "\n",
        "        return candidate_featureBase_DF,df_holder,phase2_candidates_holder,phase2_unnormalized_candidates_holder\n",
        "\n",
        "\n",
        "        # self.aggregator_incomplete_tweets= self.aggregator_incomplete_tweets.append(self.incomplete_tweets)\n",
        "        # self.just_converted_tweets=self.just_converted_tweets.append(just_converted_tweets_for_current_batch)\n",
        "\n",
        "\n",
        "    def finish(self):\n",
        "        return self.accuracy_vals\n",
        "\n",
        "    def finish_other_systems(self):\n",
        "        stanford_f1=[]\n",
        "        stanford_precision=[]\n",
        "        stanford_recall=[]\n",
        "        print(\"*****************************************STANFORD***********************\")\n",
        "        for i in self.accuracy_vals_stanford:\n",
        "            stanford_f1.append(i[0])\n",
        "            stanford_precision.append(i[1])\n",
        "            stanford_recall.append(i[2])\n",
        "            # print(i)\n",
        "        print('stanford_f1:', stanford_f1)\n",
        "        print('stanford_precision:', stanford_precision)\n",
        "        print('stanford_recall:', stanford_recall)\n",
        "\n",
        "        print(sum(stanford_f1)/len(stanford_f1))\n",
        "        print(sum(stanford_precision)/len(stanford_precision))\n",
        "        print(sum(stanford_recall)/len(stanford_recall))\n",
        "\n",
        "        print(\"*****************************************Opencalai***********************\")\n",
        "        opencalai_f1=[]\n",
        "        opencalai_precision=[]\n",
        "        opencalai_recall=[]\n",
        "        for i in self.accuracy_vals_opencalai:\n",
        "            opencalai_f1.append(i[0])\n",
        "            opencalai_precision.append(i[1])\n",
        "            opencalai_recall.append(i[2])\n",
        "        print('opencalai_f1:', opencalai_f1)\n",
        "        print('opencalai_precision:', opencalai_precision)\n",
        "        print('opencalai_recall:', opencalai_recall)\n",
        "\n",
        "        print(sum(opencalai_f1)/len(opencalai_f1))\n",
        "        print(sum(opencalai_precision)/len(opencalai_precision))\n",
        "        print(sum(opencalai_recall)/len(opencalai_recall))\n",
        "        print(\"*****************************************Ritter***********************\")\n",
        "        ritter_f1=[]\n",
        "        ritter_precision=[]\n",
        "        ritter_recall=[]\n",
        "        for i in self.accuracy_vals_ritter:\n",
        "            ritter_f1.append(i[0])\n",
        "            ritter_precision.append(i[1])\n",
        "            ritter_recall.append(i[2])\n",
        "        print('ritter_f1:', ritter_f1)\n",
        "        print('ritter_precision:', ritter_precision)\n",
        "        print('ritter_recall:', ritter_recall)\n",
        "\n",
        "        print(sum(ritter_f1)/len(ritter_f1))\n",
        "        print(sum(ritter_precision)/len(ritter_precision))\n",
        "        print(sum(ritter_recall)/len(ritter_recall))\n",
        "        print(\"*****************************************Neuroner***********************\")\n",
        "        neuroner_f1=[]\n",
        "        neuroner_precision=[]\n",
        "        neuroner_recall=[]\n",
        "        for i in self.accuracy_vals_neuroner:\n",
        "            neuroner_f1.append(i[0])\n",
        "            neuroner_precision.append(i[1])\n",
        "            neuroner_recall.append(i[2])\n",
        "        print('neuroner_f1:', neuroner_f1)\n",
        "        print('neuroner_precision:', neuroner_precision)\n",
        "        print('neuroner_recall:',neuroner_recall)\n",
        "\n",
        "        print(sum(neuroner_f1)/len(neuroner_f1))\n",
        "        print(sum(neuroner_precision)/len(neuroner_precision))\n",
        "        print(sum(neuroner_recall)/len(neuroner_recall))\n",
        "\n",
        "        return (self.accuracy_vals_stanford,self.accuracy_vals_opencalai,self.accuracy_vals_ritter,self.accuracy_vals_neuroner)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY3zs23wC6_3"
      },
      "source": [
        "## **External Run of BERTweet Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5tTI3fIl9yC"
      },
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "model_checkpoint = \"vinai/bertweet-base\"\n",
        "batch_size = 32\n",
        "# set_seed(42)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTip5Uxs29P8"
      },
      "source": [
        "# # reading the training set\n",
        "# f = open(\"wnut17_train.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_train = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_train))\n",
        "# df_train.to_csv(\"wnut17train.csv\", sep=',', encoding='utf-8',index=False)\n",
        "\n",
        "# # reading the validation set\n",
        "# f = open(\"wnut17_validation.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_validation = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_validation))\n",
        "# df_validation.to_csv(\"wnut17validation.csv\", sep=',', encoding='utf-8',index=False)\n",
        "\n",
        "# # reading the test set\n",
        "# f = open(\"wnut17_test.annotated\", \"r\")\n",
        "# file_text=f.read()\n",
        "# df_columns=['id','tokens','ner_tags']\n",
        "# df_holder=[]\n",
        "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
        "\n",
        "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
        "    \n",
        "#     words=[]\n",
        "#     annotations=[]\n",
        "#     lines=bio_sentence.split('\\n')\n",
        "\n",
        "#     for line in lines:\n",
        "#         if(line):\n",
        "#             tabs=line.split('\\t')\n",
        "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
        "#                 word=tabs[0]\n",
        "#                 tag=tabs[1]\n",
        "#                 # if('-' not in tabs[1]):\n",
        "#                 #     tag=tabs[1]\n",
        "#                 # else:\n",
        "#                 #     tag=tabs[1].split('-')[0]\n",
        "#                 if(word.strip().startswith('https:')):\n",
        "#                     word='@url'\n",
        "#                 words.append(word.strip())\n",
        "#                 annotations.append(tag)\n",
        "\n",
        "#     # text=words.strip()\n",
        "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
        "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
        "#     df_holder.append(df_dict)\n",
        "# df_test = pd.DataFrame(df_holder,columns=df_columns)\n",
        "# print(len(df_test))\n",
        "# df_test.to_csv(\"wnut17test.csv\", sep=',', encoding='utf-8',index=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlQEqoIJqo4l"
      },
      "source": [
        "# class TweetNERDataset(torch.utils.data.Dataset):\n",
        "\n",
        "#     def __init__(self, input, output):\n",
        "\n",
        "#         # print(data[0])\n",
        "#         # self.data = np.asarray(data)\n",
        "#         # self.output = np.asarray(output)\n",
        "\n",
        "#         datasets = load_dataset(\"wnut_17\")\n",
        "\n",
        "#         self.input = input\n",
        "#         self.output = output\n",
        "\n",
        "#         print(type(self.input),type(self.output))\n",
        "\n",
        "#     def prepare_input():\n",
        "        \n",
        "\n",
        "#     def tokenize_and_align_labels(self, example):\n",
        "        \n",
        "#         tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "#         inputId_to_token_dict={}\n",
        "#         for index, token in enumerate(example[\"tokens\"]):\n",
        "#             values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "#             for value in values:\n",
        "#                 try:\n",
        "#                     inputId_to_token_dict[value].append(index)\n",
        "#                 except KeyError:\n",
        "#                     inputId_to_token_dict[value]=[index]\n",
        "#         labels=[]\n",
        "#         for inputID in tokenized_input['input_ids']:\n",
        "#             try:\n",
        "#                 index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "#                 index_to_address=index_list.pop(0)\n",
        "\n",
        "#                 label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "#                 # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "#                 labels.append(label)\n",
        "#                 inputId_to_token_dict[inputID]=index_list\n",
        "#             except KeyError:\n",
        "#                 labels.append(-100)\n",
        "\n",
        "#         assert (len(tokenized_input['input_ids']) == len(labels))\n",
        "#         tokenized_input['labels']=labels\n",
        "        \n",
        "#         return tokenized_input\n",
        "\n",
        "#     def __len__(self):\n",
        "#         assert len(self.input) == len(self.output)\n",
        "#         return len(self.input)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         X = self.input[idx]\n",
        "#         y = self.output[idx]\n",
        "#         return X,y"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDmC8lnI-99G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "5b8cb9e711bd48d09ce31071513bd026",
            "7f9d6b0f59314f53b9e3eab07c587635",
            "6756fac5b0a7459a95bdfa2ad5ee5de0",
            "8ddf77cd445742989eb2494dbec6f25a",
            "aba23de6a075432786da2c3780442eca",
            "716f0bc95dca4a6883e7e997b8e4ccda",
            "b8531a137ffe4af9ac4d8b79953495ff",
            "15698eaf06104663aa41322c3cb92e20",
            "e47edb34bed24cda9e9d67a8b6250347",
            "e1678a37354f41eea770065f541abcbb",
            "39e06b8b546341feb241c1065023f8ac",
            "fd148556c6da4df28fabee30f23bc1d3",
            "427ad1e763994c589e2cdc1c8576de66",
            "ee77e2d51e1449bda3ebd0a49245d0bd",
            "00c6dda96fb54ba2bb44151f78bd9d40",
            "6c12f4b6f1bf40b4b40a4b46bbe0f952",
            "66e8604a2a8948c0a13ddf6691a20710",
            "904164cd8c3d463f88e6f6575adbad5c",
            "56da14bbfa1d445a9684f4ced9ccdb2d",
            "18e3c0410ce746dbb5ffee84fc63165f",
            "41ab613ef4ae49e89d921fed3c5aa720",
            "dfc0682fc9d449f5b61ae72afe6e036b",
            "c2976a5238c94e66ad8abb0a6a003efc",
            "c5ccc4874df34900b92037142860ecda",
            "b8e5f8d7426e4592b21e0b3f11340708",
            "74dcf7b1d7684d5b8009da28143dd696",
            "54879176b0ac436db6bbcf622c043f90",
            "84beb2d526684d3fa422d99bdd33be48",
            "59397b14c09e43d2983befcf94f0d45e",
            "dbb777d438494a8f8bb945bfe04f2d6f",
            "9a53a2e21b42400297672d6ad555e3aa",
            "144722fde9814011976faca369e81052",
            "245681fb5a93461bb259846ca060f3ba",
            "31b6bf87af454958a79ee0e32a9ace2a",
            "a9b0b7cdd0cb493cafbc0e8c44061713",
            "29a2c05c304c462aa43ad8fa58317878",
            "d0f4090f40b14303b810879408b1349c",
            "7dd858ddcbd6496faa6ff6e784555d58",
            "2abfdb9706e0432eb312af61dc2aaed4",
            "39f63baa36f14eaf943b45b1221cea4c",
            "5b9cf11979d043fe8fdbd4033dd22237",
            "ccb2c032758243a1ad8c6d79c2bc5b2d",
            "61013e3b08fb4b1fbae264d4abc2f156",
            "9e5b1f4091354b1585b9511e6961a0c2",
            "50c52ef5dca94b3898167b8d5be51e22",
            "bee4b963df734287bb540ed777ee0376",
            "ff04b42c9adc4c0584277db0cb81feae",
            "d8acb2b0cbb64306964cc774f4cf8d07",
            "dcea5682df834375b18b5260bcd3fc49",
            "7d2e57ee916e43d08aaa3450a3ca4a42",
            "4671ba268a7a48fc9c3d99c313466d84",
            "59c741286211469bb34428c5e91e5011",
            "7e1be016fc134c20b776f58e5fe9e644",
            "84c28d7fc3114038bb816bc6948fcc54",
            "41905aa6d89c4f6f9cb3441c63f53586",
            "f4c31fa59f6d4f01bc9686acc3642266",
            "06bea0d4a0854bd6bd095797fd9896dd",
            "8367e6048da84f978f27f0675dd3f201",
            "9316206ee0e741d5a7fbb23807139f0a",
            "9f1495eb852143f9be76de9062784681",
            "f490480f234446d5a1b0c0bcd407b3e0",
            "c03c4e9ef2f94c86b6d98c1e469778c5",
            "eba465c9b82643d2bc04c774c3e094c6",
            "9328c0fa6f1c4753a7e95a0a777f12f1"
          ]
        },
        "outputId": "a69409a2-9355-4b53-9cd7-958308ca9349"
      },
      "source": [
        "\n",
        "# train_dataset = load_dataset('csv', data_files=\"wnut17train.csv\")\n",
        "# validation_dataset = load_dataset('csv', data_files='wnut17validation.csv')\n",
        "# test_dataset = load_dataset('csv', data_files=\"wnut17test.csv\")\n",
        "\n",
        "datasets = load_dataset(\"wnut_17\")\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b8cb9e711bd48d09ce31071513bd026",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2543.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e47edb34bed24cda9e9d67a8b6250347",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1656.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset wnut_17/wnut_17 (download: 782.18 KiB, generated: 1.66 MiB, post-processed: Unknown size, total: 2.43 MiB) to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66e8604a2a8948c0a13ddf6691a20710",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=185319.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8e5f8d7426e4592b21e0b3f11340708",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=39129.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "245681fb5a93461bb259846ca060f3ba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=66855.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b9cf11979d043fe8fdbd4033dd22237",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcea5682df834375b18b5260bcd3fc49",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06bea0d4a0854bd6bd095797fd9896dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaoyxjMX8bTP"
      },
      "source": [
        "# training_set = TweetNERDataset(datasets[\"train\"])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z17jPrtnKuu"
      },
      "source": [
        "# train_dataset\n",
        "datasets\n",
        "# train_dataset[\"train\"].features[f\"ner_tags\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "6134950fe99a4b2dac2dc657f3e772c3",
            "fc477ff4f36d49f9925b2bf307a886d6",
            "1a1fb9a81d634e8bb5fd4c6735d6adf1",
            "8b4e6355f7d246af938b6af1d68836ae",
            "ca18289603584f08a061e0224e33d39d",
            "3309721815c74b418d2614c903a5ea81",
            "18e2c358cf094df586925298cd939118",
            "79fe201e00544514b53a087924e2746f",
            "62e5a82a794649f98e7c059e3b167598",
            "7f48c0c4acd94da8ad9fde442a75360a",
            "6274d0ddf954400693aad3335888e642",
            "c44f9b16ba3c47f3afa13affc4c8c113",
            "9adaf48f47614f39a61b8aea6f8b75c7",
            "6fa83f95b5e744ae830a061a2757dbe8",
            "c8d9c44753f4491cb4d40d883ed0fd9e",
            "c0d16af27de546be813d9f87209ab341",
            "a75deac1ef694602a7ffbdffcfc152aa",
            "959bfdab0532404abad407acd2e21715",
            "e7b4eb8522f3453d87b2c8e8ffe87a38",
            "4a8527f4c84f48debf1362931f28ba25",
            "0e29885c6fd048178bc68950ce6f10fc",
            "a417da409e1c4eac8ca2be24877a8c5d",
            "8db6e6532b3848909c713eb1a90d4ad6",
            "a112cd0ae0bc4868a120cf8fb3dcfaa7"
          ]
        },
        "id": "bkvoRJbVbIeX",
        "outputId": "93db2e48-596e-4017-ab90-fa8d1f3e18ae"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "label_all_tokens = True"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6134950fe99a4b2dac2dc657f3e772c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=558.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62e5a82a794649f98e7c059e3b167598",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=843438.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a75deac1ef694602a7ffbdffcfc152aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1078931.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csG7slfdkVwg"
      },
      "source": [
        "print(datasets['train'][-1])\n",
        "print(type(datasets['train']))\n",
        "print(type(datasets['train'][-1]))\n",
        "\n",
        "# last_record_dict = {'id': '3394', 'ner_tags': [1, 2, 0, 1, 2, 0, 0], 'tokens': ['Bill', 'Nye', 'explains', 'Global', 'Warming', 'to', 'adults']}\n",
        "# datasets['train'].append(last_record_dict)\n",
        "\n",
        "# last_id = '3394'\n",
        "# datasets['train']['id'].append(last_id)\n",
        "\n",
        "# last_ner_tags = [1, 2, 0, 1, 2, 0, 0]\n",
        "# datasets['train']['ner_tags'].append(last_ner_tags)\n",
        "\n",
        "# last_tokens = ['Bill', 'Nye', 'explains', 'Global', 'Warming', 'to', 'adults'] #Bill Nye explains Global Warming to adults\n",
        "# datasets['train']['tokens'].append(last_tokens)\n",
        "\n",
        "# example = datasets[\"train\"][4]\n",
        "# print(example[\"tokens\"])\n",
        "# tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "# # input_ids = tokenizer.encode(example[\"tokens\"])\n",
        "# # tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "# print(tokenized_input)\n",
        "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "# print(tokens)\n",
        "# # len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])\n",
        "# print({x : tokenizer.encode(x, add_special_tokens=False) for x in example[\"tokens\"]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5n31Mh1_l04"
      },
      "source": [
        "# print(datasets['train'][-1])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyth64PVbIht"
      },
      "source": [
        "expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location',\n",
        "                     8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
        "BIO_dict={'O':0,'B':1,'I':2}\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "        \n",
        "    tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "    inputId_to_token_dict={}\n",
        "    for index, token in enumerate(example[\"tokens\"]):\n",
        "        values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
        "        for value in values:\n",
        "            try:\n",
        "                inputId_to_token_dict[value].append(index)\n",
        "            except KeyError:\n",
        "                inputId_to_token_dict[value]=[index]\n",
        "    labels=[]\n",
        "    for inputID in tokenized_input['input_ids']:\n",
        "        try:\n",
        "            index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
        "            index_to_address=index_list.pop(0)\n",
        "\n",
        "            label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
        "            # label = example['ner_tags'][index_to_address]\n",
        "\n",
        "            labels.append(label)\n",
        "            inputId_to_token_dict[inputID]=index_list\n",
        "        except KeyError:\n",
        "            labels.append(-100)\n",
        "\n",
        "    assert (len(tokenized_input['input_ids']) == len(labels))\n",
        "    tokenized_input['labels']=labels\n",
        "    \n",
        "    return tokenized_input"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "c1d4e03f81da497aa24c340a2176b706",
            "5f79a375dae44a97a18ffef2837fcad7",
            "b5548a69854845fba6780fae5b34d2a2",
            "dabd6b1e60254121a47c8c7c2c8b69f3",
            "c1cafb9475984b028c08dea02507e255",
            "8b9f3d6752b84d6d84ed3cc9782f7bc1",
            "9b99043a51644581a63b445a9850b5fa",
            "1200e811a10141199e3f8f1aaf364907",
            "4b9f4d6ad11b48faa5543002325c17eb",
            "676ab4ba772542778586f6bb5d3f6c01",
            "49019872401d4f3f9a90890358d4d945",
            "abb48207ce294a83a93c409c04d88d6e",
            "e1239e2885cf4faf98854ef8ed160ff4",
            "3a3d2f9736084aa69316b5514c3e2262",
            "8248ef241fb94a91bcc261702b2fee3d",
            "0b955099508a4a5c89e2a411818fcbdc",
            "e666a1f3ac9e49f88be2ef031ec84ada",
            "ca8d17e4379449d58b7ab7078d5ea309",
            "f7e541b5317d40d6b4d147ebd53275a8",
            "839fb3fb0eb64a0baec651761f71db5e",
            "3be07279769b4633adf2400e985947fc",
            "3a66bd5681f64cd38f55bbdb426b9ef9",
            "03d3f9d608ef4a6cae62c0a5a320d670",
            "b64a5022a1be4913a80495c56750e294"
          ]
        },
        "id": "6q5SzJpD4cId",
        "outputId": "23d293cd-0cb6-4a69-85ad-24f013fbb14d"
      },
      "source": [
        "# tokenize_and_align_labels(datasets['train'][2])\n",
        "tokenized_datasets = datasets.map(tokenize_and_align_labels)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1d4e03f81da497aa24c340a2176b706",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3394.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b9f4d6ad11b48faa5543002325c17eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1009.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e666a1f3ac9e49f88be2ef031ec84ada",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1287.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XthRN5iUh_Kq",
        "outputId": "80031dc8-1a7c-4792-ffae-4289f4f63298"
      },
      "source": [
        "tokenized_datasets['train']"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3394\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz4Y6EFnvB_e"
      },
      "source": [
        "print(tokenized_datasets['train'][-1])\n",
        "print(type(tokenized_datasets['train']))\n",
        "print(type(tokenized_datasets['train'][-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLWFWwewCBjL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "37d06c5e641046cca0966a07035386f4",
            "1e7318249c6f414195721175167a1c31",
            "628a27031f9e404ca85f4a12cf62bcac",
            "d27ebdbbf25044d2b2584039878566cf",
            "3420f7e0e672402fb283072ce3235cad",
            "66d83a49356f4dbeb01e023e95730996",
            "5a556959fc2b47c0a315968953adafaf",
            "807eb17c6896419a806e4473583a3ddf"
          ]
        },
        "outputId": "1719931a-3215-4f77-84c7-c3e8226190a5"
      },
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37d06c5e641046cca0966a07035386f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2482.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhh4V16GoHKx"
      },
      "source": [
        "# tokenized_datasets['train'].features[\"labels\"]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNzwMflzkDi3",
        "outputId": "6f4a655e-53ef-4f24-ad01-d065c8514fbe"
      },
      "source": [
        "# label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "\n",
        "label_list = ['O','B','I']\n",
        "\n",
        "print(label_list)\n",
        "print(len(label_list))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'B', 'I']\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV4eL-hsIafT"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # print(p.shape)\n",
        "    output, labels = p\n",
        "\n",
        "    # print(len(predictions))\n",
        "    # print(predictions[0].shape)\n",
        "    # for elem in predictions[1]:\n",
        "    #   print(elem.shape)\n",
        "\n",
        "    predictions, _ = output\n",
        "    \n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNcDvqvUkgkc"
      },
      "source": [
        "# config = AutoConfig.from_pretrained(model_checkpoint, output_hidden_states=True)\n",
        "# config"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9akEbBY2ZZl"
      },
      "source": [
        "alt_model = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(label_list))\n",
        "alt_model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRnD-mNZUaia"
      },
      "source": [
        "batch_size=32\n",
        "alt_training_args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "alt_training_args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aACODJ6AWVDG"
      },
      "source": [
        "alt_trainer = Trainer(\n",
        "    alt_model,\n",
        "    alt_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxw-OLa6mitu"
      },
      "source": [
        "**Model output is a tuple, when all hidden states are returned, i.e. output_hidden_states =True in config: (output, hidden-layers)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uryn4hHX06d"
      },
      "source": [
        "alt_trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU3-mGQJICbx"
      },
      "source": [
        "# tokenizer.save_pretrained('test-ner/')\n",
        "# alt_model.save_pretrained('test-ner/')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRvhsCdn-NxN"
      },
      "source": [
        "## **Extracting Embeddings to train the Phrase Embedder on STS Dataset (NOT REQD TO RUN)-- unless training the phrase Embedder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfxtWYA50eDp"
      },
      "source": [
        "# sys.path.insert(0,'/content/gdrive/My Drive/BERTweet-ner')\n",
        "# print(sys.path)\n",
        "\n",
        "# sys.path.append('/content/gdrive/My Drive/BERTweet-ner')\n",
        "\n",
        "# from entityEmbedding import phraseEmbedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRg-uD1iS5Mm"
      },
      "source": [
        "class PhraseEmbeddingDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, input, output):\n",
        "\n",
        "        # print(data[0])\n",
        "        # self.data = np.asarray(data)\n",
        "        # self.output = np.asarray(output)\n",
        "\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "\n",
        "        print(type(self.input),type(self.output))\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.input) == len(self.output)\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.input[idx]\n",
        "        y = self.output[idx]\n",
        "        return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cqFnxb0l-1J"
      },
      "source": [
        "def get_STS_Data_embeddings(dataset):\n",
        "    predictions=[]\n",
        "    tokenized_sentences=[]\n",
        "    count=0\n",
        "    entity_embeddings=[]\n",
        "    with torch.no_grad():\n",
        "        for record in dataset:\n",
        "            # print(record)\n",
        "            # record=record.lower()\n",
        "            tokenized_input=tokenizer(record)\n",
        "            initial_input_ids = torch.tensor([tokenizer.encode(record)])\n",
        "            token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in record.split()}\n",
        "            input_ids = initial_input_ids.to(device)\n",
        "            tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "            # output = model(input_ids)\n",
        "\n",
        "            output = alt_model(input_ids)\n",
        "\n",
        "            if(count<5):\n",
        "                print(len(output.hidden_states))\n",
        "                print(output.hidden_states[-1].shape)\n",
        "            \n",
        "            token_embeddings=output.hidden_states[-1].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
        "\n",
        "\n",
        "            prediction = (torch.argmax(output.logits, axis=2))\n",
        "            prediction = prediction.cpu().numpy().reshape(-1)\n",
        "            prediction_labels=[label_list[l].split('-')[0] for l in prediction]\n",
        "            \n",
        "#             print(token_embeddings.shape)\n",
        "            prediction_labels, entity_aware_embeddings=collate_token_label_embedding(record.split(), token_dict, prediction_labels[1:-1],token_embeddings)\n",
        "            \n",
        "            assert (len(prediction_labels)==len(record.split()))\n",
        "            assert (len(entity_aware_embeddings)==len(record.split()))\n",
        "\n",
        "            predictions.append(prediction_labels)\n",
        "            entity_embeddings.append(torch.stack(entity_aware_embeddings))\n",
        "            tokenized_sentences.append(token_dict.keys())\n",
        "\n",
        "            if(count<5):\n",
        "                print(len(entity_aware_embeddings))\n",
        "\n",
        "            count+=1\n",
        "\n",
        "    print(len(predictions),len(tokenized_sentences),len(entity_embeddings))\n",
        "    return entity_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFKMsh5Arns8"
      },
      "source": [
        "def get_sts_data(filename):\n",
        "    stsDataDictList=[]\n",
        "    stsData_columns =['sentence1', 'sentence2', 'score']\n",
        "    f=open(\"data/stsbenchmark/\"+filename+\".csv\",'r')\n",
        "    file_text= f.read()\n",
        "    lines=file_text.split('\\n')\n",
        "    for line in lines:\n",
        "        if(line):\n",
        "            fields=line.split('\\t')\n",
        "    #         print(len(fields))\n",
        "            dataDict={'sentence1':fields[5],'sentence2':fields[6],'score':float(fields[4])/5.0}\n",
        "    #         print(dataDict)\n",
        "            stsDataDictList.append(dataDict)\n",
        "    stsData=pd.DataFrame(stsDataDictList)\n",
        "    return stsData\n",
        "# stsTrainData.columns =['genre', 'filename', 'year', 'unidentified', 'score', 'sentence1', 'sentence2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvMv5O0MmFVG",
        "outputId": "04691718-d80f-4c07-d0d3-9f4616cf4914"
      },
      "source": [
        "stsTrainData = get_sts_data('sts-train')\n",
        "print(len(stsTrainData))\n",
        "print(stsTrainData.columns)\n",
        "\n",
        "stsDevData = get_sts_data('sts-dev')\n",
        "print(len(stsDevData))\n",
        "print(stsDevData.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5749\n",
            "Index(['sentence1', 'sentence2', 'score'], dtype='object')\n",
            "1500\n",
            "Index(['sentence1', 'sentence2', 'score'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fSzSvqFmVEW"
      },
      "source": [
        "# first pass it through NER Engine and get the contextual embeddings\n",
        "\n",
        "#Training Data\n",
        "#For Source Sentences:\n",
        "source_sentence_embeddings_train = get_STS_Data_embeddings(stsTrainData['sentence1'].tolist())\n",
        "#For Target Sentences:\n",
        "target_sentence_embeddings_train = get_STS_Data_embeddings(stsTrainData['sentence2'].tolist())\n",
        "assert len(source_sentence_embeddings_train)==len(target_sentence_embeddings_train)\n",
        "\n",
        "#Validation Data\n",
        "#For Source Sentences:\n",
        "source_sentence_embeddings_dev = get_STS_Data_embeddings(stsDevData['sentence1'].tolist())\n",
        "#For Target Sentences:\n",
        "target_sentence_embeddings_dev = get_STS_Data_embeddings(stsDevData['sentence2'].tolist())\n",
        "assert len(source_sentence_embeddings_dev)==len(target_sentence_embeddings_dev)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TXrkbXVIXp3"
      },
      "source": [
        "#Printing some shapes\n",
        "print(len(source_sentence_embeddings_train), len(target_sentence_embeddings_train))\n",
        "print(source_sentence_embeddings_train[0].shape)\n",
        "embeddingSize=list(source_sentence_embeddings_train[0][0].shape)[0]\n",
        "# print(embeddingSize)\n",
        "# print(type(list(embeddingSize)[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2WxtYySGMHA"
      },
      "source": [
        "# Datasets and DataLoaders\n",
        "training_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_train, target_sentence_embeddings_train))),stsTrainData['score'].tolist())\n",
        "training_generator = torch.utils.data.DataLoader(training_set, shuffle=True)\n",
        "\n",
        "validation_set = PhraseEmbeddingDataset(list(map(list, zip(source_sentence_embeddings_dev, target_sentence_embeddings_dev))),stsDevData['score'].tolist())\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfFolw3IhrA"
      },
      "source": [
        "x,y =training_set.__getitem__(0)\n",
        "print(type(x))\n",
        "print(x[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dje0rlO66WDC"
      },
      "source": [
        "def save_ckp(state, is_best, checkpoint_dir):\n",
        "    # f_path = checkpoint_dir + '/checkpoint.pt' \n",
        "    f_path = checkpoint_dir + '/checkpoint_model300.pt' #_model300\n",
        "    torch.save(state, f_path)\n",
        "    # if is_best:\n",
        "    #     best_fpath = best_model_dir +'/best_model.pt'\n",
        "    #     shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGbFUV5PcX8Q"
      },
      "source": [
        "# !pip3 install pytorchtools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7rLtDdOREIH"
      },
      "source": [
        "# import EarlyStopping\n",
        "# from pytorchtools import EarlyStopping\n",
        "\n",
        "# Initialize network\n",
        "phraseEmbeddingModel = PhraseEmbedding(embeddingSize, output_embedding_size, device).to(device)\n",
        "\n",
        "#Loss and Optimizer\n",
        "criterion = nn.MSELoss(reduction='mean' )\n",
        "optimizer = optim.Adam(phraseEmbeddingModel.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# initialize the early_stopping object\n",
        "# early_stopping = EarlyStopping(patience=patience, verbose=True, path='entityEmbedding/model_checkpoints/checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM4tsJ6l0BxX"
      },
      "source": [
        "checkpoint_dir='entityEmbedding/model_checkpoints'\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 200\n",
        "patience = 5\n",
        "\n",
        "# Train Network\n",
        "history_training= []\n",
        "history_validation = []\n",
        "best_loss = np.float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_batch_loss=[]\n",
        "    for batch_idx, (data, target) in enumerate(training_generator):\n",
        "        target = torch.tensor(float(target)).to(device=device)\n",
        "        out = phraseEmbeddingModel(data)\n",
        "        # print(out.item())\n",
        "        # if(not math.isnan(out.item())):\n",
        "            # print(data)\n",
        "        loss = criterion(out, target)\n",
        "        # print(loss.item())\n",
        "        training_batch_loss.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(phraseEmbeddingModel.parameters(), 1.0)\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "    combined_training_loss = np.mean(training_batch_loss)\n",
        "    print('combined epoch training loss:', combined_training_loss)\n",
        "    history_training.append(combined_training_loss)\n",
        "\n",
        "    #Validation: DO NOT BACKPROPAGATE HERE\n",
        "    validation_batch_loss = []\n",
        "    print_only_one=True\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(validation_generator):\n",
        "            target = torch.tensor(float(target)).to(device=device)\n",
        "            if(print_only_one):\n",
        "                # print(len(data[0]),len(data[1]))\n",
        "                # print(type(data))\n",
        "                print_only_one=False\n",
        "            out = phraseEmbeddingModel(data)\n",
        "            loss = criterion(out, target)\n",
        "            validation_batch_loss.append(loss.item())\n",
        "            # print(validation_batch_loss)\n",
        "    combined_validation_loss= np.mean(validation_batch_loss)\n",
        "    history_validation.append(combined_validation_loss)\n",
        "    # if(((epoch+1)%10==0)|(epoch == (num_epochs-1))):\n",
        "    print('\\nEpoch',str(epoch+1),' Validation Loss:',combined_validation_loss)\n",
        "    print('=====================================================================\\n')\n",
        "    \n",
        "    if(combined_validation_loss<best_loss):\n",
        "        best_loss = combined_validation_loss\n",
        "        print('saving this checkpoint')\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        save_ckp(checkpoint, True, checkpoint_dir)\n",
        "        no_improvement_counter=0\n",
        "    else:\n",
        "        no_improvement_counter+=1\n",
        "        if(no_improvement_counter>patience):\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhKf3SBXPjI"
      },
      "source": [
        "# checkpoint = {\n",
        "#     'epoch': epoch + 1,\n",
        "#     'state_dict': phraseEmbeddingModel.state_dict(),\n",
        "#     'optimizer': optimizer.state_dict()\n",
        "# }\n",
        "# checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint.pt' #768\n",
        "checkpoint_path = 'entityEmbedding/model_checkpoints/checkpoint_model300.pt' #300\n",
        "# model_dir = 'best-model'\n",
        "# save_ckp(checkpoint, True, checkpoint_dir, model_dir)\n",
        "\n",
        "# load the last checkpoint with the best model\n",
        "entityPhraseEmbedder, optimizer, start_epoch = load_ckp(checkpoint_path, phraseEmbeddingModel, optimizer)\n",
        "print('Loading model from epoch:', start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhVLIVUy57ZX"
      },
      "source": [
        "## **Running the engine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PCjtUKR6IOl"
      },
      "source": [
        "# import localNER as localNER (self, sentenceTokenizer, nerTokenizer, nerEngine, label_list, device):\n",
        "# local_NER_Module= localNER.LocalNERModule(sentenceTokenizer, tokenizer, alt_model, label_list, device)\n",
        "\n",
        "#tokenizer here is the BERT model's tokenizer\n",
        "local_NER_Module= LocalNERModule(sentence_tokenizer, tokenizer, alt_model, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I9OVbA6Q1sR"
      },
      "source": [
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'wnut17')\n",
        "\n",
        "global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'btc')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_3k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'tweets_6k_annotated')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'venezuela')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'covid_2K')\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billdeblasio')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'ripcity')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'pikapika')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'roevwade')\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'billnye')\n",
        "\n",
        "\n",
        "# global_NER_Module = GlobalNERModule(entityPhraseEmbedder, device,'classifier-train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Qa9DQDpWIG"
      },
      "source": [
        "print('Tweets are in memory...')\n",
        "\n",
        "length=len(tweets_unpartitoned)\n",
        "batch_size=length\n",
        "print(length, batch_size)\n",
        "val=math.ceil(length/batch_size)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThM_gzA8CqPd"
      },
      "source": [
        "tweet_batch = tweets_unpartitoned\n",
        "print(len(tweet_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH3mGcUI9p-Q"
      },
      "source": [
        "tweet_to_sentences_w_annotation={}\n",
        "# df_out_holder_Phase1=[]\n",
        "total_time=0\n",
        "reintroduction_threshold_dummy=0\n",
        "max_batch_value=112\n",
        "z_score=1\n",
        "\n",
        "# for g, tweet_batch in tweets_unpartitoned.groupby(np.arange(length) //batch_size):\n",
        "phaseI_timein=time.time()\n",
        "g=0\n",
        "tuple_of= local_NER_Module.extract(tweet_batch,g)\n",
        "phaseI_timeout=time.time()\n",
        "\n",
        "print('local emd time',(phaseI_timeout-phaseI_timein))\n",
        "\n",
        "tweet_base=tuple_of[0]\n",
        "contextual_embeddings=tuple_of[1]\n",
        "candidate_base=tuple_of[2]\n",
        "elapsedTime= tuple_of[4] - tuple_of[3]\n",
        "phase2stopwordList=tuple_of[5]\n",
        "# print('len of tweet_base = '  len(tweet_base))\n",
        "tweet_to_sentences_w_annotation=tuple_of[6]\n",
        "total_time+=elapsedTime\n",
        "print(elapsedTime,total_time)\n",
        "\n",
        "# df_out_holder_Phase1.append(tweet_base)\n",
        "\n",
        "print ('Produced', g)\n",
        "print(\"**********************************************************\")\n",
        "\n",
        "# candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5RnF9Es20I1"
      },
      "source": [
        "# # # #PHASE I CHECKING:\n",
        "# # processed_tweets=pd.concat(df_out_holder_Phase1,ignore_index=True)\n",
        "# print(len(tweet_base.phase1Candidates.values),local_NER_Module.sentenceID)\n",
        "calculate_f1(tweet_to_sentences_w_annotation, tweet_base.phase1Candidates.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSH_Ae_LDXWB"
      },
      "source": [
        "# candidates=candidate_base.displayTrie(\"\",[])\n",
        "# print(len(candidates))\n",
        "# print(candidates)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzk71F1f-d1J"
      },
      "source": [
        "phaseII_timein=time.time()\n",
        "candidate_base_post_Phase2, complete_tweet_dataframe,time_out= global_NER_Module.executor(max_batch_value,tweet_base,candidate_base,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_batch)\n",
        "phaseII_timeout=time_out\n",
        "\n",
        "print('global emd time',(phaseII_timeout-phaseII_timein))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}